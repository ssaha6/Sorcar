function {:existential true} my_inv (
 b0000: bool,
 b0001: bool,
 b0002: bool,
 b0003: bool,
 b0004: bool,
 b0005: bool,
 b0006: bool,
 b0007: bool,
 b0008: bool,
 b0009: bool,
 b0010: bool,
 b0011: bool,
 b0012: bool,
 b0013: bool,
 b0014: bool,
 b0015: bool,
 b0016: bool,
 b0017: bool,
 b0018: bool,
 b0019: bool,
 b0020: bool,
 b0021: bool,
 b0022: bool,
 b0023: bool,
 b0024: bool,
 b0025: bool,
 b0026: bool,
 b0027: bool,
 b0028: bool,
 b0029: bool,
 b0030: bool,
 b0031: bool,
 b0032: bool,
 b0033: bool,
 b0034: bool,
 b0035: bool,
 b0036: bool,
 b0037: bool,
 b0038: bool,
 b0039: bool,
 b0040: bool,
 b0041: bool,
 b0042: bool,
 b0043: bool
 ) : bool;
type _SIZE_T_TYPE = bv32;

procedure _ATOMIC_OP64(x: [int]int, y: int) returns (z$1: int, A$1: [int]int, z$2: int, A$2: [int]int);



var {:source_name "A"} {:global} $$A: [int]int;

axiom {:array_info "$$A"} {:global} {:elem_width 64} {:source_name "A"} {:source_elem_width 64} {:source_dimensions "*"} true;

var {:race_checking} {:global} {:elem_width 64} {:source_elem_width 64} {:source_dimensions "*"} _READ_HAS_OCCURRED_$$A: bool;

var {:race_checking} {:global} {:elem_width 64} {:source_elem_width 64} {:source_dimensions "*"} _WRITE_HAS_OCCURRED_$$A: bool;

var {:race_checking} {:global} {:elem_width 64} {:source_elem_width 64} {:source_dimensions "*"} _ATOMIC_HAS_OCCURRED_$$A: bool;

const _WATCHED_OFFSET: int;

const {:global_offset_x} global_offset_x: int;

const {:global_offset_y} global_offset_y: int;

const {:global_offset_z} global_offset_z: int;

const {:group_id_x} group_id_x$1: int;

const {:group_id_x} group_id_x$2: int;

const {:group_id_y} group_id_y$1: int;

const {:group_id_y} group_id_y$2: int;

const {:group_size_x} group_size_x: int;

const {:group_size_y} group_size_y: int;

const {:group_size_z} group_size_z: int;

const {:local_id_x} local_id_x$1: int;

const {:local_id_x} local_id_x$2: int;

const {:local_id_y} local_id_y$1: int;

const {:local_id_y} local_id_y$2: int;

const {:num_groups_x} num_groups_x: int;

const {:num_groups_y} num_groups_y: int;

const {:num_groups_z} num_groups_z: int;

function BV32_SEXT64(int) : int;

function BV_EXTRACT(int, int, int) : int;

function FADD64(int, int) : int;

function FDIV64(int, int) : int;

function  BV1_ZEXT32(x: int) : int
{
  x
}

function  BV32_ADD(x: int, y: int) : int
{
  x + y
}

function  BV32_AND(x: int, y: int) : int
{
  (if x == y then x else (if x == 0 || y == 0 then 0 else BV32_AND_UF(x, y)))
}

function BV32_AND_UF(int, int) : int;

function  BV32_MUL(x: int, y: int) : int
{
  x * y
}

function  BV32_OR(x: int, y: int) : int
{
  (if x == y then x else (if x == 0 then y else (if y == 0 then x else BV32_OR_UF(x, y))))
}

function BV32_OR_UF(int, int) : int;

function  BV32_SGE(x: int, y: int) : bool
{
  x >= y
}

function  BV32_SLE(x: int, y: int) : bool
{
  x <= y
}

function  BV32_SUB(x: int, y: int) : int
{
  x - y
}

function  BV32_UDIV(x: int, y: int) : int
{
  x div y
}

function  BV32_UGE(x: int, y: int) : bool
{
  x >= y
}

function  BV32_ULE(x: int, y: int) : bool
{
  x <= y
}

function  BV32_UREM(x: int, y: int) : int
{
  x mod y
}

function  BV32_ZEXT64(x: int) : int
{
  x
}

function  BV64_ADD(x: int, y: int) : int
{
  x + y
}

function  BV64_MUL(x: int, y: int) : int
{
  x * y
}

function  BV64_SDIV(x: int, y: int) : int
{
  x div y
}

function  BV64_SGE(x: int, y: int) : bool
{
  x >= y
}

function  BV64_SGT(x: int, y: int) : bool
{
  x > y
}

function  BV64_SLE(x: int, y: int) : bool
{
  x <= y
}

function  BV64_SLT(x: int, y: int) : bool
{
  x < y
}

function  BV64_SREM(x: int, y: int) : int
{
  x mod y
}

function  BV64_SUB(x: int, y: int) : int
{
  x - y
}

procedure {:source_name "kernel0"} {:kernel} $kernel0($n: int, $tsteps: int, $c0: int);
  requires {:sourceloc_num 0} {:thread 1} (if $n == 64 then 1 else 0) != 0;
  requires {:sourceloc_num 1} {:thread 1} (if $tsteps == 64 then 1 else 0) != 0;
  requires {:sourceloc_num 2} {:thread 1} (if BV64_SLT($c0, BV32_SEXT64(BV32_SUB(BV32_ADD(BV32_MUL(3, $n), BV32_MUL(4, $tsteps)), 9))) then 1 else 0) != 0;
  requires {:sourceloc_num 3} {:thread 1} (if BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV1_ZEXT32((if BV32_SGE($n, 3) then 1 else 0)), BV1_ZEXT32((if BV32_SLE($n, 2147483647) then 1 else 0))), BV1_ZEXT32((if BV32_SGE($tsteps, 1) then 1 else 0))), BV1_ZEXT32((if BV32_SLE($tsteps, 2147483647) then 1 else 0))), BV1_ZEXT32((if BV64_SGE(BV32_SEXT64(BV32_ADD(BV32_MUL(3, $n), BV32_MUL(4, $tsteps))), BV64_ADD($c0, 10)) then 1 else 0))), BV1_ZEXT32((if BV64_SGE($c0, 3) then 1 else 0))) != 0 then 1 else 0) != 0;
  requires {:procedure_wide_invariant} {:do_not_predicate} {:sourceloc_num 4} {:thread 1} (if _WRITE_HAS_OCCURRED_$$A ==> BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV1_ZEXT32((if BV64_SGE(BV64_ADD(BV64_MUL(32, BV32_ZEXT64(group_id_x$1)), BV32_ZEXT64(local_id_x$1)), 0) then 1 else 0)), BV1_ZEXT32((if BV64_SLE(BV64_ADD(BV64_MUL(32, BV32_ZEXT64(group_id_x$1)), BV32_ZEXT64(local_id_x$1)), 8191) then 1 else 0))), BV1_ZEXT32((if BV32_UGE($n, BV32_ADD(BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n), 2)) then 1 else 0))), BV1_ZEXT32((if BV32_UGE(BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n), 1) then 1 else 0))), BV1_ZEXT32((if BV32_UGE($n, BV32_ADD(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), 2)) then 1 else 0))), BV1_ZEXT32((if BV32_UGE(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), 1) then 1 else 0))), BV1_ZEXT32((if BV64_SGE(BV32_ZEXT64(BV32_ADD(BV32_ADD(BV32_MUL(4, $tsteps), BV32_MUL(2, BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n))), BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n))), BV64_ADD($c0, 4)) then 1 else 0))), BV1_ZEXT32((if BV64_SGE($c0, BV32_ZEXT64(BV32_ADD(BV32_MUL(2, BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n)), BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n)))) then 1 else 0))), BV1_ZEXT32((if BV64_SGE(BV64_SREM(BV64_ADD(BV32_ZEXT64(BV32_SUB(BV32_MUL(-2, BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n)), BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n))), $c0), 32768), BV64_MUL(128, BV32_ZEXT64(group_id_y$1))) then 1 else 0))), BV1_ZEXT32((if BV64_SGE(BV64_ADD(BV64_MUL(128, BV32_ZEXT64(group_id_y$1)), 124), BV64_SREM(BV64_ADD(BV32_ZEXT64(BV32_SUB(BV32_MUL(-2, BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n)), BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n))), $c0), 32768)) then 1 else 0))), BV1_ZEXT32((if BV64_SREM(BV64_SUB(BV64_ADD(BV64_MUL(32, BV32_ZEXT64(group_id_x$1)), BV32_ZEXT64(local_id_x$1)), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n))), 8192) == 0 then 1 else 0))), BV1_ZEXT32((if BV64_SREM(BV64_SUB(BV64_ADD(BV64_ADD(BV64_MUL(2, BV32_ZEXT64(local_id_x$1)), BV64_MUL(4, BV32_ZEXT64(local_id_y$1))), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n))), $c0), 64) == 0 then 1 else 0))) != 0 then 1 else 0) != 0;
  requires {:procedure_wide_invariant} {:do_not_predicate} {:sourceloc_num 5} {:thread 1} (if _READ_HAS_OCCURRED_$$A ==> BV32_OR(BV32_OR(BV32_OR(BV32_OR(BV32_OR(BV32_OR(BV32_OR(BV32_OR(BV32_OR(BV32_OR(BV32_OR(BV32_OR(BV32_OR(BV32_OR(BV32_OR(BV32_OR(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV1_ZEXT32((if BV32_UGE(BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n), 3) then 1 else 0)), BV1_ZEXT32((if BV32_UGE($n, BV32_ADD(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), 3)) then 1 else 0))), BV1_ZEXT32((if BV64_SGE(BV32_ZEXT64(BV32_ADD(BV32_ADD(BV32_MUL(4, $tsteps), BV32_MUL(2, BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n))), BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n))), BV64_ADD($c0, 5)) then 1 else 0))), BV1_ZEXT32((if BV64_SGE(BV32_SEXT64($n), BV64_ADD(BV64_ADD(BV64_SREM(BV64_ADD(BV64_ADD(BV64_ADD(BV64_MUL(32, BV32_ZEXT64(group_id_x$1)), BV32_ZEXT64(local_id_x$1)), BV32_ZEXT64(BV32_MUL(8191, BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n)))), 1), 8192), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n))), 1)) then 1 else 0))), BV1_ZEXT32((if BV64_SLE(BV64_SREM(BV64_ADD(BV64_ADD(BV64_ADD(BV64_MUL(32, BV32_ZEXT64(group_id_x$1)), BV32_ZEXT64(local_id_x$1)), BV32_ZEXT64(BV32_MUL(8191, BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n)))), 1), 8192), 1) then 1 else 0))), BV1_ZEXT32((if BV64_SGE(BV64_ADD($c0, 1), BV64_ADD(BV64_ADD(BV64_MUL(2, BV64_SREM(BV64_ADD(BV64_ADD(BV64_ADD(BV64_MUL(32, BV32_ZEXT64(group_id_x$1)), BV32_ZEXT64(local_id_x$1)), BV32_ZEXT64(BV32_MUL(8191, BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n)))), 1), 8192)), BV32_ZEXT64(BV32_MUL(2, BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n)))), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n)))) then 1 else 0))), BV1_ZEXT32((if BV64_SGE(BV64_MUL(64, (if BV64_SLT(BV64_ADD(BV64_ADD(BV64_ADD(BV64_ADD(BV64_MUL(2, BV32_ZEXT64(local_id_x$1)), BV64_MUL(4, BV32_ZEXT64(local_id_y$1))), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n))), BV64_MUL(63, $c0)), 1), 0) then BV64_SUB(0, BV64_SDIV(BV64_SUB(BV64_ADD(BV64_SUB(0, BV64_ADD(BV64_ADD(BV64_ADD(BV64_ADD(BV64_MUL(2, BV32_ZEXT64(local_id_x$1)), BV64_MUL(4, BV32_ZEXT64(local_id_y$1))), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n))), BV64_MUL(63, $c0)), 1)), 64), 1), 64)) else BV64_SDIV(BV64_ADD(BV64_ADD(BV64_ADD(BV64_ADD(BV64_MUL(2, BV32_ZEXT64(local_id_x$1)), BV64_MUL(4, BV32_ZEXT64(local_id_y$1))), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n))), BV64_MUL(63, $c0)), 1), 64))), BV64_ADD(BV64_ADD(BV64_ADD(BV64_MUL(2, BV32_ZEXT64(local_id_x$1)), BV64_MUL(4, BV32_ZEXT64(local_id_y$1))), BV64_MUL(63, $c0)), 1)) then 1 else 0))), BV1_ZEXT32((if BV64_SGE(BV64_SREM(BV64_ADD(BV64_ADD(BV64_ADD(BV64_MUL(32, BV32_ZEXT64(group_id_x$1)), BV32_ZEXT64(local_id_x$1)), BV32_ZEXT64(BV32_MUL(8191, BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n)))), 1), 8192), BV64_SREM(BV64_ADD(BV64_ADD(BV64_ADD(BV64_ADD(BV64_MUL(2, BV32_ZEXT64(local_id_x$1)), BV64_MUL(4, BV32_ZEXT64(local_id_y$1))), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n))), BV64_MUL(63, $c0)), 1), 64)) then 1 else 0))), BV1_ZEXT32((if BV64_SGE(BV64_ADD(BV32_ZEXT64(local_id_y$1), BV64_MUL(8192, BV64_SDIV(BV64_ADD(BV64_ADD(BV64_ADD(BV64_ADD(BV64_ADD(BV64_ADD(BV64_ADD(BV64_SUB(0, BV64_MUL(2, BV64_SREM(BV64_ADD(BV64_ADD(BV64_ADD(BV64_MUL(32, BV32_ZEXT64(group_id_x$1)), BV32_ZEXT64(local_id_x$1)), BV32_ZEXT64(BV32_MUL(8191, BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n)))), 1), 8192))), BV64_MUL(128, BV32_ZEXT64(group_id_x$1))), BV64_MUL(128, BV32_ZEXT64(group_id_y$1))), BV64_MUL(4, BV32_ZEXT64(local_id_x$1))), BV32_ZEXT64(BV32_MUL(32766, BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n)))), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n))), BV64_MUL(32767, $c0)), 127), 32768))), BV64_ADD(BV64_ADD(BV64_ADD(BV64_ADD(BV64_ADD(BV64_MUL(16, BV32_ZEXT64(group_id_x$1)), BV64_MUL(32, BV32_ZEXT64(group_id_y$1))), BV32_ZEXT64(BV32_MUL(4096, BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n)))), BV64_MUL(8176, $c0)), BV64_MUL(4096, BV64_SDIV(BV64_ADD(BV64_ADD(BV64_ADD(BV64_MUL(32, BV32_ZEXT64(group_id_x$1)), BV32_ZEXT64(local_id_x$1)), BV32_ZEXT64(BV32_MUL(8191, BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n)))), 1), 8192))), BV64_MUL(16, BV64_SDIV(BV64_ADD(BV64_ADD(BV64_ADD(BV64_ADD(BV64_MUL(2, BV32_ZEXT64(local_id_x$1)), BV64_MUL(4, BV32_ZEXT64(local_id_y$1))), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n))), BV64_MUL(63, $c0)), 1), 64)))) then 1 else 0))), BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV1_ZEXT32((if BV32_ZEXT64(group_id_x$1) == 0 then 1 else 0)), BV1_ZEXT32((if BV64_SGE(BV32_SEXT64($n), BV64_ADD(BV32_ZEXT64(local_id_x$1), 2)) then 1 else 0))), BV1_ZEXT32((if BV64_SGE(BV32_ZEXT64(local_id_x$1), 1) then 1 else 0))), BV1_ZEXT32((if BV64_SGE(BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n)), BV32_ZEXT64(local_id_x$1)) then 1 else 0))), BV1_ZEXT32((if BV32_ULE(BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n), 2) then 1 else 0))), BV1_ZEXT32((if BV32_UGE($n, BV32_ADD(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), 3)) then 1 else 0))), BV1_ZEXT32((if BV64_SGE($c0, BV64_ADD(BV64_ADD(BV64_MUL(2, BV32_ZEXT64(local_id_x$1)), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n))), 1)) then 1 else 0))), BV1_ZEXT32((if BV64_SGE(BV32_ZEXT64(BV32_ADD(BV32_MUL(4, $tsteps), BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n))), BV64_ADD($c0, 1)) then 1 else 0))), BV1_ZEXT32((if BV64_SGE(BV64_MUL(64, (if BV64_SLT(BV64_ADD(BV64_ADD(BV64_ADD(BV64_ADD(BV64_MUL(2, BV32_ZEXT64(local_id_x$1)), BV64_MUL(4, BV32_ZEXT64(local_id_y$1))), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n))), BV64_MUL(63, $c0)), 1), 0) then BV64_SUB(0, BV64_SDIV(BV64_SUB(BV64_ADD(BV64_SUB(0, BV64_ADD(BV64_ADD(BV64_ADD(BV64_ADD(BV64_MUL(2, BV32_ZEXT64(local_id_x$1)), BV64_MUL(4, BV32_ZEXT64(local_id_y$1))), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n))), BV64_MUL(63, $c0)), 1)), 64), 1), 64)) else BV64_SDIV(BV64_ADD(BV64_ADD(BV64_ADD(BV64_ADD(BV64_MUL(2, BV32_ZEXT64(local_id_x$1)), BV64_MUL(4, BV32_ZEXT64(local_id_y$1))), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n))), BV64_MUL(63, $c0)), 1), 64))), BV64_ADD(BV64_ADD(BV64_ADD(BV64_MUL(2, BV32_ZEXT64(local_id_x$1)), BV64_MUL(4, BV32_ZEXT64(local_id_y$1))), BV64_MUL(63, $c0)), 1)) then 1 else 0))), BV1_ZEXT32((if BV64_SGE(BV64_ADD(BV32_ZEXT64(local_id_x$1), 1), BV64_ADD(BV64_SREM(BV64_ADD(BV64_ADD(BV64_ADD(BV64_ADD(BV64_MUL(2, BV32_ZEXT64(local_id_x$1)), BV64_MUL(4, BV32_ZEXT64(local_id_y$1))), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n))), BV64_MUL(63, $c0)), 1), 64), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n)))) then 1 else 0))), BV1_ZEXT32((if BV64_SGE(BV64_ADD(BV32_ZEXT64(local_id_y$1), BV64_MUL(8192, BV64_SDIV(BV64_ADD(BV64_ADD(BV64_ADD(BV64_ADD(BV64_MUL(128, BV32_ZEXT64(group_id_y$1)), BV64_MUL(2, BV32_ZEXT64(local_id_x$1))), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n))), BV64_MUL(32767, $c0)), 125), 32768))), BV64_ADD(BV64_ADD(BV64_MUL(32, BV32_ZEXT64(group_id_y$1)), BV64_MUL(8176, $c0)), BV64_MUL(16, BV64_SDIV(BV64_ADD(BV64_ADD(BV64_ADD(BV64_ADD(BV64_MUL(2, BV32_ZEXT64(local_id_x$1)), BV64_MUL(4, BV32_ZEXT64(local_id_y$1))), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n))), BV64_MUL(63, $c0)), 1), 64)))) then 1 else 0)))), BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV1_ZEXT32((if BV64_SGE(BV64_ADD(BV64_MUL(32, BV32_ZEXT64(group_id_x$1)), BV32_ZEXT64(local_id_x$1)), 0) then 1 else 0)), BV1_ZEXT32((if BV64_SLE(BV64_ADD(BV64_MUL(32, BV32_ZEXT64(group_id_x$1)), BV32_ZEXT64(local_id_x$1)), 8191) then 1 else 0))), BV1_ZEXT32((if BV32_UGE($n, BV32_ADD(BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n), 3)) then 1 else 0))), BV1_ZEXT32((if BV32_UGE(BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n), 0) then 1 else 0))), BV1_ZEXT32((if BV64_SGE(BV64_ADD(BV64_ADD(BV32_ZEXT64(local_id_x$1), BV64_MUL(2, BV32_ZEXT64(local_id_y$1))), BV64_MUL(32, (if BV64_SLT(BV64_ADD(BV64_ADD(BV64_SUB(BV64_SUB(BV64_MUL(-2, BV32_ZEXT64(local_id_x$1)), BV64_MUL(4, BV32_ZEXT64(local_id_y$1))), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n))), $c0), 62), 0) then BV64_SUB(0, BV64_SDIV(BV64_SUB(BV64_ADD(BV64_SUB(0, BV64_ADD(BV64_ADD(BV64_SUB(BV64_SUB(BV64_MUL(-2, BV32_ZEXT64(local_id_x$1)), BV64_MUL(4, BV32_ZEXT64(local_id_y$1))), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n))), $c0), 62)), 64), 1), 64)) else BV64_SDIV(BV64_ADD(BV64_ADD(BV64_SUB(BV64_SUB(BV64_MUL(-2, BV32_ZEXT64(local_id_x$1)), BV64_MUL(4, BV32_ZEXT64(local_id_y$1))), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n))), $c0), 62), 64)))), BV32_ZEXT64(BV32_ADD(BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n), 1))) then 1 else 0))), BV1_ZEXT32((if BV64_SGE(BV32_ZEXT64(BV32_ADD(BV32_MUL(2, $tsteps), BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n))), BV64_ADD(BV64_ADD(BV64_ADD(BV32_ZEXT64(local_id_x$1), BV64_MUL(2, BV32_ZEXT64(local_id_y$1))), BV64_MUL(32, (if BV64_SLT(BV64_ADD(BV64_ADD(BV64_SUB(BV64_SUB(BV64_MUL(-2, BV32_ZEXT64(local_id_x$1)), BV64_MUL(4, BV32_ZEXT64(local_id_y$1))), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n))), $c0), 62), 0) then BV64_SUB(0, BV64_SDIV(BV64_SUB(BV64_ADD(BV64_SUB(0, BV64_ADD(BV64_ADD(BV64_SUB(BV64_SUB(BV64_MUL(-2, BV32_ZEXT64(local_id_x$1)), BV64_MUL(4, BV32_ZEXT64(local_id_y$1))), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n))), $c0), 62)), 64), 1), 64)) else BV64_SDIV(BV64_ADD(BV64_ADD(BV64_SUB(BV64_SUB(BV64_MUL(-2, BV32_ZEXT64(local_id_x$1)), BV64_MUL(4, BV32_ZEXT64(local_id_y$1))), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n))), $c0), 62), 64)))), 1)) then 1 else 0))), BV1_ZEXT32((if BV64_SGE(BV64_ADD(BV64_ADD(BV64_ADD(BV32_SEXT64($n), BV64_MUL(2, BV32_ZEXT64(local_id_x$1))), BV64_MUL(4, BV32_ZEXT64(local_id_y$1))), BV64_MUL(64, (if BV64_SLT(BV64_ADD(BV64_ADD(BV64_SUB(BV64_SUB(BV64_MUL(-2, BV32_ZEXT64(local_id_x$1)), BV64_MUL(4, BV32_ZEXT64(local_id_y$1))), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n))), $c0), 62), 0) then BV64_SUB(0, BV64_SDIV(BV64_SUB(BV64_ADD(BV64_SUB(0, BV64_ADD(BV64_ADD(BV64_SUB(BV64_SUB(BV64_MUL(-2, BV32_ZEXT64(local_id_x$1)), BV64_MUL(4, BV32_ZEXT64(local_id_y$1))), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n))), $c0), 62)), 64), 1), 64)) else BV64_SDIV(BV64_ADD(BV64_ADD(BV64_SUB(BV64_SUB(BV64_MUL(-2, BV32_ZEXT64(local_id_x$1)), BV64_MUL(4, BV32_ZEXT64(local_id_y$1))), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n))), $c0), 62), 64)))), BV64_ADD($c0, 2)) then 1 else 0))), BV1_ZEXT32((if BV64_SGE($c0, BV64_ADD(BV64_ADD(BV64_ADD(BV64_MUL(2, BV32_ZEXT64(local_id_x$1)), BV64_MUL(4, BV32_ZEXT64(local_id_y$1))), BV64_MUL(64, (if BV64_SLT(BV64_ADD(BV64_ADD(BV64_SUB(BV64_SUB(BV64_MUL(-2, BV32_ZEXT64(local_id_x$1)), BV64_MUL(4, BV32_ZEXT64(local_id_y$1))), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n))), $c0), 62), 0) then BV64_SUB(0, BV64_SDIV(BV64_SUB(BV64_ADD(BV64_SUB(0, BV64_ADD(BV64_ADD(BV64_SUB(BV64_SUB(BV64_MUL(-2, BV32_ZEXT64(local_id_x$1)), BV64_MUL(4, BV32_ZEXT64(local_id_y$1))), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n))), $c0), 62)), 64), 1), 64)) else BV64_SDIV(BV64_ADD(BV64_ADD(BV64_SUB(BV64_SUB(BV64_MUL(-2, BV32_ZEXT64(local_id_x$1)), BV64_MUL(4, BV32_ZEXT64(local_id_y$1))), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n))), $c0), 62), 64)))), 1)) then 1 else 0))), BV1_ZEXT32((if BV64_SGE($c0, BV64_ADD(BV64_ADD(BV64_ADD(BV64_MUL(2, BV32_ZEXT64(local_id_x$1)), BV64_MUL(4, BV32_ZEXT64(local_id_y$1))), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n))), BV64_MUL(64, (if BV64_SLT(BV64_ADD(BV64_ADD(BV64_SUB(BV64_SUB(BV64_MUL(-2, BV32_ZEXT64(local_id_x$1)), BV64_MUL(4, BV32_ZEXT64(local_id_y$1))), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n))), $c0), 62), 0) then BV64_SUB(0, BV64_SDIV(BV64_SUB(BV64_ADD(BV64_SUB(0, BV64_ADD(BV64_ADD(BV64_SUB(BV64_SUB(BV64_MUL(-2, BV32_ZEXT64(local_id_x$1)), BV64_MUL(4, BV32_ZEXT64(local_id_y$1))), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n))), $c0), 62)), 64), 1), 64)) else BV64_SDIV(BV64_ADD(BV64_ADD(BV64_SUB(BV64_SUB(BV64_MUL(-2, BV32_ZEXT64(local_id_x$1)), BV64_MUL(4, BV32_ZEXT64(local_id_y$1))), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n))), $c0), 62), 64))))) then 1 else 0))), BV1_ZEXT32((if BV64_SGE(BV64_SREM(BV64_ADD(BV64_ADD(BV64_SUB(BV64_ADD(BV64_ADD(BV64_MUL(-64, BV32_ZEXT64(group_id_y$1)), BV32_ZEXT64(local_id_x$1)), BV64_MUL(2, BV32_ZEXT64(local_id_y$1))), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n))), BV64_MUL(32, BV64_SDIV(BV64_ADD(BV64_ADD(BV64_SUB(BV64_SUB(BV64_MUL(-2, BV32_ZEXT64(local_id_x$1)), BV64_MUL(4, BV32_ZEXT64(local_id_y$1))), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n))), $c0), 126), 64))), 16288), 16384), 16321) then 1 else 0))), BV1_ZEXT32((if BV64_SREM(BV64_ADD(BV64_SUB(BV64_ADD(BV64_MUL(32, BV32_ZEXT64(group_id_x$1)), BV32_ZEXT64(local_id_x$1)), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n))), 8191), 8192) == 0 then 1 else 0)))), BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV1_ZEXT32((if BV64_SLE(BV32_ZEXT64(local_id_x$1), 31) then 1 else 0)), BV1_ZEXT32((if BV64_SGE(BV32_ZEXT64(local_id_x$1), -8160) then 1 else 0))), BV1_ZEXT32((if BV32_UGE(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), 2) then 1 else 0))), BV1_ZEXT32((if BV32_UGE($n, BV32_ADD(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), 1)) then 1 else 0))), BV1_ZEXT32((if BV64_SGE(BV32_SEXT64($n), BV64_ADD(BV64_ADD(BV32_ZEXT64(local_id_x$1), BV64_MUL(32, (if BV64_SLT(BV64_ADD(BV64_ADD(BV64_SUB(0, BV32_ZEXT64(local_id_x$1)), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n))), 30), 0) then BV64_SUB(0, BV64_SDIV(BV64_SUB(BV64_ADD(BV64_SUB(0, BV64_ADD(BV64_ADD(BV64_SUB(0, BV32_ZEXT64(local_id_x$1)), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n))), 30)), 32), 1), 32)) else BV64_SDIV(BV64_ADD(BV64_ADD(BV64_SUB(0, BV32_ZEXT64(local_id_x$1)), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n))), 30), 32)))), 2)) then 1 else 0))), BV1_ZEXT32((if BV64_SGE(BV64_ADD(BV32_ZEXT64(local_id_x$1), BV64_MUL(32, (if BV64_SLT(BV64_ADD(BV64_ADD(BV64_SUB(0, BV32_ZEXT64(local_id_x$1)), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n))), 30), 0) then BV64_SUB(0, BV64_SDIV(BV64_SUB(BV64_ADD(BV64_SUB(0, BV64_ADD(BV64_ADD(BV64_SUB(0, BV32_ZEXT64(local_id_x$1)), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n))), 30)), 32), 1), 32)) else BV64_SDIV(BV64_ADD(BV64_ADD(BV64_SUB(0, BV32_ZEXT64(local_id_x$1)), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n))), 30), 32)))), 1) then 1 else 0))), BV1_ZEXT32((if BV64_SGE(BV64_SREM(BV64_ADD(BV64_ADD(BV64_SUB(0, BV32_ZEXT64(local_id_x$1)), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n))), 30), 32), 30) then 1 else 0))), BV1_ZEXT32((if BV64_SGE(BV32_ZEXT64(BV32_ADD(BV32_ADD(BV32_ADD(BV32_MUL(4, $tsteps), BV32_MUL(2, BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n))), BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n)), 55)), BV64_ADD(BV64_MUL(2, BV64_SREM(BV64_ADD(BV64_ADD(BV64_SUB(0, BV32_ZEXT64(local_id_x$1)), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n))), 30), 32)), $c0)) then 1 else 0))), BV1_ZEXT32((if BV64_SGE(BV64_ADD(BV64_MUL(2, BV64_SREM(BV64_ADD(BV64_ADD(BV64_SUB(0, BV32_ZEXT64(local_id_x$1)), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n))), 30), 32)), $c0), BV32_ZEXT64(BV32_ADD(BV32_ADD(BV32_MUL(2, BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n)), BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n)), 59))) then 1 else 0))), BV1_ZEXT32((if BV64_SGE(BV64_ADD(BV64_ADD(BV64_ADD(BV64_MUL(2, BV64_SREM(BV64_ADD(BV64_ADD(BV64_SUB(0, BV32_ZEXT64(local_id_x$1)), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n))), 30), 32)), BV64_SREM(BV64_ADD(BV64_ADD(BV64_SUB(BV64_SUB(BV64_SUB(BV64_MUL(2, BV64_SREM(BV64_ADD(BV64_ADD(BV64_SUB(0, BV32_ZEXT64(local_id_x$1)), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n))), 30), 32)), BV64_MUL(128, BV32_ZEXT64(group_id_y$1))), BV32_ZEXT64(BV32_MUL(2, BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n)))), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n))), $c0), 32584), 32768)), BV64_MUL(2, BV32_ZEXT64(local_id_x$1))), BV64_MUL(64, BV64_SDIV(BV64_ADD(BV64_ADD(BV64_SUB(0, BV32_ZEXT64(local_id_x$1)), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n))), 30), 32))), BV32_ZEXT64(BV32_ADD(BV32_MUL(2, BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n)), 32703))) then 1 else 0))), BV1_ZEXT32((if BV64_ADD(BV32_ZEXT64(group_id_x$1), BV64_MUL(256, BV64_SDIV(BV64_ADD(BV64_ADD(BV64_SUB(BV64_SUB(0, BV64_SREM(BV64_ADD(BV64_ADD(BV64_SUB(0, BV32_ZEXT64(local_id_x$1)), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n))), 30), 32)), BV64_MUL(32, BV32_ZEXT64(group_id_x$1))), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n))), 8190), 8192))) == BV64_SDIV(BV64_ADD(BV64_ADD(BV64_SUB(0, BV32_ZEXT64(local_id_x$1)), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n))), 30), 32) then 1 else 0))), BV1_ZEXT32((if BV64_SREM(BV64_ADD(BV64_SUB(BV64_ADD(BV64_ADD(BV64_MUL(2, BV32_ZEXT64(local_id_x$1)), BV64_MUL(4, BV32_ZEXT64(local_id_y$1))), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n))), $c0), 63), 64) == 0 then 1 else 0)))), BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV1_ZEXT32((if BV32_UGE(BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n), 3) then 1 else 0)), BV1_ZEXT32((if BV32_UGE($n, BV32_ADD(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), 3)) then 1 else 0))), BV1_ZEXT32((if BV64_SGE(BV32_ZEXT64(BV32_ADD(BV32_ADD(BV32_MUL(2, $n), BV32_MUL(4, $tsteps)), BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n))), BV64_ADD($c0, 7)) then 1 else 0))), BV1_ZEXT32((if BV64_SGE(BV32_ZEXT64(BV32_ADD(BV32_ADD(BV32_MUL(4, $tsteps), BV32_MUL(2, BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n))), BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n))), BV64_ADD($c0, 3)) then 1 else 0))), BV1_ZEXT32((if BV64_SGE($c0, BV32_ZEXT64(BV32_ADD(BV32_ADD(BV32_MUL(2, BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n)), BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n)), 1))) then 1 else 0))), BV1_ZEXT32((if BV64_SGE(BV64_ADD($c0, 3), BV32_ZEXT64(BV32_ADD(BV32_MUL(4, $tsteps), BV32_MUL(2, BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n))))) then 1 else 0))), BV1_ZEXT32((if BV64_SGE(BV64_ADD($c0, 4), BV32_ZEXT64(BV32_ADD(BV32_ADD(BV32_MUL(4, $tsteps), BV32_MUL(2, BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n))), BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n)))) then 1 else 0))), BV1_ZEXT32((if BV64_SGE(BV64_ADD(BV32_ZEXT64(BV32_ADD($tsteps, BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n))), BV64_MUL(8192, BV64_SDIV(BV64_ADD(BV64_ADD(BV64_ADD(BV64_SUB(BV64_ADD(BV64_ADD(BV64_ADD(BV64_MUL(128, BV32_ZEXT64(group_id_x$1)), BV64_MUL(128, BV32_ZEXT64(group_id_y$1))), BV64_MUL(4, BV32_ZEXT64(local_id_x$1))), BV64_MUL(32768, BV32_ZEXT64(local_id_y$1))), BV32_ZEXT64(BV32_MUL(2, BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n)))), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n))), BV64_MUL(32767, $c0)), 32893), 32768))), BV64_ADD(BV64_ADD(BV64_ADD(BV64_ADD(BV64_ADD(BV64_MUL(32, BV32_ZEXT64(group_id_x$1)), BV64_MUL(32, BV32_ZEXT64(group_id_y$1))), BV32_ZEXT64(local_id_x$1)), BV64_MUL(8192, BV32_ZEXT64(local_id_y$1))), BV64_MUL(8192, $c0)), 8193)) then 1 else 0))), BV1_ZEXT32((if BV64_SREM(BV64_ADD(BV64_SUB(BV32_SEXT64($tsteps), BV32_ZEXT64(local_id_y$1)), 15), 16) == 0 then 1 else 0))), BV1_ZEXT32((if BV64_SREM(BV64_SUB(BV64_ADD(BV64_MUL(32, BV32_ZEXT64(group_id_x$1)), BV32_ZEXT64(local_id_x$1)), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n))), 8192) == 0 then 1 else 0)))), BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV1_ZEXT32((if BV64_SGE(BV64_ADD(BV64_MUL(32, BV32_ZEXT64(group_id_x$1)), BV32_ZEXT64(local_id_x$1)), 0) then 1 else 0)), BV1_ZEXT32((if BV64_SLE(BV64_ADD(BV64_MUL(32, BV32_ZEXT64(group_id_x$1)), BV32_ZEXT64(local_id_x$1)), 8191) then 1 else 0))), BV1_ZEXT32((if BV32_UGE(BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n), 2) then 1 else 0))), BV1_ZEXT32((if BV32_UGE($n, BV32_ADD(BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n), 1)) then 1 else 0))), BV1_ZEXT32((if BV32_UGE($n, BV32_ADD(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), 2)) then 1 else 0))), BV1_ZEXT32((if BV32_UGE(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), 1) then 1 else 0))), BV1_ZEXT32((if BV64_SGE(BV32_ZEXT64(BV32_ADD(BV32_ADD(BV32_MUL(4, $tsteps), BV32_MUL(2, BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n))), BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n))), BV64_ADD($c0, 6)) then 1 else 0))), BV1_ZEXT32((if BV64_SGE(BV64_ADD($c0, 2), BV32_ZEXT64(BV32_ADD(BV32_MUL(2, BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n)), BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n)))) then 1 else 0))), BV1_ZEXT32((if BV64_SGE(BV64_SREM(BV64_ADD(BV64_ADD(BV32_ZEXT64(BV32_SUB(BV32_MUL(-2, BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n)), BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n))), $c0), 2), 32768), BV64_MUL(128, BV32_ZEXT64(group_id_y$1))) then 1 else 0))), BV1_ZEXT32((if BV64_SGE(BV64_ADD(BV64_MUL(128, BV32_ZEXT64(group_id_y$1)), 124), BV64_SREM(BV64_ADD(BV64_ADD(BV32_ZEXT64(BV32_SUB(BV32_MUL(-2, BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n)), BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n))), $c0), 2), 32768)) then 1 else 0))), BV1_ZEXT32((if BV64_SREM(BV64_ADD(BV64_SUB(BV64_ADD(BV64_MUL(32, BV32_ZEXT64(group_id_x$1)), BV32_ZEXT64(local_id_x$1)), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n))), 1), 8192) == 0 then 1 else 0))), BV1_ZEXT32((if BV64_SREM(BV64_SUB(BV64_ADD(BV64_ADD(BV64_MUL(2, BV32_ZEXT64(local_id_x$1)), BV64_MUL(4, BV32_ZEXT64(local_id_y$1))), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n))), $c0), 64) == 0 then 1 else 0)))), BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV1_ZEXT32((if BV64_SGE(BV64_ADD(BV64_MUL(32, BV32_ZEXT64(group_id_x$1)), BV32_ZEXT64(local_id_x$1)), 0) then 1 else 0)), BV1_ZEXT32((if BV64_SLE(BV64_ADD(BV64_MUL(32, BV32_ZEXT64(group_id_x$1)), BV32_ZEXT64(local_id_x$1)), 8191) then 1 else 0))), BV1_ZEXT32((if BV32_UGE($n, BV32_ADD(BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n), 3)) then 1 else 0))), BV1_ZEXT32((if BV32_UGE(BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n), 0) then 1 else 0))), BV1_ZEXT32((if BV32_UGE(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), 2) then 1 else 0))), BV1_ZEXT32((if BV32_UGE($n, BV32_ADD(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), 1)) then 1 else 0))), BV1_ZEXT32((if BV64_SGE(BV32_ZEXT64(BV32_ADD(BV32_ADD(BV32_MUL(4, $tsteps), BV32_MUL(2, BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n))), BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n))), BV64_ADD($c0, 3)) then 1 else 0))), BV1_ZEXT32((if BV64_SGE($c0, BV32_ZEXT64(BV32_ADD(BV32_ADD(BV32_MUL(2, BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n)), BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n)), 1))) then 1 else 0))), BV1_ZEXT32((if BV64_SGE(BV64_SREM(BV64_SUB(BV64_ADD(BV32_ZEXT64(BV32_SUB(BV32_MUL(-2, BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n)), BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n))), $c0), 1), 32768), BV64_MUL(128, BV32_ZEXT64(group_id_y$1))) then 1 else 0))), BV1_ZEXT32((if BV64_SGE(BV64_ADD(BV64_MUL(128, BV32_ZEXT64(group_id_y$1)), 124), BV64_SREM(BV64_SUB(BV64_ADD(BV32_ZEXT64(BV32_SUB(BV32_MUL(-2, BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n)), BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n))), $c0), 1), 32768)) then 1 else 0))), BV1_ZEXT32((if BV64_SREM(BV64_ADD(BV64_SUB(BV64_ADD(BV64_MUL(32, BV32_ZEXT64(group_id_x$1)), BV32_ZEXT64(local_id_x$1)), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n))), 8191), 8192) == 0 then 1 else 0))), BV1_ZEXT32((if BV64_SREM(BV64_ADD(BV64_SUB(BV64_ADD(BV64_ADD(BV64_MUL(2, BV32_ZEXT64(local_id_x$1)), BV64_MUL(4, BV32_ZEXT64(local_id_y$1))), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n))), $c0), 63), 64) == 0 then 1 else 0)))), BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV1_ZEXT32((if BV32_UGE(BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n), 3) then 1 else 0)), BV1_ZEXT32((if BV32_UGE($n, BV32_ADD(BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n), 2)) then 1 else 0))), BV1_ZEXT32((if BV32_ADD(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), 2) == $n then 1 else 0))), BV1_ZEXT32((if BV64_SGE(BV32_ZEXT64(BV32_ADD(BV32_ADD($n, BV32_MUL(4, $tsteps)), BV32_MUL(2, BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n)))), BV64_ADD($c0, 10)) then 1 else 0))), BV1_ZEXT32((if BV64_SGE(BV64_ADD($c0, 2), BV32_ZEXT64(BV32_ADD($n, BV32_MUL(2, BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n))))) then 1 else 0))), BV1_ZEXT32((if BV64_SLE(BV64_SREM(BV64_SUB(BV64_ADD(BV64_ADD(BV64_ADD(BV64_ADD(BV64_ADD(BV64_ADD(BV32_SEXT64(BV32_MUL(32769, $n)), BV64_MUL(128, BV32_ZEXT64(group_id_x$1))), BV64_MUL(128, BV32_ZEXT64(group_id_y$1))), BV64_MUL(32772, BV32_ZEXT64(local_id_x$1))), BV64_MUL(32768, BV32_ZEXT64(local_id_y$1))), BV32_ZEXT64(BV32_MUL(32766, BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n)))), BV64_MUL(32767, $c0)), 65413), 32768), 125) then 1 else 0))), BV1_ZEXT32((if BV64_SREM(BV64_SUB(BV64_ADD(BV64_MUL(32, BV32_ZEXT64(group_id_x$1)), BV32_ZEXT64(local_id_x$1)), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n))), 8192) == 0 then 1 else 0))), BV1_ZEXT32((if BV64_SREM(BV64_SUB(BV64_SUB(BV64_ADD(BV64_ADD(BV32_SEXT64($n), BV64_MUL(2, BV32_ZEXT64(local_id_x$1))), BV64_MUL(4, BV32_ZEXT64(local_id_y$1))), $c0), 2), 64) == 0 then 1 else 0)))), BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV1_ZEXT32((if BV32_SGE($tsteps, 2) then 1 else 0)), BV1_ZEXT32((if BV32_ZEXT64(group_id_y$1) == 0 then 1 else 0))), BV1_ZEXT32((if BV32_ZEXT64(local_id_y$1) == 0 then 1 else 0))), BV1_ZEXT32((if BV32_UGE(BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n), 3) then 1 else 0))), BV1_ZEXT32((if BV32_UGE($n, BV32_ADD(BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n), 2)) then 1 else 0))), BV1_ZEXT32((if BV32_UGE($n, BV32_ADD(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), 3)) then 1 else 0))), BV1_ZEXT32((if BV32_UGE(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), 1) then 1 else 0))), BV1_ZEXT32((if $c0 == BV32_ZEXT64(BV32_ADD(BV32_MUL(2, BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n)), BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n))) then 1 else 0))), BV1_ZEXT32((if BV64_SREM(BV64_SUB(BV64_ADD(BV64_MUL(32, BV32_ZEXT64(group_id_x$1)), BV32_ZEXT64(local_id_x$1)), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n))), 8192) == 0 then 1 else 0)))), BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV1_ZEXT32((if BV32_ZEXT64(group_id_x$1) == 0 then 1 else 0)), BV1_ZEXT32((if BV64_SGE(BV32_SEXT64($n), BV64_ADD(BV32_ZEXT64(local_id_x$1), 2)) then 1 else 0))), BV1_ZEXT32((if BV64_SGE(BV32_ZEXT64(local_id_x$1), 1) then 1 else 0))), BV1_ZEXT32((if BV64_SLE(BV32_ZEXT64(local_id_x$1), 2) then 1 else 0))), BV1_ZEXT32((if BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n)) == BV32_ZEXT64(local_id_x$1) then 1 else 0))), BV1_ZEXT32((if BV32_ADD(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), 2) == $n then 1 else 0))), BV1_ZEXT32((if BV64_SGE(BV32_SEXT64(BV32_ADD($n, BV32_MUL(4, $tsteps))), BV64_ADD($c0, 4)) then 1 else 0))), BV1_ZEXT32((if BV64_SGE(BV64_ADD($c0, 2), BV64_ADD(BV32_SEXT64($n), BV64_MUL(2, BV32_ZEXT64(local_id_x$1)))) then 1 else 0))), BV1_ZEXT32((if BV64_SLE(BV64_SREM(BV64_ADD(BV64_ADD(BV64_ADD(BV64_ADD(BV32_SEXT64($n), BV64_MUL(128, BV32_ZEXT64(group_id_y$1))), BV64_MUL(2, BV32_ZEXT64(local_id_x$1))), BV64_MUL(32767, $c0)), 123), 32768), 125) then 1 else 0))), BV1_ZEXT32((if BV64_SREM(BV64_SUB(BV64_SUB(BV64_ADD(BV64_ADD(BV32_SEXT64($n), BV64_MUL(2, BV32_ZEXT64(local_id_x$1))), BV64_MUL(4, BV32_ZEXT64(local_id_y$1))), $c0), 2), 64) == 0 then 1 else 0)))), BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV1_ZEXT32((if BV32_ZEXT64(group_id_x$1) == 0 then 1 else 0)), BV1_ZEXT32((if BV32_ZEXT64(local_id_x$1) == 2 then 1 else 0))), BV1_ZEXT32((if BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n) == 2 then 1 else 0))), BV1_ZEXT32((if BV32_UGE($n, BV32_ADD(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), 3)) then 1 else 0))), BV1_ZEXT32((if BV64_SGE($c0, BV32_ZEXT64(BV32_ADD(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), 5))) then 1 else 0))), BV1_ZEXT32((if BV64_SGE($c0, BV32_SEXT64(BV32_ADD(BV32_MUL(4, $tsteps), 1))) then 1 else 0))), BV1_ZEXT32((if BV64_SGE($c0, BV32_ZEXT64(BV32_ADD(BV32_MUL(4, $tsteps), BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n)))) then 1 else 0))), BV1_ZEXT32((if BV64_SGE(BV32_ZEXT64(BV32_ADD(BV32_ADD(BV32_MUL(4, $tsteps), BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n)), 1)), $c0) then 1 else 0))), BV1_ZEXT32((if BV64_SGE(BV64_ADD(BV32_SEXT64($tsteps), BV64_MUL(8192, BV64_SDIV(BV64_ADD(BV64_ADD(BV64_ADD(BV64_MUL(128, BV32_ZEXT64(group_id_y$1)), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n))), BV64_MUL(32767, $c0)), 129), 32768))), BV64_ADD(BV64_ADD(BV64_MUL(32, BV32_ZEXT64(group_id_y$1)), BV64_MUL(8192, $c0)), 1)) then 1 else 0))), BV1_ZEXT32((if BV64_SREM(BV64_ADD(BV64_SUB(BV32_SEXT64($tsteps), BV32_ZEXT64(local_id_y$1)), 15), 16) == 0 then 1 else 0)))), BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV1_ZEXT32((if BV32_UGE(BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n), 3) then 1 else 0)), BV1_ZEXT32((if BV32_ADD(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), 2) == $n then 1 else 0))), BV1_ZEXT32((if BV64_ADD($c0, 6) == BV32_ZEXT64(BV32_ADD(BV32_ADD($n, BV32_MUL(4, $tsteps)), BV32_MUL(2, BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n)))) then 1 else 0))), BV1_ZEXT32((if BV64_SLE(BV64_SREM(BV64_ADD(BV64_ADD(BV64_ADD(BV64_ADD(BV64_ADD(BV64_ADD(BV32_SEXT64(BV32_MUL(8191, $tsteps)), BV64_MUL(32, BV32_ZEXT64(group_id_x$1))), BV64_MUL(32, BV32_ZEXT64(group_id_y$1))), BV64_MUL(8193, BV32_ZEXT64(local_id_x$1))), BV64_MUL(8192, BV32_ZEXT64(local_id_y$1))), BV32_ZEXT64(BV32_MUL(8191, BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n)))), 8224), 8192), 31) then 1 else 0))), BV1_ZEXT32((if BV64_SREM(BV64_SUB(BV64_ADD(BV64_MUL(32, BV32_ZEXT64(group_id_x$1)), BV32_ZEXT64(local_id_x$1)), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n))), 8192) == 0 then 1 else 0))), BV1_ZEXT32((if BV64_SREM(BV64_ADD(BV64_ADD(BV32_SEXT64(BV32_SUB(0, $tsteps)), BV32_ZEXT64(local_id_y$1)), 1), 16) == 0 then 1 else 0)))), BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV1_ZEXT32((if $tsteps == 1 then 1 else 0)), BV1_ZEXT32((if BV32_ZEXT64(group_id_y$1) == 0 then 1 else 0))), BV1_ZEXT32((if BV32_ZEXT64(local_id_y$1) == 0 then 1 else 0))), BV1_ZEXT32((if BV32_UGE(BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n), 3) then 1 else 0))), BV1_ZEXT32((if BV32_UGE($n, BV32_ADD(BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n), 2)) then 1 else 0))), BV1_ZEXT32((if BV32_UGE($n, BV32_ADD(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), 3)) then 1 else 0))), BV1_ZEXT32((if BV32_UGE(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), 1) then 1 else 0))), BV1_ZEXT32((if $c0 == BV32_ZEXT64(BV32_ADD(BV32_MUL(2, BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n)), BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n))) then 1 else 0))), BV1_ZEXT32((if BV64_SREM(BV64_SUB(BV64_ADD(BV64_MUL(32, BV32_ZEXT64(group_id_x$1)), BV32_ZEXT64(local_id_x$1)), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n))), 8192) == 0 then 1 else 0)))), BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV1_ZEXT32((if BV32_SGE($tsteps, 2) then 1 else 0)), BV1_ZEXT32((if BV32_ZEXT64(group_id_x$1) == 0 then 1 else 0))), BV1_ZEXT32((if BV32_ZEXT64(group_id_y$1) == 0 then 1 else 0))), BV1_ZEXT32((if BV32_ZEXT64(local_id_x$1) == 2 then 1 else 0))), BV1_ZEXT32((if BV32_ZEXT64(local_id_y$1) == 0 then 1 else 0))), BV1_ZEXT32((if BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n) == 2 then 1 else 0))), BV1_ZEXT32((if BV32_UGE($n, BV32_ADD(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), 3)) then 1 else 0))), BV1_ZEXT32((if BV32_UGE(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), 1) then 1 else 0))), BV1_ZEXT32((if $c0 == BV32_ZEXT64(BV32_ADD(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), 4)) then 1 else 0)))), BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV1_ZEXT32((if BV32_ZEXT64(group_id_x$1) == 0 then 1 else 0)), BV1_ZEXT32((if BV32_ZEXT64(group_id_y$1) == 0 then 1 else 0))), BV1_ZEXT32((if BV32_ZEXT64(local_id_x$1) == 1 then 1 else 0))), BV1_ZEXT32((if BV32_ZEXT64(local_id_y$1) == 0 then 1 else 0))), BV1_ZEXT32((if BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n) == 1 then 1 else 0))), BV1_ZEXT32((if BV32_UGE($n, BV32_ADD(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), 3)) then 1 else 0))), BV1_ZEXT32((if $c0 == BV32_ZEXT64(BV32_ADD(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), 2)) then 1 else 0)))), BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV1_ZEXT32((if BV32_ZEXT64(group_id_x$1) == 0 then 1 else 0)), BV1_ZEXT32((if BV32_ZEXT64(local_id_x$1) == 2 then 1 else 0))), BV1_ZEXT32((if BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n) == 2 then 1 else 0))), BV1_ZEXT32((if BV32_ADD(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), 2) == $n then 1 else 0))), BV1_ZEXT32((if BV64_ADD($c0, 2) == BV32_SEXT64(BV32_ADD($n, BV32_MUL(4, $tsteps))) then 1 else 0))), BV1_ZEXT32((if BV64_SGE(BV64_SREM(BV64_ADD(BV64_SUB(BV32_SEXT64($tsteps), BV64_MUL(32, BV32_ZEXT64(group_id_y$1))), 8159), 8192), 8160) then 1 else 0))), BV1_ZEXT32((if BV64_SREM(BV64_ADD(BV64_ADD(BV32_SEXT64(BV32_SUB(0, $tsteps)), BV32_ZEXT64(local_id_y$1)), 1), 16) == 0 then 1 else 0)))), BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV1_ZEXT32((if $tsteps == 1 then 1 else 0)), BV1_ZEXT32((if BV32_ZEXT64(group_id_x$1) == 0 then 1 else 0))), BV1_ZEXT32((if BV32_ZEXT64(group_id_y$1) == 0 then 1 else 0))), BV1_ZEXT32((if BV32_ZEXT64(local_id_x$1) == 2 then 1 else 0))), BV1_ZEXT32((if BV32_ZEXT64(local_id_y$1) == 0 then 1 else 0))), BV1_ZEXT32((if BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n) == 2 then 1 else 0))), BV1_ZEXT32((if BV32_UGE($n, BV32_ADD(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), 3)) then 1 else 0))), BV1_ZEXT32((if BV32_UGE(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), 1) then 1 else 0))), BV1_ZEXT32((if $c0 == BV32_ZEXT64(BV32_ADD(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), 4)) then 1 else 0)))) != 0 then 1 else 0) != 0;
  requires !_READ_HAS_OCCURRED_$$A && !_WRITE_HAS_OCCURRED_$$A && !_ATOMIC_HAS_OCCURRED_$$A;
  requires BV32_SGT(group_size_x, 0);
  requires BV32_SGT(num_groups_x, 0);
  requires BV32_SGE(group_id_x$1, 0);
  requires BV32_SGE(group_id_x$2, 0);
  requires BV32_SLT(group_id_x$1, num_groups_x);
  requires BV32_SLT(group_id_x$2, num_groups_x);
  requires BV32_SGE(local_id_x$1, 0);
  requires BV32_SGE(local_id_x$2, 0);
  requires BV32_SLT(local_id_x$1, group_size_x);
  requires BV32_SLT(local_id_x$2, group_size_x);
  requires BV32_SGT(group_size_y, 0);
  requires BV32_SGT(num_groups_y, 0);
  requires BV32_SGE(group_id_y$1, 0);
  requires BV32_SGE(group_id_y$2, 0);
  requires BV32_SLT(group_id_y$1, num_groups_y);
  requires BV32_SLT(group_id_y$2, num_groups_y);
  requires BV32_SGE(local_id_y$1, 0);
  requires BV32_SGE(local_id_y$2, 0);
  requires BV32_SLT(local_id_y$1, group_size_y);
  requires BV32_SLT(local_id_y$2, group_size_y);
  requires BV32_SGT(group_size_z, 0);
  requires BV32_SGT(num_groups_z, 0);
  requires BV32_SGE(group_id_z$1, 0);
  requires BV32_SGE(group_id_z$2, 0);
  requires BV32_SLT(group_id_z$1, num_groups_z);
  requires BV32_SLT(group_id_z$2, num_groups_z);
  requires BV32_SGE(local_id_z$1, 0);
  requires BV32_SGE(local_id_z$2, 0);
  requires BV32_SLT(local_id_z$1, group_size_z);
  requires BV32_SLT(local_id_z$2, group_size_z);
  requires group_id_x$1 == group_id_x$2 && group_id_y$1 == group_id_y$2 && group_id_z$1 == group_id_z$2 ==> local_id_x$1 != local_id_x$2 || local_id_y$1 != local_id_y$2 || local_id_z$1 != local_id_z$2;
  modifies _READ_HAS_OCCURRED_$$A, _WRITE_HAS_OCCURRED_$$A, _WRITE_READ_BENIGN_FLAG_$$A, _WRITE_READ_BENIGN_FLAG_$$A;



implementation {:source_name "kernel0"} {:kernel} $kernel0($n: int, $tsteps: int, $c0: int)
{
  var $0$1: int;
  var $0$2: int;
  var $1$1: int;
  var $1$2: int;
  var $2$1: int;
  var $2$2: int;
  var $c1.0$1: int;
  var $c1.0$2: int;
  var $3$1: int;
  var $3$2: int;
  var $4$1: int;
  var $4$2: int;
  var $5$1: int;
  var $5$2: int;
  var $6$1: int;
  var $6$2: int;
  var $7$1: int;
  var $7$2: int;
  var $8$1: int;
  var $8$2: int;
  var $9$1: int;
  var $9$2: int;
  var $10$1: int;
  var $10$2: int;
  var $11$1: int;
  var $11$2: int;
  var $12$1: int;
  var $12$2: int;
  var $c2.0$1: int;
  var $c2.0$2: int;
  var $13$1: int;
  var $13$2: int;
  var $14$1: int;
  var $14$2: int;
  var $15$1: int;
  var $15$2: int;
  var $16$1: int;
  var $16$2: int;
  var $17$1: int;
  var $17$2: int;
  var $18$1: int;
  var $18$2: int;
  var $c4.0$1: int;
  var $c4.0$2: int;
  var $19$1: int;
  var $19$2: int;
  var $20$1: int;
  var $20$2: int;
  var $21$1: int;
  var $21$2: int;
  var v0$1: int;
  var v0$2: int;
  var v1$1: int;
  var v1$2: int;
  var v2$1: int;
  var v2$2: int;
  var v3$1: int;
  var v3$2: int;
  var v4$1: bool;
  var v4$2: bool;
  var v5$1: bool;
  var v5$2: bool;
  var v6$1: bool;
  var v6$2: bool;
  var v7$1: bool;
  var v7$2: bool;
  var v8$1: bool;
  var v8$2: bool;
  var v9$1: bool;
  var v9$2: bool;
  var v10$1: bool;
  var v10$2: bool;
  var v11$1: bool;
  var v11$2: bool;
  var v12$1: bool;
  var v12$2: bool;
  var v13$1: bool;
  var v13$2: bool;
  var v14$1: bool;
  var v14$2: bool;
  var v15$1: bool;
  var v15$2: bool;
  var v16$1: bool;
  var v16$2: bool;
  var v17$1: bool;
  var v17$2: bool;
  var v18$1: bool;
  var v18$2: bool;
  var v19$1: bool;
  var v19$2: bool;
  var v20$1: bool;
  var v20$2: bool;
  var v21$1: bool;
  var v21$2: bool;
  var v22$1: bool;
  var v22$2: bool;
  var v23$1: bool;
  var v23$2: bool;
  var v24$1: bool;
  var v24$2: bool;
  var v25$1: bool;
  var v25$2: bool;
  var v26$1: bool;
  var v26$2: bool;
  var v27$1: bool;
  var v27$2: bool;
  var v28$1: bool;
  var v28$2: bool;
  var v29$1: bool;
  var v29$2: bool;
  var v30$1: bool;
  var v30$2: bool;
  var v31$1: int;
  var v31$2: int;
  var v32$1: int;
  var v32$2: int;
  var v33$1: int;
  var v33$2: int;
  var v34$1: int;
  var v34$2: int;
  var v35$1: int;
  var v35$2: int;
  var v36$1: int;
  var v36$2: int;
  var v37$1: int;
  var v37$2: int;
  var v38$1: int;
  var v38$2: int;
  var v39$1: int;
  var v39$2: int;
  var p0$1: bool;
  var p0$2: bool;
  var p1$1: bool;
  var p1$2: bool;
  var p2$1: bool;
  var p2$2: bool;
  var p3$1: bool;
  var p3$2: bool;
  var p4$1: bool;
  var p4$2: bool;
  var p5$1: bool;
  var p5$2: bool;
  var p6$1: bool;
  var p6$2: bool;
  var p7$1: bool;
  var p7$2: bool;
  var p8$1: bool;
  var p8$2: bool;
  var p9$1: bool;
  var p9$2: bool;
  var p10$1: bool;
  var p10$2: bool;
  var p11$1: bool;
  var p11$2: bool;
  var p12$1: bool;
  var p12$2: bool;
  var p13$1: bool;
  var p13$2: bool;
  var p14$1: bool;
  var p14$2: bool;
  var p15$1: bool;
  var p15$2: bool;
  var p16$1: bool;
  var p16$2: bool;
  var p17$1: bool;
  var p17$2: bool;
  var p18$1: bool;
  var p18$2: bool;
  var p19$1: bool;
  var p19$2: bool;
  var p20$1: bool;
  var p20$2: bool;
  var p21$1: bool;
  var p21$2: bool;
  var p22$1: bool;
  var p22$2: bool;
  var p23$1: bool;
  var p23$2: bool;
  var p24$1: bool;
  var p24$2: bool;
  var p25$1: bool;
  var p25$2: bool;
  var p26$1: bool;
  var p26$2: bool;
  var p27$1: bool;
  var p27$2: bool;
  var p28$1: bool;
  var p28$2: bool;
  var p29$1: bool;
  var p29$2: bool;
  var p30$1: bool;
  var p30$2: bool;
  var p31$1: bool;
  var p31$2: bool;
  var p32$1: bool;
  var p32$2: bool;
  var p33$1: bool;
  var p33$2: bool;
  var p34$1: bool;
  var p34$2: bool;
  var p35$1: bool;
  var p35$2: bool;
  var p36$1: bool;
  var p36$2: bool;
  var p37$1: bool;
  var p37$2: bool;
  var p38$1: bool;
  var p38$2: bool;
  var p39$1: bool;
  var p39$2: bool;
  var p40$1: bool;
  var p40$2: bool;
  var p41$1: bool;
  var p41$2: bool;
  var p42$1: bool;
  var p42$2: bool;
  var p43$1: bool;
  var p43$2: bool;
  var p44$1: bool;
  var p44$2: bool;
  var p45$1: bool;
  var p45$2: bool;
  var p46$1: bool;
  var p46$2: bool;
  var p47$1: bool;
  var p47$2: bool;
  var p48$1: bool;
  var p48$2: bool;
  var p49$1: bool;
  var p49$2: bool;
  var p50$1: bool;
  var p50$2: bool;
  var p51$1: bool;
  var p51$2: bool;
  var p52$1: bool;
  var p52$2: bool;
  var p53$1: bool;
  var p53$2: bool;
  var p54$1: bool;
  var p54$2: bool;
  var _READ_HAS_OCCURRED_$$A$ghost$$44: bool;
  var _WRITE_HAS_OCCURRED_$$A$ghost$$44: bool;
  var _READ_HAS_OCCURRED_$$A$ghost$$64: bool;
  var _WRITE_HAS_OCCURRED_$$A$ghost$$64: bool;


  $0:
    v0$1 := BV32_ZEXT64(group_id_x$1);
    v0$2 := BV32_ZEXT64(group_id_x$2);
    v1$1 := BV32_ZEXT64(group_id_y$1);
    v1$2 := BV32_ZEXT64(group_id_y$2);
    v2$1 := BV32_ZEXT64(local_id_x$1);
    v2$2 := BV32_ZEXT64(local_id_x$2);
    v3$1 := BV32_ZEXT64(local_id_y$1);
    v3$2 := BV32_ZEXT64(local_id_y$2);
    v4$1 := BV64_SLT(BV64_SUB(BV64_ADD(BV64_SUB(BV32_SEXT64(BV32_SUB(BV32_SUB(0, $n), BV32_MUL(4, $tsteps))), BV64_MUL(64, v0$1)), $c0), 57), 0);
    v4$2 := BV64_SLT(BV64_SUB(BV64_ADD(BV64_SUB(BV32_SEXT64(BV32_SUB(BV32_SUB(0, $n), BV32_MUL(4, $tsteps))), BV64_MUL(64, v0$2)), $c0), 57), 0);
    p0$1 := false;
    p0$2 := false;
    p1$1 := false;
    p1$2 := false;
    p2$1 := false;
    p2$2 := false;
    p3$1 := false;
    p3$2 := false;
    p4$1 := false;
    p4$2 := false;
    p5$1 := false;
    p5$2 := false;
    p6$1 := false;
    p6$2 := false;
    p0$1 := (if v4$1 then v4$1 else p0$1);
    p0$2 := (if v4$2 then v4$2 else p0$2);
    p1$1 := (if !v4$1 then !v4$1 else p1$1);
    p1$2 := (if !v4$2 then !v4$2 else p1$2);
    $0$1 := (if p0$1 then BV64_SUB(0, BV64_SDIV(BV64_SUB(BV64_ADD(BV64_SUB(0, BV64_SUB(BV64_ADD(BV64_SUB(BV32_SEXT64(BV32_SUB(BV32_SUB(0, $n), BV32_MUL(4, $tsteps))), BV64_MUL(64, v0$1)), $c0), 57)), 16384), 1), 16384)) else $0$1);
    $0$2 := (if p0$2 then BV64_SUB(0, BV64_SDIV(BV64_SUB(BV64_ADD(BV64_SUB(0, BV64_SUB(BV64_ADD(BV64_SUB(BV32_SEXT64(BV32_SUB(BV32_SUB(0, $n), BV32_MUL(4, $tsteps))), BV64_MUL(64, v0$2)), $c0), 57)), 16384), 1), 16384)) else $0$2);
    $0$1 := (if p1$1 then BV64_SDIV(BV64_SUB(BV64_ADD(BV64_SUB(BV32_SEXT64(BV32_SUB(BV32_SUB(0, $n), BV32_MUL(4, $tsteps))), BV64_MUL(64, v0$1)), $c0), 57), 16384) else $0$1);
    $0$2 := (if p1$2 then BV64_SDIV(BV64_SUB(BV64_ADD(BV64_SUB(BV32_SEXT64(BV32_SUB(BV32_SUB(0, $n), BV32_MUL(4, $tsteps))), BV64_MUL(64, v0$2)), $c0), 57), 16384) else $0$2);
    v5$1 := BV64_SGT(BV64_MUL(32, v0$1), BV64_ADD(BV64_ADD(BV64_MUL(32, v0$1), BV64_MUL(8192, $0$1)), 8192));
    v5$2 := BV64_SGT(BV64_MUL(32, v0$2), BV64_ADD(BV64_ADD(BV64_MUL(32, v0$2), BV64_MUL(8192, $0$2)), 8192));
    p2$1 := (if v5$1 then v5$1 else p2$1);
    p2$2 := (if v5$2 then v5$2 else p2$2);
    p3$1 := (if !v5$1 then !v5$1 else p3$1);
    p3$2 := (if !v5$2 then !v5$2 else p3$2);
    $1$1 := (if p2$1 then BV64_MUL(32, v0$1) else $1$1);
    $1$2 := (if p2$2 then BV64_MUL(32, v0$2) else $1$2);
    v6$1 := (if p3$1 then BV64_SLT(BV64_SUB(BV64_ADD(BV64_SUB(BV32_SEXT64(BV32_SUB(BV32_SUB(0, $n), BV32_MUL(4, $tsteps))), BV64_MUL(64, v0$1)), $c0), 57), 0) else v6$1);
    v6$2 := (if p3$2 then BV64_SLT(BV64_SUB(BV64_ADD(BV64_SUB(BV32_SEXT64(BV32_SUB(BV32_SUB(0, $n), BV32_MUL(4, $tsteps))), BV64_MUL(64, v0$2)), $c0), 57), 0) else v6$2);
    p4$1 := (if p3$1 && v6$1 then v6$1 else p4$1);
    p4$2 := (if p3$2 && v6$2 then v6$2 else p4$2);
    p5$1 := (if p3$1 && !v6$1 then !v6$1 else p5$1);
    p5$2 := (if p3$2 && !v6$2 then !v6$2 else p5$2);
    $2$1 := (if p4$1 then BV64_SUB(0, BV64_SDIV(BV64_SUB(BV64_ADD(BV64_SUB(0, BV64_SUB(BV64_ADD(BV64_SUB(BV32_SEXT64(BV32_SUB(BV32_SUB(0, $n), BV32_MUL(4, $tsteps))), BV64_MUL(64, v0$1)), $c0), 57)), 16384), 1), 16384)) else $2$1);
    $2$2 := (if p4$2 then BV64_SUB(0, BV64_SDIV(BV64_SUB(BV64_ADD(BV64_SUB(0, BV64_SUB(BV64_ADD(BV64_SUB(BV32_SEXT64(BV32_SUB(BV32_SUB(0, $n), BV32_MUL(4, $tsteps))), BV64_MUL(64, v0$2)), $c0), 57)), 16384), 1), 16384)) else $2$2);
    $2$1 := (if p5$1 then BV64_SDIV(BV64_SUB(BV64_ADD(BV64_SUB(BV32_SEXT64(BV32_SUB(BV32_SUB(0, $n), BV32_MUL(4, $tsteps))), BV64_MUL(64, v0$1)), $c0), 57), 16384) else $2$1);
    $2$2 := (if p5$2 then BV64_SDIV(BV64_SUB(BV64_ADD(BV64_SUB(BV32_SEXT64(BV32_SUB(BV32_SUB(0, $n), BV32_MUL(4, $tsteps))), BV64_MUL(64, v0$2)), $c0), 57), 16384) else $2$2);
    $1$1 := (if p3$1 then BV64_ADD(BV64_ADD(BV64_MUL(32, v0$1), BV64_MUL(8192, $2$1)), 8192) else $1$1);
    $1$2 := (if p3$2 then BV64_ADD(BV64_ADD(BV64_MUL(32, v0$2), BV64_MUL(8192, $2$2)), 8192) else $1$2);
    $c1.0$1 := $1$1;
    $c1.0$2 := $1$2;
    p6$1 := true;
    p6$2 := true;
    assume {:captureState "loop_entry_state_0_0"} true;
    goto $10;

  $10:
    assume {:captureState "loop_head_state_0"} true;
    
    
    
    
    
    
    
    
    
    
    
    
    
    
assert  my_inv (  ( p6$1 ==> $c1.0$1 mod 8192 == $1$1 mod 8192 )  && ( p6$2 ==> $c1.0$2 mod 8192 == $1$2 mod 8192 ) ,  ( p6$1 ==> BV32_SLE($c1.0$1, $1$1) )  && ( p6$2 ==> BV32_SLE($c1.0$2, $1$2) ) ,  ( p6$1 ==> BV32_SGE($c1.0$1, $1$1) )  && ( p6$2 ==> BV32_SGE($c1.0$2, $1$2) ) ,  ( p6$1 ==> BV32_ULE($c1.0$1, $1$1) )  && ( p6$2 ==> BV32_ULE($c1.0$2, $1$2) ) ,  ( p6$1 ==> BV32_UGE($c1.0$1, $1$1) )  && ( p6$2 ==> BV32_UGE($c1.0$2, $1$2) ) ,  (  BV64_SLT($c1.0$1, $3$1) ==> p6$1 )  && (  BV64_SLT($c1.0$2, $3$2) ==> p6$2 ) ,  true ,  true ,  true ,  true ,  true ,  true ,  true ,  true ,  true ,  true ,  true ,  true ,  true ,  true ,  true ,  true ,  true ,  true ,  true ,  true ,  true ,  true ,  true ,  true ,  true ,  true ,  true ,  true ,  (  _READ_HAS_OCCURRED_$$A ==> _WATCHED_OFFSET mod 1 == 0 mod 1 || _WATCHED_OFFSET mod 1 == 0 mod 1 || _WATCHED_OFFSET mod 1 == 0 mod 1 || _WATCHED_OFFSET mod 1 == 0 mod 1 || _WATCHED_OFFSET mod 1 == 0 mod 1 || _WATCHED_OFFSET mod 1 == 0 mod 1 || _WATCHED_OFFSET mod 1 == 0 mod 1 || _WATCHED_OFFSET mod 1 == 0 mod 1 || _WATCHED_OFFSET mod 1 == 0 mod 1 ) ,  (  _WRITE_HAS_OCCURRED_$$A ==> _WATCHED_OFFSET mod 1 == 0 mod 1 ) ,  true ,  true ,  true ,  true ,  true ,  true ,  true ,  true  ); 


    assert {:procedure_wide_invariant} {:do_not_predicate} {:sourceloc_num 5} {:thread 1} (if _READ_HAS_OCCURRED_$$A ==> BV32_OR(BV32_OR(BV32_OR(BV32_OR(BV32_OR(BV32_OR(BV32_OR(BV32_OR(BV32_OR(BV32_OR(BV32_OR(BV32_OR(BV32_OR(BV32_OR(BV32_OR(BV32_OR(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV1_ZEXT32((if BV32_UGE(BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n), 3) then 1 else 0)), BV1_ZEXT32((if BV32_UGE($n, BV32_ADD(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), 3)) then 1 else 0))), BV1_ZEXT32((if BV64_SGE(BV32_ZEXT64(BV32_ADD(BV32_ADD(BV32_MUL(4, $tsteps), BV32_MUL(2, BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n))), BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n))), BV64_ADD($c0, 5)) then 1 else 0))), BV1_ZEXT32((if BV64_SGE(BV32_SEXT64($n), BV64_ADD(BV64_ADD(BV64_SREM(BV64_ADD(BV64_ADD(BV64_ADD(BV64_MUL(32, BV32_ZEXT64(group_id_x$1)), BV32_ZEXT64(local_id_x$1)), BV32_ZEXT64(BV32_MUL(8191, BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n)))), 1), 8192), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n))), 1)) then 1 else 0))), BV1_ZEXT32((if BV64_SLE(BV64_SREM(BV64_ADD(BV64_ADD(BV64_ADD(BV64_MUL(32, BV32_ZEXT64(group_id_x$1)), BV32_ZEXT64(local_id_x$1)), BV32_ZEXT64(BV32_MUL(8191, BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n)))), 1), 8192), 1) then 1 else 0))), BV1_ZEXT32((if BV64_SGE(BV64_ADD($c0, 1), BV64_ADD(BV64_ADD(BV64_MUL(2, BV64_SREM(BV64_ADD(BV64_ADD(BV64_ADD(BV64_MUL(32, BV32_ZEXT64(group_id_x$1)), BV32_ZEXT64(local_id_x$1)), BV32_ZEXT64(BV32_MUL(8191, BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n)))), 1), 8192)), BV32_ZEXT64(BV32_MUL(2, BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n)))), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n)))) then 1 else 0))), BV1_ZEXT32((if BV64_SGE(BV64_MUL(64, (if BV64_SLT(BV64_ADD(BV64_ADD(BV64_ADD(BV64_ADD(BV64_MUL(2, BV32_ZEXT64(local_id_x$1)), BV64_MUL(4, BV32_ZEXT64(local_id_y$1))), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n))), BV64_MUL(63, $c0)), 1), 0) then BV64_SUB(0, BV64_SDIV(BV64_SUB(BV64_ADD(BV64_SUB(0, BV64_ADD(BV64_ADD(BV64_ADD(BV64_ADD(BV64_MUL(2, BV32_ZEXT64(local_id_x$1)), BV64_MUL(4, BV32_ZEXT64(local_id_y$1))), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n))), BV64_MUL(63, $c0)), 1)), 64), 1), 64)) else BV64_SDIV(BV64_ADD(BV64_ADD(BV64_ADD(BV64_ADD(BV64_MUL(2, BV32_ZEXT64(local_id_x$1)), BV64_MUL(4, BV32_ZEXT64(local_id_y$1))), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n))), BV64_MUL(63, $c0)), 1), 64))), BV64_ADD(BV64_ADD(BV64_ADD(BV64_MUL(2, BV32_ZEXT64(local_id_x$1)), BV64_MUL(4, BV32_ZEXT64(local_id_y$1))), BV64_MUL(63, $c0)), 1)) then 1 else 0))), BV1_ZEXT32((if BV64_SGE(BV64_SREM(BV64_ADD(BV64_ADD(BV64_ADD(BV64_MUL(32, BV32_ZEXT64(group_id_x$1)), BV32_ZEXT64(local_id_x$1)), BV32_ZEXT64(BV32_MUL(8191, BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n)))), 1), 8192), BV64_SREM(BV64_ADD(BV64_ADD(BV64_ADD(BV64_ADD(BV64_MUL(2, BV32_ZEXT64(local_id_x$1)), BV64_MUL(4, BV32_ZEXT64(local_id_y$1))), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n))), BV64_MUL(63, $c0)), 1), 64)) then 1 else 0))), BV1_ZEXT32((if BV64_SGE(BV64_ADD(BV32_ZEXT64(local_id_y$1), BV64_MUL(8192, BV64_SDIV(BV64_ADD(BV64_ADD(BV64_ADD(BV64_ADD(BV64_ADD(BV64_ADD(BV64_ADD(BV64_SUB(0, BV64_MUL(2, BV64_SREM(BV64_ADD(BV64_ADD(BV64_ADD(BV64_MUL(32, BV32_ZEXT64(group_id_x$1)), BV32_ZEXT64(local_id_x$1)), BV32_ZEXT64(BV32_MUL(8191, BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n)))), 1), 8192))), BV64_MUL(128, BV32_ZEXT64(group_id_x$1))), BV64_MUL(128, BV32_ZEXT64(group_id_y$1))), BV64_MUL(4, BV32_ZEXT64(local_id_x$1))), BV32_ZEXT64(BV32_MUL(32766, BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n)))), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n))), BV64_MUL(32767, $c0)), 127), 32768))), BV64_ADD(BV64_ADD(BV64_ADD(BV64_ADD(BV64_ADD(BV64_MUL(16, BV32_ZEXT64(group_id_x$1)), BV64_MUL(32, BV32_ZEXT64(group_id_y$1))), BV32_ZEXT64(BV32_MUL(4096, BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n)))), BV64_MUL(8176, $c0)), BV64_MUL(4096, BV64_SDIV(BV64_ADD(BV64_ADD(BV64_ADD(BV64_MUL(32, BV32_ZEXT64(group_id_x$1)), BV32_ZEXT64(local_id_x$1)), BV32_ZEXT64(BV32_MUL(8191, BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n)))), 1), 8192))), BV64_MUL(16, BV64_SDIV(BV64_ADD(BV64_ADD(BV64_ADD(BV64_ADD(BV64_MUL(2, BV32_ZEXT64(local_id_x$1)), BV64_MUL(4, BV32_ZEXT64(local_id_y$1))), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n))), BV64_MUL(63, $c0)), 1), 64)))) then 1 else 0))), BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV1_ZEXT32((if BV32_ZEXT64(group_id_x$1) == 0 then 1 else 0)), BV1_ZEXT32((if BV64_SGE(BV32_SEXT64($n), BV64_ADD(BV32_ZEXT64(local_id_x$1), 2)) then 1 else 0))), BV1_ZEXT32((if BV64_SGE(BV32_ZEXT64(local_id_x$1), 1) then 1 else 0))), BV1_ZEXT32((if BV64_SGE(BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n)), BV32_ZEXT64(local_id_x$1)) then 1 else 0))), BV1_ZEXT32((if BV32_ULE(BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n), 2) then 1 else 0))), BV1_ZEXT32((if BV32_UGE($n, BV32_ADD(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), 3)) then 1 else 0))), BV1_ZEXT32((if BV64_SGE($c0, BV64_ADD(BV64_ADD(BV64_MUL(2, BV32_ZEXT64(local_id_x$1)), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n))), 1)) then 1 else 0))), BV1_ZEXT32((if BV64_SGE(BV32_ZEXT64(BV32_ADD(BV32_MUL(4, $tsteps), BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n))), BV64_ADD($c0, 1)) then 1 else 0))), BV1_ZEXT32((if BV64_SGE(BV64_MUL(64, (if BV64_SLT(BV64_ADD(BV64_ADD(BV64_ADD(BV64_ADD(BV64_MUL(2, BV32_ZEXT64(local_id_x$1)), BV64_MUL(4, BV32_ZEXT64(local_id_y$1))), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n))), BV64_MUL(63, $c0)), 1), 0) then BV64_SUB(0, BV64_SDIV(BV64_SUB(BV64_ADD(BV64_SUB(0, BV64_ADD(BV64_ADD(BV64_ADD(BV64_ADD(BV64_MUL(2, BV32_ZEXT64(local_id_x$1)), BV64_MUL(4, BV32_ZEXT64(local_id_y$1))), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n))), BV64_MUL(63, $c0)), 1)), 64), 1), 64)) else BV64_SDIV(BV64_ADD(BV64_ADD(BV64_ADD(BV64_ADD(BV64_MUL(2, BV32_ZEXT64(local_id_x$1)), BV64_MUL(4, BV32_ZEXT64(local_id_y$1))), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n))), BV64_MUL(63, $c0)), 1), 64))), BV64_ADD(BV64_ADD(BV64_ADD(BV64_MUL(2, BV32_ZEXT64(local_id_x$1)), BV64_MUL(4, BV32_ZEXT64(local_id_y$1))), BV64_MUL(63, $c0)), 1)) then 1 else 0))), BV1_ZEXT32((if BV64_SGE(BV64_ADD(BV32_ZEXT64(local_id_x$1), 1), BV64_ADD(BV64_SREM(BV64_ADD(BV64_ADD(BV64_ADD(BV64_ADD(BV64_MUL(2, BV32_ZEXT64(local_id_x$1)), BV64_MUL(4, BV32_ZEXT64(local_id_y$1))), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n))), BV64_MUL(63, $c0)), 1), 64), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n)))) then 1 else 0))), BV1_ZEXT32((if BV64_SGE(BV64_ADD(BV32_ZEXT64(local_id_y$1), BV64_MUL(8192, BV64_SDIV(BV64_ADD(BV64_ADD(BV64_ADD(BV64_ADD(BV64_MUL(128, BV32_ZEXT64(group_id_y$1)), BV64_MUL(2, BV32_ZEXT64(local_id_x$1))), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n))), BV64_MUL(32767, $c0)), 125), 32768))), BV64_ADD(BV64_ADD(BV64_MUL(32, BV32_ZEXT64(group_id_y$1)), BV64_MUL(8176, $c0)), BV64_MUL(16, BV64_SDIV(BV64_ADD(BV64_ADD(BV64_ADD(BV64_ADD(BV64_MUL(2, BV32_ZEXT64(local_id_x$1)), BV64_MUL(4, BV32_ZEXT64(local_id_y$1))), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n))), BV64_MUL(63, $c0)), 1), 64)))) then 1 else 0)))), BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV1_ZEXT32((if BV64_SGE(BV64_ADD(BV64_MUL(32, BV32_ZEXT64(group_id_x$1)), BV32_ZEXT64(local_id_x$1)), 0) then 1 else 0)), BV1_ZEXT32((if BV64_SLE(BV64_ADD(BV64_MUL(32, BV32_ZEXT64(group_id_x$1)), BV32_ZEXT64(local_id_x$1)), 8191) then 1 else 0))), BV1_ZEXT32((if BV32_UGE($n, BV32_ADD(BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n), 3)) then 1 else 0))), BV1_ZEXT32((if BV32_UGE(BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n), 0) then 1 else 0))), BV1_ZEXT32((if BV64_SGE(BV64_ADD(BV64_ADD(BV32_ZEXT64(local_id_x$1), BV64_MUL(2, BV32_ZEXT64(local_id_y$1))), BV64_MUL(32, (if BV64_SLT(BV64_ADD(BV64_ADD(BV64_SUB(BV64_SUB(BV64_MUL(-2, BV32_ZEXT64(local_id_x$1)), BV64_MUL(4, BV32_ZEXT64(local_id_y$1))), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n))), $c0), 62), 0) then BV64_SUB(0, BV64_SDIV(BV64_SUB(BV64_ADD(BV64_SUB(0, BV64_ADD(BV64_ADD(BV64_SUB(BV64_SUB(BV64_MUL(-2, BV32_ZEXT64(local_id_x$1)), BV64_MUL(4, BV32_ZEXT64(local_id_y$1))), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n))), $c0), 62)), 64), 1), 64)) else BV64_SDIV(BV64_ADD(BV64_ADD(BV64_SUB(BV64_SUB(BV64_MUL(-2, BV32_ZEXT64(local_id_x$1)), BV64_MUL(4, BV32_ZEXT64(local_id_y$1))), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n))), $c0), 62), 64)))), BV32_ZEXT64(BV32_ADD(BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n), 1))) then 1 else 0))), BV1_ZEXT32((if BV64_SGE(BV32_ZEXT64(BV32_ADD(BV32_MUL(2, $tsteps), BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n))), BV64_ADD(BV64_ADD(BV64_ADD(BV32_ZEXT64(local_id_x$1), BV64_MUL(2, BV32_ZEXT64(local_id_y$1))), BV64_MUL(32, (if BV64_SLT(BV64_ADD(BV64_ADD(BV64_SUB(BV64_SUB(BV64_MUL(-2, BV32_ZEXT64(local_id_x$1)), BV64_MUL(4, BV32_ZEXT64(local_id_y$1))), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n))), $c0), 62), 0) then BV64_SUB(0, BV64_SDIV(BV64_SUB(BV64_ADD(BV64_SUB(0, BV64_ADD(BV64_ADD(BV64_SUB(BV64_SUB(BV64_MUL(-2, BV32_ZEXT64(local_id_x$1)), BV64_MUL(4, BV32_ZEXT64(local_id_y$1))), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n))), $c0), 62)), 64), 1), 64)) else BV64_SDIV(BV64_ADD(BV64_ADD(BV64_SUB(BV64_SUB(BV64_MUL(-2, BV32_ZEXT64(local_id_x$1)), BV64_MUL(4, BV32_ZEXT64(local_id_y$1))), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n))), $c0), 62), 64)))), 1)) then 1 else 0))), BV1_ZEXT32((if BV64_SGE(BV64_ADD(BV64_ADD(BV64_ADD(BV32_SEXT64($n), BV64_MUL(2, BV32_ZEXT64(local_id_x$1))), BV64_MUL(4, BV32_ZEXT64(local_id_y$1))), BV64_MUL(64, (if BV64_SLT(BV64_ADD(BV64_ADD(BV64_SUB(BV64_SUB(BV64_MUL(-2, BV32_ZEXT64(local_id_x$1)), BV64_MUL(4, BV32_ZEXT64(local_id_y$1))), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n))), $c0), 62), 0) then BV64_SUB(0, BV64_SDIV(BV64_SUB(BV64_ADD(BV64_SUB(0, BV64_ADD(BV64_ADD(BV64_SUB(BV64_SUB(BV64_MUL(-2, BV32_ZEXT64(local_id_x$1)), BV64_MUL(4, BV32_ZEXT64(local_id_y$1))), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n))), $c0), 62)), 64), 1), 64)) else BV64_SDIV(BV64_ADD(BV64_ADD(BV64_SUB(BV64_SUB(BV64_MUL(-2, BV32_ZEXT64(local_id_x$1)), BV64_MUL(4, BV32_ZEXT64(local_id_y$1))), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n))), $c0), 62), 64)))), BV64_ADD($c0, 2)) then 1 else 0))), BV1_ZEXT32((if BV64_SGE($c0, BV64_ADD(BV64_ADD(BV64_ADD(BV64_MUL(2, BV32_ZEXT64(local_id_x$1)), BV64_MUL(4, BV32_ZEXT64(local_id_y$1))), BV64_MUL(64, (if BV64_SLT(BV64_ADD(BV64_ADD(BV64_SUB(BV64_SUB(BV64_MUL(-2, BV32_ZEXT64(local_id_x$1)), BV64_MUL(4, BV32_ZEXT64(local_id_y$1))), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n))), $c0), 62), 0) then BV64_SUB(0, BV64_SDIV(BV64_SUB(BV64_ADD(BV64_SUB(0, BV64_ADD(BV64_ADD(BV64_SUB(BV64_SUB(BV64_MUL(-2, BV32_ZEXT64(local_id_x$1)), BV64_MUL(4, BV32_ZEXT64(local_id_y$1))), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n))), $c0), 62)), 64), 1), 64)) else BV64_SDIV(BV64_ADD(BV64_ADD(BV64_SUB(BV64_SUB(BV64_MUL(-2, BV32_ZEXT64(local_id_x$1)), BV64_MUL(4, BV32_ZEXT64(local_id_y$1))), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n))), $c0), 62), 64)))), 1)) then 1 else 0))), BV1_ZEXT32((if BV64_SGE($c0, BV64_ADD(BV64_ADD(BV64_ADD(BV64_MUL(2, BV32_ZEXT64(local_id_x$1)), BV64_MUL(4, BV32_ZEXT64(local_id_y$1))), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n))), BV64_MUL(64, (if BV64_SLT(BV64_ADD(BV64_ADD(BV64_SUB(BV64_SUB(BV64_MUL(-2, BV32_ZEXT64(local_id_x$1)), BV64_MUL(4, BV32_ZEXT64(local_id_y$1))), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n))), $c0), 62), 0) then BV64_SUB(0, BV64_SDIV(BV64_SUB(BV64_ADD(BV64_SUB(0, BV64_ADD(BV64_ADD(BV64_SUB(BV64_SUB(BV64_MUL(-2, BV32_ZEXT64(local_id_x$1)), BV64_MUL(4, BV32_ZEXT64(local_id_y$1))), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n))), $c0), 62)), 64), 1), 64)) else BV64_SDIV(BV64_ADD(BV64_ADD(BV64_SUB(BV64_SUB(BV64_MUL(-2, BV32_ZEXT64(local_id_x$1)), BV64_MUL(4, BV32_ZEXT64(local_id_y$1))), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n))), $c0), 62), 64))))) then 1 else 0))), BV1_ZEXT32((if BV64_SGE(BV64_SREM(BV64_ADD(BV64_ADD(BV64_SUB(BV64_ADD(BV64_ADD(BV64_MUL(-64, BV32_ZEXT64(group_id_y$1)), BV32_ZEXT64(local_id_x$1)), BV64_MUL(2, BV32_ZEXT64(local_id_y$1))), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n))), BV64_MUL(32, BV64_SDIV(BV64_ADD(BV64_ADD(BV64_SUB(BV64_SUB(BV64_MUL(-2, BV32_ZEXT64(local_id_x$1)), BV64_MUL(4, BV32_ZEXT64(local_id_y$1))), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n))), $c0), 126), 64))), 16288), 16384), 16321) then 1 else 0))), BV1_ZEXT32((if BV64_SREM(BV64_ADD(BV64_SUB(BV64_ADD(BV64_MUL(32, BV32_ZEXT64(group_id_x$1)), BV32_ZEXT64(local_id_x$1)), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n))), 8191), 8192) == 0 then 1 else 0)))), BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV1_ZEXT32((if BV64_SLE(BV32_ZEXT64(local_id_x$1), 31) then 1 else 0)), BV1_ZEXT32((if BV64_SGE(BV32_ZEXT64(local_id_x$1), -8160) then 1 else 0))), BV1_ZEXT32((if BV32_UGE(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), 2) then 1 else 0))), BV1_ZEXT32((if BV32_UGE($n, BV32_ADD(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), 1)) then 1 else 0))), BV1_ZEXT32((if BV64_SGE(BV32_SEXT64($n), BV64_ADD(BV64_ADD(BV32_ZEXT64(local_id_x$1), BV64_MUL(32, (if BV64_SLT(BV64_ADD(BV64_ADD(BV64_SUB(0, BV32_ZEXT64(local_id_x$1)), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n))), 30), 0) then BV64_SUB(0, BV64_SDIV(BV64_SUB(BV64_ADD(BV64_SUB(0, BV64_ADD(BV64_ADD(BV64_SUB(0, BV32_ZEXT64(local_id_x$1)), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n))), 30)), 32), 1), 32)) else BV64_SDIV(BV64_ADD(BV64_ADD(BV64_SUB(0, BV32_ZEXT64(local_id_x$1)), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n))), 30), 32)))), 2)) then 1 else 0))), BV1_ZEXT32((if BV64_SGE(BV64_ADD(BV32_ZEXT64(local_id_x$1), BV64_MUL(32, (if BV64_SLT(BV64_ADD(BV64_ADD(BV64_SUB(0, BV32_ZEXT64(local_id_x$1)), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n))), 30), 0) then BV64_SUB(0, BV64_SDIV(BV64_SUB(BV64_ADD(BV64_SUB(0, BV64_ADD(BV64_ADD(BV64_SUB(0, BV32_ZEXT64(local_id_x$1)), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n))), 30)), 32), 1), 32)) else BV64_SDIV(BV64_ADD(BV64_ADD(BV64_SUB(0, BV32_ZEXT64(local_id_x$1)), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n))), 30), 32)))), 1) then 1 else 0))), BV1_ZEXT32((if BV64_SGE(BV64_SREM(BV64_ADD(BV64_ADD(BV64_SUB(0, BV32_ZEXT64(local_id_x$1)), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n))), 30), 32), 30) then 1 else 0))), BV1_ZEXT32((if BV64_SGE(BV32_ZEXT64(BV32_ADD(BV32_ADD(BV32_ADD(BV32_MUL(4, $tsteps), BV32_MUL(2, BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n))), BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n)), 55)), BV64_ADD(BV64_MUL(2, BV64_SREM(BV64_ADD(BV64_ADD(BV64_SUB(0, BV32_ZEXT64(local_id_x$1)), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n))), 30), 32)), $c0)) then 1 else 0))), BV1_ZEXT32((if BV64_SGE(BV64_ADD(BV64_MUL(2, BV64_SREM(BV64_ADD(BV64_ADD(BV64_SUB(0, BV32_ZEXT64(local_id_x$1)), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n))), 30), 32)), $c0), BV32_ZEXT64(BV32_ADD(BV32_ADD(BV32_MUL(2, BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n)), BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n)), 59))) then 1 else 0))), BV1_ZEXT32((if BV64_SGE(BV64_ADD(BV64_ADD(BV64_ADD(BV64_MUL(2, BV64_SREM(BV64_ADD(BV64_ADD(BV64_SUB(0, BV32_ZEXT64(local_id_x$1)), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n))), 30), 32)), BV64_SREM(BV64_ADD(BV64_ADD(BV64_SUB(BV64_SUB(BV64_SUB(BV64_MUL(2, BV64_SREM(BV64_ADD(BV64_ADD(BV64_SUB(0, BV32_ZEXT64(local_id_x$1)), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n))), 30), 32)), BV64_MUL(128, BV32_ZEXT64(group_id_y$1))), BV32_ZEXT64(BV32_MUL(2, BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n)))), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n))), $c0), 32584), 32768)), BV64_MUL(2, BV32_ZEXT64(local_id_x$1))), BV64_MUL(64, BV64_SDIV(BV64_ADD(BV64_ADD(BV64_SUB(0, BV32_ZEXT64(local_id_x$1)), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n))), 30), 32))), BV32_ZEXT64(BV32_ADD(BV32_MUL(2, BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n)), 32703))) then 1 else 0))), BV1_ZEXT32((if BV64_ADD(BV32_ZEXT64(group_id_x$1), BV64_MUL(256, BV64_SDIV(BV64_ADD(BV64_ADD(BV64_SUB(BV64_SUB(0, BV64_SREM(BV64_ADD(BV64_ADD(BV64_SUB(0, BV32_ZEXT64(local_id_x$1)), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n))), 30), 32)), BV64_MUL(32, BV32_ZEXT64(group_id_x$1))), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n))), 8190), 8192))) == BV64_SDIV(BV64_ADD(BV64_ADD(BV64_SUB(0, BV32_ZEXT64(local_id_x$1)), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n))), 30), 32) then 1 else 0))), BV1_ZEXT32((if BV64_SREM(BV64_ADD(BV64_SUB(BV64_ADD(BV64_ADD(BV64_MUL(2, BV32_ZEXT64(local_id_x$1)), BV64_MUL(4, BV32_ZEXT64(local_id_y$1))), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n))), $c0), 63), 64) == 0 then 1 else 0)))), BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV1_ZEXT32((if BV32_UGE(BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n), 3) then 1 else 0)), BV1_ZEXT32((if BV32_UGE($n, BV32_ADD(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), 3)) then 1 else 0))), BV1_ZEXT32((if BV64_SGE(BV32_ZEXT64(BV32_ADD(BV32_ADD(BV32_MUL(2, $n), BV32_MUL(4, $tsteps)), BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n))), BV64_ADD($c0, 7)) then 1 else 0))), BV1_ZEXT32((if BV64_SGE(BV32_ZEXT64(BV32_ADD(BV32_ADD(BV32_MUL(4, $tsteps), BV32_MUL(2, BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n))), BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n))), BV64_ADD($c0, 3)) then 1 else 0))), BV1_ZEXT32((if BV64_SGE($c0, BV32_ZEXT64(BV32_ADD(BV32_ADD(BV32_MUL(2, BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n)), BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n)), 1))) then 1 else 0))), BV1_ZEXT32((if BV64_SGE(BV64_ADD($c0, 3), BV32_ZEXT64(BV32_ADD(BV32_MUL(4, $tsteps), BV32_MUL(2, BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n))))) then 1 else 0))), BV1_ZEXT32((if BV64_SGE(BV64_ADD($c0, 4), BV32_ZEXT64(BV32_ADD(BV32_ADD(BV32_MUL(4, $tsteps), BV32_MUL(2, BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n))), BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n)))) then 1 else 0))), BV1_ZEXT32((if BV64_SGE(BV64_ADD(BV32_ZEXT64(BV32_ADD($tsteps, BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n))), BV64_MUL(8192, BV64_SDIV(BV64_ADD(BV64_ADD(BV64_ADD(BV64_SUB(BV64_ADD(BV64_ADD(BV64_ADD(BV64_MUL(128, BV32_ZEXT64(group_id_x$1)), BV64_MUL(128, BV32_ZEXT64(group_id_y$1))), BV64_MUL(4, BV32_ZEXT64(local_id_x$1))), BV64_MUL(32768, BV32_ZEXT64(local_id_y$1))), BV32_ZEXT64(BV32_MUL(2, BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n)))), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n))), BV64_MUL(32767, $c0)), 32893), 32768))), BV64_ADD(BV64_ADD(BV64_ADD(BV64_ADD(BV64_ADD(BV64_MUL(32, BV32_ZEXT64(group_id_x$1)), BV64_MUL(32, BV32_ZEXT64(group_id_y$1))), BV32_ZEXT64(local_id_x$1)), BV64_MUL(8192, BV32_ZEXT64(local_id_y$1))), BV64_MUL(8192, $c0)), 8193)) then 1 else 0))), BV1_ZEXT32((if BV64_SREM(BV64_ADD(BV64_SUB(BV32_SEXT64($tsteps), BV32_ZEXT64(local_id_y$1)), 15), 16) == 0 then 1 else 0))), BV1_ZEXT32((if BV64_SREM(BV64_SUB(BV64_ADD(BV64_MUL(32, BV32_ZEXT64(group_id_x$1)), BV32_ZEXT64(local_id_x$1)), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n))), 8192) == 0 then 1 else 0)))), BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV1_ZEXT32((if BV64_SGE(BV64_ADD(BV64_MUL(32, BV32_ZEXT64(group_id_x$1)), BV32_ZEXT64(local_id_x$1)), 0) then 1 else 0)), BV1_ZEXT32((if BV64_SLE(BV64_ADD(BV64_MUL(32, BV32_ZEXT64(group_id_x$1)), BV32_ZEXT64(local_id_x$1)), 8191) then 1 else 0))), BV1_ZEXT32((if BV32_UGE(BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n), 2) then 1 else 0))), BV1_ZEXT32((if BV32_UGE($n, BV32_ADD(BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n), 1)) then 1 else 0))), BV1_ZEXT32((if BV32_UGE($n, BV32_ADD(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), 2)) then 1 else 0))), BV1_ZEXT32((if BV32_UGE(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), 1) then 1 else 0))), BV1_ZEXT32((if BV64_SGE(BV32_ZEXT64(BV32_ADD(BV32_ADD(BV32_MUL(4, $tsteps), BV32_MUL(2, BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n))), BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n))), BV64_ADD($c0, 6)) then 1 else 0))), BV1_ZEXT32((if BV64_SGE(BV64_ADD($c0, 2), BV32_ZEXT64(BV32_ADD(BV32_MUL(2, BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n)), BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n)))) then 1 else 0))), BV1_ZEXT32((if BV64_SGE(BV64_SREM(BV64_ADD(BV64_ADD(BV32_ZEXT64(BV32_SUB(BV32_MUL(-2, BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n)), BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n))), $c0), 2), 32768), BV64_MUL(128, BV32_ZEXT64(group_id_y$1))) then 1 else 0))), BV1_ZEXT32((if BV64_SGE(BV64_ADD(BV64_MUL(128, BV32_ZEXT64(group_id_y$1)), 124), BV64_SREM(BV64_ADD(BV64_ADD(BV32_ZEXT64(BV32_SUB(BV32_MUL(-2, BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n)), BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n))), $c0), 2), 32768)) then 1 else 0))), BV1_ZEXT32((if BV64_SREM(BV64_ADD(BV64_SUB(BV64_ADD(BV64_MUL(32, BV32_ZEXT64(group_id_x$1)), BV32_ZEXT64(local_id_x$1)), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n))), 1), 8192) == 0 then 1 else 0))), BV1_ZEXT32((if BV64_SREM(BV64_SUB(BV64_ADD(BV64_ADD(BV64_MUL(2, BV32_ZEXT64(local_id_x$1)), BV64_MUL(4, BV32_ZEXT64(local_id_y$1))), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n))), $c0), 64) == 0 then 1 else 0)))), BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV1_ZEXT32((if BV64_SGE(BV64_ADD(BV64_MUL(32, BV32_ZEXT64(group_id_x$1)), BV32_ZEXT64(local_id_x$1)), 0) then 1 else 0)), BV1_ZEXT32((if BV64_SLE(BV64_ADD(BV64_MUL(32, BV32_ZEXT64(group_id_x$1)), BV32_ZEXT64(local_id_x$1)), 8191) then 1 else 0))), BV1_ZEXT32((if BV32_UGE($n, BV32_ADD(BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n), 3)) then 1 else 0))), BV1_ZEXT32((if BV32_UGE(BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n), 0) then 1 else 0))), BV1_ZEXT32((if BV32_UGE(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), 2) then 1 else 0))), BV1_ZEXT32((if BV32_UGE($n, BV32_ADD(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), 1)) then 1 else 0))), BV1_ZEXT32((if BV64_SGE(BV32_ZEXT64(BV32_ADD(BV32_ADD(BV32_MUL(4, $tsteps), BV32_MUL(2, BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n))), BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n))), BV64_ADD($c0, 3)) then 1 else 0))), BV1_ZEXT32((if BV64_SGE($c0, BV32_ZEXT64(BV32_ADD(BV32_ADD(BV32_MUL(2, BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n)), BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n)), 1))) then 1 else 0))), BV1_ZEXT32((if BV64_SGE(BV64_SREM(BV64_SUB(BV64_ADD(BV32_ZEXT64(BV32_SUB(BV32_MUL(-2, BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n)), BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n))), $c0), 1), 32768), BV64_MUL(128, BV32_ZEXT64(group_id_y$1))) then 1 else 0))), BV1_ZEXT32((if BV64_SGE(BV64_ADD(BV64_MUL(128, BV32_ZEXT64(group_id_y$1)), 124), BV64_SREM(BV64_SUB(BV64_ADD(BV32_ZEXT64(BV32_SUB(BV32_MUL(-2, BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n)), BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n))), $c0), 1), 32768)) then 1 else 0))), BV1_ZEXT32((if BV64_SREM(BV64_ADD(BV64_SUB(BV64_ADD(BV64_MUL(32, BV32_ZEXT64(group_id_x$1)), BV32_ZEXT64(local_id_x$1)), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n))), 8191), 8192) == 0 then 1 else 0))), BV1_ZEXT32((if BV64_SREM(BV64_ADD(BV64_SUB(BV64_ADD(BV64_ADD(BV64_MUL(2, BV32_ZEXT64(local_id_x$1)), BV64_MUL(4, BV32_ZEXT64(local_id_y$1))), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n))), $c0), 63), 64) == 0 then 1 else 0)))), BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV1_ZEXT32((if BV32_UGE(BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n), 3) then 1 else 0)), BV1_ZEXT32((if BV32_UGE($n, BV32_ADD(BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n), 2)) then 1 else 0))), BV1_ZEXT32((if BV32_ADD(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), 2) == $n then 1 else 0))), BV1_ZEXT32((if BV64_SGE(BV32_ZEXT64(BV32_ADD(BV32_ADD($n, BV32_MUL(4, $tsteps)), BV32_MUL(2, BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n)))), BV64_ADD($c0, 10)) then 1 else 0))), BV1_ZEXT32((if BV64_SGE(BV64_ADD($c0, 2), BV32_ZEXT64(BV32_ADD($n, BV32_MUL(2, BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n))))) then 1 else 0))), BV1_ZEXT32((if BV64_SLE(BV64_SREM(BV64_SUB(BV64_ADD(BV64_ADD(BV64_ADD(BV64_ADD(BV64_ADD(BV64_ADD(BV32_SEXT64(BV32_MUL(32769, $n)), BV64_MUL(128, BV32_ZEXT64(group_id_x$1))), BV64_MUL(128, BV32_ZEXT64(group_id_y$1))), BV64_MUL(32772, BV32_ZEXT64(local_id_x$1))), BV64_MUL(32768, BV32_ZEXT64(local_id_y$1))), BV32_ZEXT64(BV32_MUL(32766, BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n)))), BV64_MUL(32767, $c0)), 65413), 32768), 125) then 1 else 0))), BV1_ZEXT32((if BV64_SREM(BV64_SUB(BV64_ADD(BV64_MUL(32, BV32_ZEXT64(group_id_x$1)), BV32_ZEXT64(local_id_x$1)), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n))), 8192) == 0 then 1 else 0))), BV1_ZEXT32((if BV64_SREM(BV64_SUB(BV64_SUB(BV64_ADD(BV64_ADD(BV32_SEXT64($n), BV64_MUL(2, BV32_ZEXT64(local_id_x$1))), BV64_MUL(4, BV32_ZEXT64(local_id_y$1))), $c0), 2), 64) == 0 then 1 else 0)))), BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV1_ZEXT32((if BV32_SGE($tsteps, 2) then 1 else 0)), BV1_ZEXT32((if BV32_ZEXT64(group_id_y$1) == 0 then 1 else 0))), BV1_ZEXT32((if BV32_ZEXT64(local_id_y$1) == 0 then 1 else 0))), BV1_ZEXT32((if BV32_UGE(BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n), 3) then 1 else 0))), BV1_ZEXT32((if BV32_UGE($n, BV32_ADD(BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n), 2)) then 1 else 0))), BV1_ZEXT32((if BV32_UGE($n, BV32_ADD(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), 3)) then 1 else 0))), BV1_ZEXT32((if BV32_UGE(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), 1) then 1 else 0))), BV1_ZEXT32((if $c0 == BV32_ZEXT64(BV32_ADD(BV32_MUL(2, BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n)), BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n))) then 1 else 0))), BV1_ZEXT32((if BV64_SREM(BV64_SUB(BV64_ADD(BV64_MUL(32, BV32_ZEXT64(group_id_x$1)), BV32_ZEXT64(local_id_x$1)), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n))), 8192) == 0 then 1 else 0)))), BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV1_ZEXT32((if BV32_ZEXT64(group_id_x$1) == 0 then 1 else 0)), BV1_ZEXT32((if BV64_SGE(BV32_SEXT64($n), BV64_ADD(BV32_ZEXT64(local_id_x$1), 2)) then 1 else 0))), BV1_ZEXT32((if BV64_SGE(BV32_ZEXT64(local_id_x$1), 1) then 1 else 0))), BV1_ZEXT32((if BV64_SLE(BV32_ZEXT64(local_id_x$1), 2) then 1 else 0))), BV1_ZEXT32((if BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n)) == BV32_ZEXT64(local_id_x$1) then 1 else 0))), BV1_ZEXT32((if BV32_ADD(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), 2) == $n then 1 else 0))), BV1_ZEXT32((if BV64_SGE(BV32_SEXT64(BV32_ADD($n, BV32_MUL(4, $tsteps))), BV64_ADD($c0, 4)) then 1 else 0))), BV1_ZEXT32((if BV64_SGE(BV64_ADD($c0, 2), BV64_ADD(BV32_SEXT64($n), BV64_MUL(2, BV32_ZEXT64(local_id_x$1)))) then 1 else 0))), BV1_ZEXT32((if BV64_SLE(BV64_SREM(BV64_ADD(BV64_ADD(BV64_ADD(BV64_ADD(BV32_SEXT64($n), BV64_MUL(128, BV32_ZEXT64(group_id_y$1))), BV64_MUL(2, BV32_ZEXT64(local_id_x$1))), BV64_MUL(32767, $c0)), 123), 32768), 125) then 1 else 0))), BV1_ZEXT32((if BV64_SREM(BV64_SUB(BV64_SUB(BV64_ADD(BV64_ADD(BV32_SEXT64($n), BV64_MUL(2, BV32_ZEXT64(local_id_x$1))), BV64_MUL(4, BV32_ZEXT64(local_id_y$1))), $c0), 2), 64) == 0 then 1 else 0)))), BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV1_ZEXT32((if BV32_ZEXT64(group_id_x$1) == 0 then 1 else 0)), BV1_ZEXT32((if BV32_ZEXT64(local_id_x$1) == 2 then 1 else 0))), BV1_ZEXT32((if BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n) == 2 then 1 else 0))), BV1_ZEXT32((if BV32_UGE($n, BV32_ADD(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), 3)) then 1 else 0))), BV1_ZEXT32((if BV64_SGE($c0, BV32_ZEXT64(BV32_ADD(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), 5))) then 1 else 0))), BV1_ZEXT32((if BV64_SGE($c0, BV32_SEXT64(BV32_ADD(BV32_MUL(4, $tsteps), 1))) then 1 else 0))), BV1_ZEXT32((if BV64_SGE($c0, BV32_ZEXT64(BV32_ADD(BV32_MUL(4, $tsteps), BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n)))) then 1 else 0))), BV1_ZEXT32((if BV64_SGE(BV32_ZEXT64(BV32_ADD(BV32_ADD(BV32_MUL(4, $tsteps), BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n)), 1)), $c0) then 1 else 0))), BV1_ZEXT32((if BV64_SGE(BV64_ADD(BV32_SEXT64($tsteps), BV64_MUL(8192, BV64_SDIV(BV64_ADD(BV64_ADD(BV64_ADD(BV64_MUL(128, BV32_ZEXT64(group_id_y$1)), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n))), BV64_MUL(32767, $c0)), 129), 32768))), BV64_ADD(BV64_ADD(BV64_MUL(32, BV32_ZEXT64(group_id_y$1)), BV64_MUL(8192, $c0)), 1)) then 1 else 0))), BV1_ZEXT32((if BV64_SREM(BV64_ADD(BV64_SUB(BV32_SEXT64($tsteps), BV32_ZEXT64(local_id_y$1)), 15), 16) == 0 then 1 else 0)))), BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV1_ZEXT32((if BV32_UGE(BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n), 3) then 1 else 0)), BV1_ZEXT32((if BV32_ADD(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), 2) == $n then 1 else 0))), BV1_ZEXT32((if BV64_ADD($c0, 6) == BV32_ZEXT64(BV32_ADD(BV32_ADD($n, BV32_MUL(4, $tsteps)), BV32_MUL(2, BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n)))) then 1 else 0))), BV1_ZEXT32((if BV64_SLE(BV64_SREM(BV64_ADD(BV64_ADD(BV64_ADD(BV64_ADD(BV64_ADD(BV64_ADD(BV32_SEXT64(BV32_MUL(8191, $tsteps)), BV64_MUL(32, BV32_ZEXT64(group_id_x$1))), BV64_MUL(32, BV32_ZEXT64(group_id_y$1))), BV64_MUL(8193, BV32_ZEXT64(local_id_x$1))), BV64_MUL(8192, BV32_ZEXT64(local_id_y$1))), BV32_ZEXT64(BV32_MUL(8191, BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n)))), 8224), 8192), 31) then 1 else 0))), BV1_ZEXT32((if BV64_SREM(BV64_SUB(BV64_ADD(BV64_MUL(32, BV32_ZEXT64(group_id_x$1)), BV32_ZEXT64(local_id_x$1)), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n))), 8192) == 0 then 1 else 0))), BV1_ZEXT32((if BV64_SREM(BV64_ADD(BV64_ADD(BV32_SEXT64(BV32_SUB(0, $tsteps)), BV32_ZEXT64(local_id_y$1)), 1), 16) == 0 then 1 else 0)))), BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV1_ZEXT32((if $tsteps == 1 then 1 else 0)), BV1_ZEXT32((if BV32_ZEXT64(group_id_y$1) == 0 then 1 else 0))), BV1_ZEXT32((if BV32_ZEXT64(local_id_y$1) == 0 then 1 else 0))), BV1_ZEXT32((if BV32_UGE(BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n), 3) then 1 else 0))), BV1_ZEXT32((if BV32_UGE($n, BV32_ADD(BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n), 2)) then 1 else 0))), BV1_ZEXT32((if BV32_UGE($n, BV32_ADD(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), 3)) then 1 else 0))), BV1_ZEXT32((if BV32_UGE(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), 1) then 1 else 0))), BV1_ZEXT32((if $c0 == BV32_ZEXT64(BV32_ADD(BV32_MUL(2, BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n)), BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n))) then 1 else 0))), BV1_ZEXT32((if BV64_SREM(BV64_SUB(BV64_ADD(BV64_MUL(32, BV32_ZEXT64(group_id_x$1)), BV32_ZEXT64(local_id_x$1)), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n))), 8192) == 0 then 1 else 0)))), BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV1_ZEXT32((if BV32_SGE($tsteps, 2) then 1 else 0)), BV1_ZEXT32((if BV32_ZEXT64(group_id_x$1) == 0 then 1 else 0))), BV1_ZEXT32((if BV32_ZEXT64(group_id_y$1) == 0 then 1 else 0))), BV1_ZEXT32((if BV32_ZEXT64(local_id_x$1) == 2 then 1 else 0))), BV1_ZEXT32((if BV32_ZEXT64(local_id_y$1) == 0 then 1 else 0))), BV1_ZEXT32((if BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n) == 2 then 1 else 0))), BV1_ZEXT32((if BV32_UGE($n, BV32_ADD(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), 3)) then 1 else 0))), BV1_ZEXT32((if BV32_UGE(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), 1) then 1 else 0))), BV1_ZEXT32((if $c0 == BV32_ZEXT64(BV32_ADD(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), 4)) then 1 else 0)))), BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV1_ZEXT32((if BV32_ZEXT64(group_id_x$1) == 0 then 1 else 0)), BV1_ZEXT32((if BV32_ZEXT64(group_id_y$1) == 0 then 1 else 0))), BV1_ZEXT32((if BV32_ZEXT64(local_id_x$1) == 1 then 1 else 0))), BV1_ZEXT32((if BV32_ZEXT64(local_id_y$1) == 0 then 1 else 0))), BV1_ZEXT32((if BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n) == 1 then 1 else 0))), BV1_ZEXT32((if BV32_UGE($n, BV32_ADD(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), 3)) then 1 else 0))), BV1_ZEXT32((if $c0 == BV32_ZEXT64(BV32_ADD(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), 2)) then 1 else 0)))), BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV1_ZEXT32((if BV32_ZEXT64(group_id_x$1) == 0 then 1 else 0)), BV1_ZEXT32((if BV32_ZEXT64(local_id_x$1) == 2 then 1 else 0))), BV1_ZEXT32((if BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n) == 2 then 1 else 0))), BV1_ZEXT32((if BV32_ADD(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), 2) == $n then 1 else 0))), BV1_ZEXT32((if BV64_ADD($c0, 2) == BV32_SEXT64(BV32_ADD($n, BV32_MUL(4, $tsteps))) then 1 else 0))), BV1_ZEXT32((if BV64_SGE(BV64_SREM(BV64_ADD(BV64_SUB(BV32_SEXT64($tsteps), BV64_MUL(32, BV32_ZEXT64(group_id_y$1))), 8159), 8192), 8160) then 1 else 0))), BV1_ZEXT32((if BV64_SREM(BV64_ADD(BV64_ADD(BV32_SEXT64(BV32_SUB(0, $tsteps)), BV32_ZEXT64(local_id_y$1)), 1), 16) == 0 then 1 else 0)))), BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV1_ZEXT32((if $tsteps == 1 then 1 else 0)), BV1_ZEXT32((if BV32_ZEXT64(group_id_x$1) == 0 then 1 else 0))), BV1_ZEXT32((if BV32_ZEXT64(group_id_y$1) == 0 then 1 else 0))), BV1_ZEXT32((if BV32_ZEXT64(local_id_x$1) == 2 then 1 else 0))), BV1_ZEXT32((if BV32_ZEXT64(local_id_y$1) == 0 then 1 else 0))), BV1_ZEXT32((if BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n) == 2 then 1 else 0))), BV1_ZEXT32((if BV32_UGE($n, BV32_ADD(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), 3)) then 1 else 0))), BV1_ZEXT32((if BV32_UGE(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), 1) then 1 else 0))), BV1_ZEXT32((if $c0 == BV32_ZEXT64(BV32_ADD(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), 4)) then 1 else 0)))) != 0 then 1 else 0) != 0;
    assert {:procedure_wide_invariant} {:do_not_predicate} {:sourceloc_num 4} {:thread 1} (if _WRITE_HAS_OCCURRED_$$A ==> BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV1_ZEXT32((if BV64_SGE(BV64_ADD(BV64_MUL(32, BV32_ZEXT64(group_id_x$1)), BV32_ZEXT64(local_id_x$1)), 0) then 1 else 0)), BV1_ZEXT32((if BV64_SLE(BV64_ADD(BV64_MUL(32, BV32_ZEXT64(group_id_x$1)), BV32_ZEXT64(local_id_x$1)), 8191) then 1 else 0))), BV1_ZEXT32((if BV32_UGE($n, BV32_ADD(BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n), 2)) then 1 else 0))), BV1_ZEXT32((if BV32_UGE(BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n), 1) then 1 else 0))), BV1_ZEXT32((if BV32_UGE($n, BV32_ADD(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), 2)) then 1 else 0))), BV1_ZEXT32((if BV32_UGE(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), 1) then 1 else 0))), BV1_ZEXT32((if BV64_SGE(BV32_ZEXT64(BV32_ADD(BV32_ADD(BV32_MUL(4, $tsteps), BV32_MUL(2, BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n))), BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n))), BV64_ADD($c0, 4)) then 1 else 0))), BV1_ZEXT32((if BV64_SGE($c0, BV32_ZEXT64(BV32_ADD(BV32_MUL(2, BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n)), BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n)))) then 1 else 0))), BV1_ZEXT32((if BV64_SGE(BV64_SREM(BV64_ADD(BV32_ZEXT64(BV32_SUB(BV32_MUL(-2, BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n)), BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n))), $c0), 32768), BV64_MUL(128, BV32_ZEXT64(group_id_y$1))) then 1 else 0))), BV1_ZEXT32((if BV64_SGE(BV64_ADD(BV64_MUL(128, BV32_ZEXT64(group_id_y$1)), 124), BV64_SREM(BV64_ADD(BV32_ZEXT64(BV32_SUB(BV32_MUL(-2, BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n)), BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n))), $c0), 32768)) then 1 else 0))), BV1_ZEXT32((if BV64_SREM(BV64_SUB(BV64_ADD(BV64_MUL(32, BV32_ZEXT64(group_id_x$1)), BV32_ZEXT64(local_id_x$1)), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n))), 8192) == 0 then 1 else 0))), BV1_ZEXT32((if BV64_SREM(BV64_SUB(BV64_ADD(BV64_ADD(BV64_MUL(2, BV32_ZEXT64(local_id_x$1)), BV64_MUL(4, BV32_ZEXT64(local_id_y$1))), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n))), $c0), 64) == 0 then 1 else 0))) != 0 then 1 else 0) != 0;
    assert {:block_sourceloc} {:sourceloc_num 16} p6$1 ==> true;
    v7$1 := (if p6$1 then BV64_SLT(BV32_SEXT64(BV32_SUB($n, 1)), BV64_SDIV(BV64_ADD($c0, 1), 2)) else v7$1);
    v7$2 := (if p6$2 then BV64_SLT(BV32_SEXT64(BV32_SUB($n, 1)), BV64_SDIV(BV64_ADD($c0, 1), 2)) else v7$2);
    p7$1 := false;
    p7$2 := false;
    p8$1 := false;
    p8$2 := false;
    p9$1 := false;
    p9$2 := false;
    p10$1 := false;
    p10$2 := false;
    p11$1 := false;
    p11$2 := false;
    p12$1 := false;
    p12$2 := false;
    p13$1 := false;
    p13$2 := false;
    p14$1 := false;
    p14$2 := false;
    p15$1 := false;
    p15$2 := false;
    p16$1 := false;
    p16$2 := false;
    p17$1 := false;
    p17$2 := false;
    p18$1 := false;
    p18$2 := false;
    p19$1 := false;
    p19$2 := false;
    p20$1 := false;
    p20$2 := false;
    p21$1 := false;
    p21$2 := false;
    p22$1 := false;
    p22$2 := false;
    p23$1 := false;
    p23$2 := false;
    p24$1 := false;
    p24$2 := false;
    p25$1 := false;
    p25$2 := false;
    p26$1 := false;
    p26$2 := false;
    p27$1 := false;
    p27$2 := false;
    p28$1 := false;
    p28$2 := false;
    p29$1 := false;
    p29$2 := false;
    p30$1 := false;
    p30$2 := false;
    p31$1 := false;
    p31$2 := false;
    p32$1 := false;
    p32$2 := false;
    p54$1 := false;
    p54$2 := false;
    p8$1 := (if p6$1 && v7$1 then v7$1 else p8$1);
    p8$2 := (if p6$2 && v7$2 then v7$2 else p8$2);
    p7$1 := (if p6$1 && !v7$1 then !v7$1 else p7$1);
    p7$2 := (if p6$2 && !v7$2 then !v7$2 else p7$2);
    $3$1 := (if p7$1 then BV64_SDIV(BV64_ADD($c0, 1), 2) else $3$1);
    $3$2 := (if p7$2 then BV64_SDIV(BV64_ADD($c0, 1), 2) else $3$2);
    $3$1 := (if p8$1 then BV32_SEXT64(BV32_SUB($n, 1)) else $3$1);
    $3$2 := (if p8$2 then BV32_SEXT64(BV32_SUB($n, 1)) else $3$2);
    v8$1 := (if p6$1 then BV64_SLT($c1.0$1, $3$1) else v8$1);
    v8$2 := (if p6$2 then BV64_SLT($c1.0$2, $3$2) else v8$2);
    p9$1 := (if p6$1 && v8$1 then v8$1 else p9$1);
    p9$2 := (if p6$2 && v8$2 then v8$2 else p9$2);
    p6$1 := (if p6$1 && !v8$1 then v8$1 else p6$1);
    p6$2 := (if p6$2 && !v8$2 then v8$2 else p6$2);
    v9$1 := (if p9$1 then BV64_SGE(BV32_SEXT64($n), BV64_ADD(BV64_ADD(v2$1, $c1.0$1), 2)) else v9$1);
    v9$2 := (if p9$2 then BV64_SGE(BV32_SEXT64($n), BV64_ADD(BV64_ADD(v2$2, $c1.0$2), 2)) else v9$2);
    p11$1 := (if p9$1 && v9$1 then v9$1 else p11$1);
    p11$2 := (if p9$2 && v9$2 then v9$2 else p11$2);
    v10$1 := (if p11$1 then BV64_SGE(BV64_ADD(v2$1, $c1.0$1), 1) else v10$1);
    v10$2 := (if p11$2 then BV64_SGE(BV64_ADD(v2$2, $c1.0$2), 1) else v10$2);
    p13$1 := (if p11$1 && v10$1 then v10$1 else p13$1);
    p13$2 := (if p11$2 && v10$2 then v10$2 else p13$2);
    v11$1 := (if p13$1 then BV64_SLT(BV64_SUB(BV64_ADD(BV64_SUB(BV32_SEXT64(BV32_MUL(-3, $n)), BV64_MUL(128, v1$1)), $c0), 119), 0) else v11$1);
    v11$2 := (if p13$2 then BV64_SLT(BV64_SUB(BV64_ADD(BV64_SUB(BV32_SEXT64(BV32_MUL(-3, $n)), BV64_MUL(128, v1$2)), $c0), 119), 0) else v11$2);
    p15$1 := (if p13$1 && v11$1 then v11$1 else p15$1);
    p15$2 := (if p13$2 && v11$2 then v11$2 else p15$2);
    p14$1 := (if p13$1 && !v11$1 then !v11$1 else p14$1);
    p14$2 := (if p13$2 && !v11$2 then !v11$2 else p14$2);
    $4$1 := (if p14$1 then BV64_SDIV(BV64_SUB(BV64_ADD(BV64_SUB(BV32_SEXT64(BV32_MUL(-3, $n)), BV64_MUL(128, v1$1)), $c0), 119), 32768) else $4$1);
    $4$2 := (if p14$2 then BV64_SDIV(BV64_SUB(BV64_ADD(BV64_SUB(BV32_SEXT64(BV32_MUL(-3, $n)), BV64_MUL(128, v1$2)), $c0), 119), 32768) else $4$2);
    $4$1 := (if p15$1 then BV64_SUB(0, BV64_SDIV(BV64_SUB(BV64_ADD(BV64_SUB(0, BV64_SUB(BV64_ADD(BV64_SUB(BV32_SEXT64(BV32_MUL(-3, $n)), BV64_MUL(128, v1$1)), $c0), 119)), 32768), 1), 32768)) else $4$1);
    $4$2 := (if p15$2 then BV64_SUB(0, BV64_SDIV(BV64_SUB(BV64_ADD(BV64_SUB(0, BV64_SUB(BV64_ADD(BV64_SUB(BV32_SEXT64(BV32_MUL(-3, $n)), BV64_MUL(128, v1$2)), $c0), 119)), 32768), 1), 32768)) else $4$2);
    v12$1 := (if p13$1 then BV64_SGT(BV64_MUL(32, v1$1), BV64_ADD(BV64_ADD(BV64_MUL(32, v1$1), BV64_MUL(8192, $4$1)), 8192)) else v12$1);
    v12$2 := (if p13$2 then BV64_SGT(BV64_MUL(32, v1$2), BV64_ADD(BV64_ADD(BV64_MUL(32, v1$2), BV64_MUL(8192, $4$2)), 8192)) else v12$2);
    p16$1 := (if p13$1 && v12$1 then v12$1 else p16$1);
    p16$2 := (if p13$2 && v12$2 then v12$2 else p16$2);
    p17$1 := (if p13$1 && !v12$1 then !v12$1 else p17$1);
    p17$2 := (if p13$2 && !v12$2 then !v12$2 else p17$2);
    $5$1 := (if p16$1 then BV64_MUL(32, v1$1) else $5$1);
    $5$2 := (if p16$2 then BV64_MUL(32, v1$2) else $5$2);
    v13$1 := (if p17$1 then BV64_SLT(BV64_SUB(BV64_ADD(BV64_SUB(BV32_SEXT64(BV32_MUL(-3, $n)), BV64_MUL(128, v1$1)), $c0), 119), 0) else v13$1);
    v13$2 := (if p17$2 then BV64_SLT(BV64_SUB(BV64_ADD(BV64_SUB(BV32_SEXT64(BV32_MUL(-3, $n)), BV64_MUL(128, v1$2)), $c0), 119), 0) else v13$2);
    p19$1 := (if p17$1 && v13$1 then v13$1 else p19$1);
    p19$2 := (if p17$2 && v13$2 then v13$2 else p19$2);
    p18$1 := (if p17$1 && !v13$1 then !v13$1 else p18$1);
    p18$2 := (if p17$2 && !v13$2 then !v13$2 else p18$2);
    $6$1 := (if p18$1 then BV64_SDIV(BV64_SUB(BV64_ADD(BV64_SUB(BV32_SEXT64(BV32_MUL(-3, $n)), BV64_MUL(128, v1$1)), $c0), 119), 32768) else $6$1);
    $6$2 := (if p18$2 then BV64_SDIV(BV64_SUB(BV64_ADD(BV64_SUB(BV32_SEXT64(BV32_MUL(-3, $n)), BV64_MUL(128, v1$2)), $c0), 119), 32768) else $6$2);
    $6$1 := (if p19$1 then BV64_SUB(0, BV64_SDIV(BV64_SUB(BV64_ADD(BV64_SUB(0, BV64_SUB(BV64_ADD(BV64_SUB(BV32_SEXT64(BV32_MUL(-3, $n)), BV64_MUL(128, v1$1)), $c0), 119)), 32768), 1), 32768)) else $6$1);
    $6$2 := (if p19$2 then BV64_SUB(0, BV64_SDIV(BV64_SUB(BV64_ADD(BV64_SUB(0, BV64_SUB(BV64_ADD(BV64_SUB(BV32_SEXT64(BV32_MUL(-3, $n)), BV64_MUL(128, v1$2)), $c0), 119)), 32768), 1), 32768)) else $6$2);
    $5$1 := (if p17$1 then BV64_ADD(BV64_ADD(BV64_MUL(32, v1$1), BV64_MUL(8192, $6$1)), 8192) else $5$1);
    $5$2 := (if p17$2 then BV64_ADD(BV64_ADD(BV64_MUL(32, v1$2), BV64_MUL(8192, $6$2)), 8192) else $5$2);
    v14$1 := (if p13$1 then BV64_SLT(BV64_SUB(BV64_SUB(BV64_ADD(BV64_SUB(BV32_SEXT64(BV32_SUB(0, $n)), BV64_MUL(128, v1$1)), $c0), BV64_MUL(2, $c1.0$1)), 185), 0) else v14$1);
    v14$2 := (if p13$2 then BV64_SLT(BV64_SUB(BV64_SUB(BV64_ADD(BV64_SUB(BV32_SEXT64(BV32_SUB(0, $n)), BV64_MUL(128, v1$2)), $c0), BV64_MUL(2, $c1.0$2)), 185), 0) else v14$2);
    p21$1 := (if p13$1 && v14$1 then v14$1 else p21$1);
    p21$2 := (if p13$2 && v14$2 then v14$2 else p21$2);
    p20$1 := (if p13$1 && !v14$1 then !v14$1 else p20$1);
    p20$2 := (if p13$2 && !v14$2 then !v14$2 else p20$2);
    $7$1 := (if p20$1 then BV64_SDIV(BV64_SUB(BV64_SUB(BV64_ADD(BV64_SUB(BV32_SEXT64(BV32_SUB(0, $n)), BV64_MUL(128, v1$1)), $c0), BV64_MUL(2, $c1.0$1)), 185), 32768) else $7$1);
    $7$2 := (if p20$2 then BV64_SDIV(BV64_SUB(BV64_SUB(BV64_ADD(BV64_SUB(BV32_SEXT64(BV32_SUB(0, $n)), BV64_MUL(128, v1$2)), $c0), BV64_MUL(2, $c1.0$2)), 185), 32768) else $7$2);
    $7$1 := (if p21$1 then BV64_SUB(0, BV64_SDIV(BV64_SUB(BV64_ADD(BV64_SUB(0, BV64_SUB(BV64_SUB(BV64_ADD(BV64_SUB(BV32_SEXT64(BV32_SUB(0, $n)), BV64_MUL(128, v1$1)), $c0), BV64_MUL(2, $c1.0$1)), 185)), 32768), 1), 32768)) else $7$1);
    $7$2 := (if p21$2 then BV64_SUB(0, BV64_SDIV(BV64_SUB(BV64_ADD(BV64_SUB(0, BV64_SUB(BV64_SUB(BV64_ADD(BV64_SUB(BV32_SEXT64(BV32_SUB(0, $n)), BV64_MUL(128, v1$2)), $c0), BV64_MUL(2, $c1.0$2)), 185)), 32768), 1), 32768)) else $7$2);
    v15$1 := (if p13$1 then BV64_SGT($5$1, BV64_ADD(BV64_ADD(BV64_MUL(32, v1$1), BV64_MUL(8192, $7$1)), 8192)) else v15$1);
    v15$2 := (if p13$2 then BV64_SGT($5$2, BV64_ADD(BV64_ADD(BV64_MUL(32, v1$2), BV64_MUL(8192, $7$2)), 8192)) else v15$2);
    p22$1 := (if p13$1 && v15$1 then v15$1 else p22$1);
    p22$2 := (if p13$2 && v15$2 then v15$2 else p22$2);
    p29$1 := (if p13$1 && !v15$1 then !v15$1 else p29$1);
    p29$2 := (if p13$2 && !v15$2 then !v15$2 else p29$2);
    v16$1 := (if p22$1 then BV64_SLT(BV64_SUB(BV64_ADD(BV64_SUB(BV32_SEXT64(BV32_MUL(-3, $n)), BV64_MUL(128, v1$1)), $c0), 119), 0) else v16$1);
    v16$2 := (if p22$2 then BV64_SLT(BV64_SUB(BV64_ADD(BV64_SUB(BV32_SEXT64(BV32_MUL(-3, $n)), BV64_MUL(128, v1$2)), $c0), 119), 0) else v16$2);
    p24$1 := (if p22$1 && v16$1 then v16$1 else p24$1);
    p24$2 := (if p22$2 && v16$2 then v16$2 else p24$2);
    p23$1 := (if p22$1 && !v16$1 then !v16$1 else p23$1);
    p23$2 := (if p22$2 && !v16$2 then !v16$2 else p23$2);
    $8$1 := (if p23$1 then BV64_SDIV(BV64_SUB(BV64_ADD(BV64_SUB(BV32_SEXT64(BV32_MUL(-3, $n)), BV64_MUL(128, v1$1)), $c0), 119), 32768) else $8$1);
    $8$2 := (if p23$2 then BV64_SDIV(BV64_SUB(BV64_ADD(BV64_SUB(BV32_SEXT64(BV32_MUL(-3, $n)), BV64_MUL(128, v1$2)), $c0), 119), 32768) else $8$2);
    $8$1 := (if p24$1 then BV64_SUB(0, BV64_SDIV(BV64_SUB(BV64_ADD(BV64_SUB(0, BV64_SUB(BV64_ADD(BV64_SUB(BV32_SEXT64(BV32_MUL(-3, $n)), BV64_MUL(128, v1$1)), $c0), 119)), 32768), 1), 32768)) else $8$1);
    $8$2 := (if p24$2 then BV64_SUB(0, BV64_SDIV(BV64_SUB(BV64_ADD(BV64_SUB(0, BV64_SUB(BV64_ADD(BV64_SUB(BV32_SEXT64(BV32_MUL(-3, $n)), BV64_MUL(128, v1$2)), $c0), 119)), 32768), 1), 32768)) else $8$2);
    v17$1 := (if p22$1 then BV64_SGT(BV64_MUL(32, v1$1), BV64_ADD(BV64_ADD(BV64_MUL(32, v1$1), BV64_MUL(8192, $8$1)), 8192)) else v17$1);
    v17$2 := (if p22$2 then BV64_SGT(BV64_MUL(32, v1$2), BV64_ADD(BV64_ADD(BV64_MUL(32, v1$2), BV64_MUL(8192, $8$2)), 8192)) else v17$2);
    p25$1 := (if p22$1 && v17$1 then v17$1 else p25$1);
    p25$2 := (if p22$2 && v17$2 then v17$2 else p25$2);
    p26$1 := (if p22$1 && !v17$1 then !v17$1 else p26$1);
    p26$2 := (if p22$2 && !v17$2 then !v17$2 else p26$2);
    $9$1 := (if p25$1 then BV64_MUL(32, v1$1) else $9$1);
    $9$2 := (if p25$2 then BV64_MUL(32, v1$2) else $9$2);
    v18$1 := (if p26$1 then BV64_SLT(BV64_SUB(BV64_ADD(BV64_SUB(BV32_SEXT64(BV32_MUL(-3, $n)), BV64_MUL(128, v1$1)), $c0), 119), 0) else v18$1);
    v18$2 := (if p26$2 then BV64_SLT(BV64_SUB(BV64_ADD(BV64_SUB(BV32_SEXT64(BV32_MUL(-3, $n)), BV64_MUL(128, v1$2)), $c0), 119), 0) else v18$2);
    p28$1 := (if p26$1 && v18$1 then v18$1 else p28$1);
    p28$2 := (if p26$2 && v18$2 then v18$2 else p28$2);
    p27$1 := (if p26$1 && !v18$1 then !v18$1 else p27$1);
    p27$2 := (if p26$2 && !v18$2 then !v18$2 else p27$2);
    $10$1 := (if p27$1 then BV64_SDIV(BV64_SUB(BV64_ADD(BV64_SUB(BV32_SEXT64(BV32_MUL(-3, $n)), BV64_MUL(128, v1$1)), $c0), 119), 32768) else $10$1);
    $10$2 := (if p27$2 then BV64_SDIV(BV64_SUB(BV64_ADD(BV64_SUB(BV32_SEXT64(BV32_MUL(-3, $n)), BV64_MUL(128, v1$2)), $c0), 119), 32768) else $10$2);
    $10$1 := (if p28$1 then BV64_SUB(0, BV64_SDIV(BV64_SUB(BV64_ADD(BV64_SUB(0, BV64_SUB(BV64_ADD(BV64_SUB(BV32_SEXT64(BV32_MUL(-3, $n)), BV64_MUL(128, v1$1)), $c0), 119)), 32768), 1), 32768)) else $10$1);
    $10$2 := (if p28$2 then BV64_SUB(0, BV64_SDIV(BV64_SUB(BV64_ADD(BV64_SUB(0, BV64_SUB(BV64_ADD(BV64_SUB(BV32_SEXT64(BV32_MUL(-3, $n)), BV64_MUL(128, v1$2)), $c0), 119)), 32768), 1), 32768)) else $10$2);
    $9$1 := (if p26$1 then BV64_ADD(BV64_ADD(BV64_MUL(32, v1$1), BV64_MUL(8192, $10$1)), 8192) else $9$1);
    $9$2 := (if p26$2 then BV64_ADD(BV64_ADD(BV64_MUL(32, v1$2), BV64_MUL(8192, $10$2)), 8192) else $9$2);
    $11$1 := (if p22$1 then $9$1 else $11$1);
    $11$2 := (if p22$2 then $9$2 else $11$2);
    v19$1 := (if p29$1 then BV64_SLT(BV64_SUB(BV64_SUB(BV64_ADD(BV64_SUB(BV32_SEXT64(BV32_SUB(0, $n)), BV64_MUL(128, v1$1)), $c0), BV64_MUL(2, $c1.0$1)), 185), 0) else v19$1);
    v19$2 := (if p29$2 then BV64_SLT(BV64_SUB(BV64_SUB(BV64_ADD(BV64_SUB(BV32_SEXT64(BV32_SUB(0, $n)), BV64_MUL(128, v1$2)), $c0), BV64_MUL(2, $c1.0$2)), 185), 0) else v19$2);
    p31$1 := (if p29$1 && v19$1 then v19$1 else p31$1);
    p31$2 := (if p29$2 && v19$2 then v19$2 else p31$2);
    p30$1 := (if p29$1 && !v19$1 then !v19$1 else p30$1);
    p30$2 := (if p29$2 && !v19$2 then !v19$2 else p30$2);
    $12$1 := (if p30$1 then BV64_SDIV(BV64_SUB(BV64_SUB(BV64_ADD(BV64_SUB(BV32_SEXT64(BV32_SUB(0, $n)), BV64_MUL(128, v1$1)), $c0), BV64_MUL(2, $c1.0$1)), 185), 32768) else $12$1);
    $12$2 := (if p30$2 then BV64_SDIV(BV64_SUB(BV64_SUB(BV64_ADD(BV64_SUB(BV32_SEXT64(BV32_SUB(0, $n)), BV64_MUL(128, v1$2)), $c0), BV64_MUL(2, $c1.0$2)), 185), 32768) else $12$2);
    $12$1 := (if p31$1 then BV64_SUB(0, BV64_SDIV(BV64_SUB(BV64_ADD(BV64_SUB(0, BV64_SUB(BV64_SUB(BV64_ADD(BV64_SUB(BV32_SEXT64(BV32_SUB(0, $n)), BV64_MUL(128, v1$1)), $c0), BV64_MUL(2, $c1.0$1)), 185)), 32768), 1), 32768)) else $12$1);
    $12$2 := (if p31$2 then BV64_SUB(0, BV64_SDIV(BV64_SUB(BV64_ADD(BV64_SUB(0, BV64_SUB(BV64_SUB(BV64_ADD(BV64_SUB(BV32_SEXT64(BV32_SUB(0, $n)), BV64_MUL(128, v1$2)), $c0), BV64_MUL(2, $c1.0$2)), 185)), 32768), 1), 32768)) else $12$2);
    $11$1 := (if p29$1 then BV64_ADD(BV64_ADD(BV64_MUL(32, v1$1), BV64_MUL(8192, $12$1)), 8192) else $11$1);
    $11$2 := (if p29$2 then BV64_ADD(BV64_ADD(BV64_MUL(32, v1$2), BV64_MUL(8192, $12$2)), 8192) else $11$2);
    $c2.0$1 := (if p13$1 then $11$1 else $c2.0$1);
    $c2.0$2 := (if p13$2 then $11$2 else $c2.0$2);
    p32$1 := (if p13$1 then true else p32$1);
    p32$2 := (if p13$2 then true else p32$2);
    _READ_HAS_OCCURRED_$$A$ghost$$44 := _READ_HAS_OCCURRED_$$A;
    _WRITE_HAS_OCCURRED_$$A$ghost$$44 := _WRITE_HAS_OCCURRED_$$A;
    assume {:captureState "loop_entry_state_1_0"} true;
    goto $44;

  $44:
    assume {:captureState "loop_head_state_1"} true;
    
    
    
    
assert  my_inv (  true ,  true ,  true ,  true ,  true ,  true ,  true ,  true ,  true ,  true ,  true ,  true ,  true ,  true ,  true ,  true ,  true ,  true ,  true ,  true ,  true ,  true ,  true ,  true ,  true ,  true ,  true ,  true ,  true ,  true ,  true ,  true ,  true ,  true ,  true ,  true ,  (  _READ_HAS_OCCURRED_$$A ==> _WATCHED_OFFSET mod 1 == 0 mod 1 || _WATCHED_OFFSET mod 1 == 0 mod 1 || _WATCHED_OFFSET mod 1 == 0 mod 1 || _WATCHED_OFFSET mod 1 == 0 mod 1 || _WATCHED_OFFSET mod 1 == 0 mod 1 || _WATCHED_OFFSET mod 1 == 0 mod 1 || _WATCHED_OFFSET mod 1 == 0 mod 1 || _WATCHED_OFFSET mod 1 == 0 mod 1 || _WATCHED_OFFSET mod 1 == 0 mod 1 ) ,  (  _WRITE_HAS_OCCURRED_$$A ==> _WATCHED_OFFSET mod 1 == 0 mod 1 ) ,  (  !p13$1 ==> _READ_HAS_OCCURRED_$$A$ghost$$44 == _READ_HAS_OCCURRED_$$A ) ,  (  !p13$1 ==> _WRITE_HAS_OCCURRED_$$A$ghost$$44 == _WRITE_HAS_OCCURRED_$$A ) ,  true ,  true ,  true ,  true  ); 


    assume {:predicate "p32"} {:dominator_predicate "p13"} true;
    assert p32$1 ==> p6$1;
    assert p32$2 ==> p6$2;
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
assert  my_inv (  true ,  true ,  true ,  true ,  true ,  true ,  ( p32$1 ==> $c2.0$1 mod 8192 == $11$1 mod 8192 )  && ( p32$2 ==> $c2.0$2 mod 8192 == $11$2 mod 8192 ) ,  ( p32$1 ==> BV32_SLE($c2.0$1, $11$1) )  && ( p32$2 ==> BV32_SLE($c2.0$2, $11$2) ) ,  ( p32$1 ==> BV32_SGE($c2.0$1, $11$1) )  && ( p32$2 ==> BV32_SGE($c2.0$2, $11$2) ) ,  ( p32$1 ==> BV32_ULE($c2.0$1, $11$1) )  && ( p32$2 ==> BV32_ULE($c2.0$2, $11$2) ) ,  ( p32$1 ==> BV32_UGE($c2.0$1, $11$1) )  && ( p32$2 ==> BV32_UGE($c2.0$2, $11$2) ) ,  ( p32$1 ==> p32$1 ==> BV64_SLT($c1.0$1, $3$1) && BV64_SGE(BV32_SEXT64($n), BV64_ADD(BV64_ADD(BV32_ZEXT64(local_id_x$1), $c1.0$1), 2)) && BV64_SGE(BV64_ADD(BV32_ZEXT64(local_id_x$1), $c1.0$1), 1) )  && ( p32$2 ==> p32$2 ==> BV64_SLT($c1.0$2, $3$2) && BV64_SGE(BV32_SEXT64($n), BV64_ADD(BV64_ADD(BV32_ZEXT64(local_id_x$2), $c1.0$2), 2)) && BV64_SGE(BV64_ADD(BV32_ZEXT64(local_id_x$2), $c1.0$2), 1) ) ,  (  BV64_SLT($c1.0$1, $3$1) && BV64_SGE(BV32_SEXT64($n), BV64_ADD(BV64_ADD(BV32_ZEXT64(local_id_x$1), $c1.0$1), 2)) && BV64_SGE(BV64_ADD(BV32_ZEXT64(local_id_x$1), $c1.0$1), 1) && BV64_SLE($c2.0$1, $15$1) ==> p32$1 )  && (  BV64_SLT($c1.0$2, $3$2) && BV64_SGE(BV32_SEXT64($n), BV64_ADD(BV64_ADD(BV32_ZEXT64(local_id_x$2), $c1.0$2), 2)) && BV64_SGE(BV64_ADD(BV32_ZEXT64(local_id_x$2), $c1.0$2), 1) && BV64_SLE($c2.0$2, $15$2) ==> p32$2 ) ,  (  _READ_HAS_OCCURRED_$$A ==> BV64_SLT($c1.0$1, $3$1) ) ,  (  _READ_HAS_OCCURRED_$$A ==> BV64_SGE(BV32_SEXT64($n), BV64_ADD(BV64_ADD(BV32_ZEXT64(local_id_x$1), $c1.0$1), 2)) ) ,  (  _READ_HAS_OCCURRED_$$A ==> BV64_SGE(BV64_ADD(BV32_ZEXT64(local_id_x$1), $c1.0$1), 1) ) ,  (  _WRITE_HAS_OCCURRED_$$A ==> BV64_SLT($c1.0$1, $3$1) ) ,  (  _WRITE_HAS_OCCURRED_$$A ==> BV64_SGE(BV32_SEXT64($n), BV64_ADD(BV64_ADD(BV32_ZEXT64(local_id_x$1), $c1.0$1), 2)) ) ,  (  _WRITE_HAS_OCCURRED_$$A ==> BV64_SGE(BV64_ADD(BV32_ZEXT64(local_id_x$1), $c1.0$1), 1) ) ,  true ,  true ,  true ,  true ,  true ,  true ,  true ,  true ,  true ,  true ,  true ,  true ,  true ,  true ,  true ,  true ,  true ,  true ,  true ,  true ,  true ,  true ,  true ,  true ,  true  ); 


    assert {:procedure_wide_invariant} {:do_not_predicate} {:sourceloc_num 5} {:thread 1} (if _READ_HAS_OCCURRED_$$A ==> BV32_OR(BV32_OR(BV32_OR(BV32_OR(BV32_OR(BV32_OR(BV32_OR(BV32_OR(BV32_OR(BV32_OR(BV32_OR(BV32_OR(BV32_OR(BV32_OR(BV32_OR(BV32_OR(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV1_ZEXT32((if BV32_UGE(BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n), 3) then 1 else 0)), BV1_ZEXT32((if BV32_UGE($n, BV32_ADD(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), 3)) then 1 else 0))), BV1_ZEXT32((if BV64_SGE(BV32_ZEXT64(BV32_ADD(BV32_ADD(BV32_MUL(4, $tsteps), BV32_MUL(2, BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n))), BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n))), BV64_ADD($c0, 5)) then 1 else 0))), BV1_ZEXT32((if BV64_SGE(BV32_SEXT64($n), BV64_ADD(BV64_ADD(BV64_SREM(BV64_ADD(BV64_ADD(BV64_ADD(BV64_MUL(32, BV32_ZEXT64(group_id_x$1)), BV32_ZEXT64(local_id_x$1)), BV32_ZEXT64(BV32_MUL(8191, BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n)))), 1), 8192), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n))), 1)) then 1 else 0))), BV1_ZEXT32((if BV64_SLE(BV64_SREM(BV64_ADD(BV64_ADD(BV64_ADD(BV64_MUL(32, BV32_ZEXT64(group_id_x$1)), BV32_ZEXT64(local_id_x$1)), BV32_ZEXT64(BV32_MUL(8191, BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n)))), 1), 8192), 1) then 1 else 0))), BV1_ZEXT32((if BV64_SGE(BV64_ADD($c0, 1), BV64_ADD(BV64_ADD(BV64_MUL(2, BV64_SREM(BV64_ADD(BV64_ADD(BV64_ADD(BV64_MUL(32, BV32_ZEXT64(group_id_x$1)), BV32_ZEXT64(local_id_x$1)), BV32_ZEXT64(BV32_MUL(8191, BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n)))), 1), 8192)), BV32_ZEXT64(BV32_MUL(2, BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n)))), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n)))) then 1 else 0))), BV1_ZEXT32((if BV64_SGE(BV64_MUL(64, (if BV64_SLT(BV64_ADD(BV64_ADD(BV64_ADD(BV64_ADD(BV64_MUL(2, BV32_ZEXT64(local_id_x$1)), BV64_MUL(4, BV32_ZEXT64(local_id_y$1))), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n))), BV64_MUL(63, $c0)), 1), 0) then BV64_SUB(0, BV64_SDIV(BV64_SUB(BV64_ADD(BV64_SUB(0, BV64_ADD(BV64_ADD(BV64_ADD(BV64_ADD(BV64_MUL(2, BV32_ZEXT64(local_id_x$1)), BV64_MUL(4, BV32_ZEXT64(local_id_y$1))), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n))), BV64_MUL(63, $c0)), 1)), 64), 1), 64)) else BV64_SDIV(BV64_ADD(BV64_ADD(BV64_ADD(BV64_ADD(BV64_MUL(2, BV32_ZEXT64(local_id_x$1)), BV64_MUL(4, BV32_ZEXT64(local_id_y$1))), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n))), BV64_MUL(63, $c0)), 1), 64))), BV64_ADD(BV64_ADD(BV64_ADD(BV64_MUL(2, BV32_ZEXT64(local_id_x$1)), BV64_MUL(4, BV32_ZEXT64(local_id_y$1))), BV64_MUL(63, $c0)), 1)) then 1 else 0))), BV1_ZEXT32((if BV64_SGE(BV64_SREM(BV64_ADD(BV64_ADD(BV64_ADD(BV64_MUL(32, BV32_ZEXT64(group_id_x$1)), BV32_ZEXT64(local_id_x$1)), BV32_ZEXT64(BV32_MUL(8191, BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n)))), 1), 8192), BV64_SREM(BV64_ADD(BV64_ADD(BV64_ADD(BV64_ADD(BV64_MUL(2, BV32_ZEXT64(local_id_x$1)), BV64_MUL(4, BV32_ZEXT64(local_id_y$1))), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n))), BV64_MUL(63, $c0)), 1), 64)) then 1 else 0))), BV1_ZEXT32((if BV64_SGE(BV64_ADD(BV32_ZEXT64(local_id_y$1), BV64_MUL(8192, BV64_SDIV(BV64_ADD(BV64_ADD(BV64_ADD(BV64_ADD(BV64_ADD(BV64_ADD(BV64_ADD(BV64_SUB(0, BV64_MUL(2, BV64_SREM(BV64_ADD(BV64_ADD(BV64_ADD(BV64_MUL(32, BV32_ZEXT64(group_id_x$1)), BV32_ZEXT64(local_id_x$1)), BV32_ZEXT64(BV32_MUL(8191, BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n)))), 1), 8192))), BV64_MUL(128, BV32_ZEXT64(group_id_x$1))), BV64_MUL(128, BV32_ZEXT64(group_id_y$1))), BV64_MUL(4, BV32_ZEXT64(local_id_x$1))), BV32_ZEXT64(BV32_MUL(32766, BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n)))), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n))), BV64_MUL(32767, $c0)), 127), 32768))), BV64_ADD(BV64_ADD(BV64_ADD(BV64_ADD(BV64_ADD(BV64_MUL(16, BV32_ZEXT64(group_id_x$1)), BV64_MUL(32, BV32_ZEXT64(group_id_y$1))), BV32_ZEXT64(BV32_MUL(4096, BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n)))), BV64_MUL(8176, $c0)), BV64_MUL(4096, BV64_SDIV(BV64_ADD(BV64_ADD(BV64_ADD(BV64_MUL(32, BV32_ZEXT64(group_id_x$1)), BV32_ZEXT64(local_id_x$1)), BV32_ZEXT64(BV32_MUL(8191, BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n)))), 1), 8192))), BV64_MUL(16, BV64_SDIV(BV64_ADD(BV64_ADD(BV64_ADD(BV64_ADD(BV64_MUL(2, BV32_ZEXT64(local_id_x$1)), BV64_MUL(4, BV32_ZEXT64(local_id_y$1))), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n))), BV64_MUL(63, $c0)), 1), 64)))) then 1 else 0))), BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV1_ZEXT32((if BV32_ZEXT64(group_id_x$1) == 0 then 1 else 0)), BV1_ZEXT32((if BV64_SGE(BV32_SEXT64($n), BV64_ADD(BV32_ZEXT64(local_id_x$1), 2)) then 1 else 0))), BV1_ZEXT32((if BV64_SGE(BV32_ZEXT64(local_id_x$1), 1) then 1 else 0))), BV1_ZEXT32((if BV64_SGE(BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n)), BV32_ZEXT64(local_id_x$1)) then 1 else 0))), BV1_ZEXT32((if BV32_ULE(BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n), 2) then 1 else 0))), BV1_ZEXT32((if BV32_UGE($n, BV32_ADD(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), 3)) then 1 else 0))), BV1_ZEXT32((if BV64_SGE($c0, BV64_ADD(BV64_ADD(BV64_MUL(2, BV32_ZEXT64(local_id_x$1)), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n))), 1)) then 1 else 0))), BV1_ZEXT32((if BV64_SGE(BV32_ZEXT64(BV32_ADD(BV32_MUL(4, $tsteps), BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n))), BV64_ADD($c0, 1)) then 1 else 0))), BV1_ZEXT32((if BV64_SGE(BV64_MUL(64, (if BV64_SLT(BV64_ADD(BV64_ADD(BV64_ADD(BV64_ADD(BV64_MUL(2, BV32_ZEXT64(local_id_x$1)), BV64_MUL(4, BV32_ZEXT64(local_id_y$1))), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n))), BV64_MUL(63, $c0)), 1), 0) then BV64_SUB(0, BV64_SDIV(BV64_SUB(BV64_ADD(BV64_SUB(0, BV64_ADD(BV64_ADD(BV64_ADD(BV64_ADD(BV64_MUL(2, BV32_ZEXT64(local_id_x$1)), BV64_MUL(4, BV32_ZEXT64(local_id_y$1))), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n))), BV64_MUL(63, $c0)), 1)), 64), 1), 64)) else BV64_SDIV(BV64_ADD(BV64_ADD(BV64_ADD(BV64_ADD(BV64_MUL(2, BV32_ZEXT64(local_id_x$1)), BV64_MUL(4, BV32_ZEXT64(local_id_y$1))), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n))), BV64_MUL(63, $c0)), 1), 64))), BV64_ADD(BV64_ADD(BV64_ADD(BV64_MUL(2, BV32_ZEXT64(local_id_x$1)), BV64_MUL(4, BV32_ZEXT64(local_id_y$1))), BV64_MUL(63, $c0)), 1)) then 1 else 0))), BV1_ZEXT32((if BV64_SGE(BV64_ADD(BV32_ZEXT64(local_id_x$1), 1), BV64_ADD(BV64_SREM(BV64_ADD(BV64_ADD(BV64_ADD(BV64_ADD(BV64_MUL(2, BV32_ZEXT64(local_id_x$1)), BV64_MUL(4, BV32_ZEXT64(local_id_y$1))), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n))), BV64_MUL(63, $c0)), 1), 64), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n)))) then 1 else 0))), BV1_ZEXT32((if BV64_SGE(BV64_ADD(BV32_ZEXT64(local_id_y$1), BV64_MUL(8192, BV64_SDIV(BV64_ADD(BV64_ADD(BV64_ADD(BV64_ADD(BV64_MUL(128, BV32_ZEXT64(group_id_y$1)), BV64_MUL(2, BV32_ZEXT64(local_id_x$1))), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n))), BV64_MUL(32767, $c0)), 125), 32768))), BV64_ADD(BV64_ADD(BV64_MUL(32, BV32_ZEXT64(group_id_y$1)), BV64_MUL(8176, $c0)), BV64_MUL(16, BV64_SDIV(BV64_ADD(BV64_ADD(BV64_ADD(BV64_ADD(BV64_MUL(2, BV32_ZEXT64(local_id_x$1)), BV64_MUL(4, BV32_ZEXT64(local_id_y$1))), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n))), BV64_MUL(63, $c0)), 1), 64)))) then 1 else 0)))), BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV1_ZEXT32((if BV64_SGE(BV64_ADD(BV64_MUL(32, BV32_ZEXT64(group_id_x$1)), BV32_ZEXT64(local_id_x$1)), 0) then 1 else 0)), BV1_ZEXT32((if BV64_SLE(BV64_ADD(BV64_MUL(32, BV32_ZEXT64(group_id_x$1)), BV32_ZEXT64(local_id_x$1)), 8191) then 1 else 0))), BV1_ZEXT32((if BV32_UGE($n, BV32_ADD(BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n), 3)) then 1 else 0))), BV1_ZEXT32((if BV32_UGE(BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n), 0) then 1 else 0))), BV1_ZEXT32((if BV64_SGE(BV64_ADD(BV64_ADD(BV32_ZEXT64(local_id_x$1), BV64_MUL(2, BV32_ZEXT64(local_id_y$1))), BV64_MUL(32, (if BV64_SLT(BV64_ADD(BV64_ADD(BV64_SUB(BV64_SUB(BV64_MUL(-2, BV32_ZEXT64(local_id_x$1)), BV64_MUL(4, BV32_ZEXT64(local_id_y$1))), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n))), $c0), 62), 0) then BV64_SUB(0, BV64_SDIV(BV64_SUB(BV64_ADD(BV64_SUB(0, BV64_ADD(BV64_ADD(BV64_SUB(BV64_SUB(BV64_MUL(-2, BV32_ZEXT64(local_id_x$1)), BV64_MUL(4, BV32_ZEXT64(local_id_y$1))), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n))), $c0), 62)), 64), 1), 64)) else BV64_SDIV(BV64_ADD(BV64_ADD(BV64_SUB(BV64_SUB(BV64_MUL(-2, BV32_ZEXT64(local_id_x$1)), BV64_MUL(4, BV32_ZEXT64(local_id_y$1))), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n))), $c0), 62), 64)))), BV32_ZEXT64(BV32_ADD(BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n), 1))) then 1 else 0))), BV1_ZEXT32((if BV64_SGE(BV32_ZEXT64(BV32_ADD(BV32_MUL(2, $tsteps), BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n))), BV64_ADD(BV64_ADD(BV64_ADD(BV32_ZEXT64(local_id_x$1), BV64_MUL(2, BV32_ZEXT64(local_id_y$1))), BV64_MUL(32, (if BV64_SLT(BV64_ADD(BV64_ADD(BV64_SUB(BV64_SUB(BV64_MUL(-2, BV32_ZEXT64(local_id_x$1)), BV64_MUL(4, BV32_ZEXT64(local_id_y$1))), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n))), $c0), 62), 0) then BV64_SUB(0, BV64_SDIV(BV64_SUB(BV64_ADD(BV64_SUB(0, BV64_ADD(BV64_ADD(BV64_SUB(BV64_SUB(BV64_MUL(-2, BV32_ZEXT64(local_id_x$1)), BV64_MUL(4, BV32_ZEXT64(local_id_y$1))), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n))), $c0), 62)), 64), 1), 64)) else BV64_SDIV(BV64_ADD(BV64_ADD(BV64_SUB(BV64_SUB(BV64_MUL(-2, BV32_ZEXT64(local_id_x$1)), BV64_MUL(4, BV32_ZEXT64(local_id_y$1))), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n))), $c0), 62), 64)))), 1)) then 1 else 0))), BV1_ZEXT32((if BV64_SGE(BV64_ADD(BV64_ADD(BV64_ADD(BV32_SEXT64($n), BV64_MUL(2, BV32_ZEXT64(local_id_x$1))), BV64_MUL(4, BV32_ZEXT64(local_id_y$1))), BV64_MUL(64, (if BV64_SLT(BV64_ADD(BV64_ADD(BV64_SUB(BV64_SUB(BV64_MUL(-2, BV32_ZEXT64(local_id_x$1)), BV64_MUL(4, BV32_ZEXT64(local_id_y$1))), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n))), $c0), 62), 0) then BV64_SUB(0, BV64_SDIV(BV64_SUB(BV64_ADD(BV64_SUB(0, BV64_ADD(BV64_ADD(BV64_SUB(BV64_SUB(BV64_MUL(-2, BV32_ZEXT64(local_id_x$1)), BV64_MUL(4, BV32_ZEXT64(local_id_y$1))), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n))), $c0), 62)), 64), 1), 64)) else BV64_SDIV(BV64_ADD(BV64_ADD(BV64_SUB(BV64_SUB(BV64_MUL(-2, BV32_ZEXT64(local_id_x$1)), BV64_MUL(4, BV32_ZEXT64(local_id_y$1))), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n))), $c0), 62), 64)))), BV64_ADD($c0, 2)) then 1 else 0))), BV1_ZEXT32((if BV64_SGE($c0, BV64_ADD(BV64_ADD(BV64_ADD(BV64_MUL(2, BV32_ZEXT64(local_id_x$1)), BV64_MUL(4, BV32_ZEXT64(local_id_y$1))), BV64_MUL(64, (if BV64_SLT(BV64_ADD(BV64_ADD(BV64_SUB(BV64_SUB(BV64_MUL(-2, BV32_ZEXT64(local_id_x$1)), BV64_MUL(4, BV32_ZEXT64(local_id_y$1))), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n))), $c0), 62), 0) then BV64_SUB(0, BV64_SDIV(BV64_SUB(BV64_ADD(BV64_SUB(0, BV64_ADD(BV64_ADD(BV64_SUB(BV64_SUB(BV64_MUL(-2, BV32_ZEXT64(local_id_x$1)), BV64_MUL(4, BV32_ZEXT64(local_id_y$1))), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n))), $c0), 62)), 64), 1), 64)) else BV64_SDIV(BV64_ADD(BV64_ADD(BV64_SUB(BV64_SUB(BV64_MUL(-2, BV32_ZEXT64(local_id_x$1)), BV64_MUL(4, BV32_ZEXT64(local_id_y$1))), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n))), $c0), 62), 64)))), 1)) then 1 else 0))), BV1_ZEXT32((if BV64_SGE($c0, BV64_ADD(BV64_ADD(BV64_ADD(BV64_MUL(2, BV32_ZEXT64(local_id_x$1)), BV64_MUL(4, BV32_ZEXT64(local_id_y$1))), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n))), BV64_MUL(64, (if BV64_SLT(BV64_ADD(BV64_ADD(BV64_SUB(BV64_SUB(BV64_MUL(-2, BV32_ZEXT64(local_id_x$1)), BV64_MUL(4, BV32_ZEXT64(local_id_y$1))), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n))), $c0), 62), 0) then BV64_SUB(0, BV64_SDIV(BV64_SUB(BV64_ADD(BV64_SUB(0, BV64_ADD(BV64_ADD(BV64_SUB(BV64_SUB(BV64_MUL(-2, BV32_ZEXT64(local_id_x$1)), BV64_MUL(4, BV32_ZEXT64(local_id_y$1))), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n))), $c0), 62)), 64), 1), 64)) else BV64_SDIV(BV64_ADD(BV64_ADD(BV64_SUB(BV64_SUB(BV64_MUL(-2, BV32_ZEXT64(local_id_x$1)), BV64_MUL(4, BV32_ZEXT64(local_id_y$1))), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n))), $c0), 62), 64))))) then 1 else 0))), BV1_ZEXT32((if BV64_SGE(BV64_SREM(BV64_ADD(BV64_ADD(BV64_SUB(BV64_ADD(BV64_ADD(BV64_MUL(-64, BV32_ZEXT64(group_id_y$1)), BV32_ZEXT64(local_id_x$1)), BV64_MUL(2, BV32_ZEXT64(local_id_y$1))), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n))), BV64_MUL(32, BV64_SDIV(BV64_ADD(BV64_ADD(BV64_SUB(BV64_SUB(BV64_MUL(-2, BV32_ZEXT64(local_id_x$1)), BV64_MUL(4, BV32_ZEXT64(local_id_y$1))), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n))), $c0), 126), 64))), 16288), 16384), 16321) then 1 else 0))), BV1_ZEXT32((if BV64_SREM(BV64_ADD(BV64_SUB(BV64_ADD(BV64_MUL(32, BV32_ZEXT64(group_id_x$1)), BV32_ZEXT64(local_id_x$1)), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n))), 8191), 8192) == 0 then 1 else 0)))), BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV1_ZEXT32((if BV64_SLE(BV32_ZEXT64(local_id_x$1), 31) then 1 else 0)), BV1_ZEXT32((if BV64_SGE(BV32_ZEXT64(local_id_x$1), -8160) then 1 else 0))), BV1_ZEXT32((if BV32_UGE(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), 2) then 1 else 0))), BV1_ZEXT32((if BV32_UGE($n, BV32_ADD(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), 1)) then 1 else 0))), BV1_ZEXT32((if BV64_SGE(BV32_SEXT64($n), BV64_ADD(BV64_ADD(BV32_ZEXT64(local_id_x$1), BV64_MUL(32, (if BV64_SLT(BV64_ADD(BV64_ADD(BV64_SUB(0, BV32_ZEXT64(local_id_x$1)), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n))), 30), 0) then BV64_SUB(0, BV64_SDIV(BV64_SUB(BV64_ADD(BV64_SUB(0, BV64_ADD(BV64_ADD(BV64_SUB(0, BV32_ZEXT64(local_id_x$1)), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n))), 30)), 32), 1), 32)) else BV64_SDIV(BV64_ADD(BV64_ADD(BV64_SUB(0, BV32_ZEXT64(local_id_x$1)), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n))), 30), 32)))), 2)) then 1 else 0))), BV1_ZEXT32((if BV64_SGE(BV64_ADD(BV32_ZEXT64(local_id_x$1), BV64_MUL(32, (if BV64_SLT(BV64_ADD(BV64_ADD(BV64_SUB(0, BV32_ZEXT64(local_id_x$1)), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n))), 30), 0) then BV64_SUB(0, BV64_SDIV(BV64_SUB(BV64_ADD(BV64_SUB(0, BV64_ADD(BV64_ADD(BV64_SUB(0, BV32_ZEXT64(local_id_x$1)), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n))), 30)), 32), 1), 32)) else BV64_SDIV(BV64_ADD(BV64_ADD(BV64_SUB(0, BV32_ZEXT64(local_id_x$1)), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n))), 30), 32)))), 1) then 1 else 0))), BV1_ZEXT32((if BV64_SGE(BV64_SREM(BV64_ADD(BV64_ADD(BV64_SUB(0, BV32_ZEXT64(local_id_x$1)), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n))), 30), 32), 30) then 1 else 0))), BV1_ZEXT32((if BV64_SGE(BV32_ZEXT64(BV32_ADD(BV32_ADD(BV32_ADD(BV32_MUL(4, $tsteps), BV32_MUL(2, BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n))), BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n)), 55)), BV64_ADD(BV64_MUL(2, BV64_SREM(BV64_ADD(BV64_ADD(BV64_SUB(0, BV32_ZEXT64(local_id_x$1)), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n))), 30), 32)), $c0)) then 1 else 0))), BV1_ZEXT32((if BV64_SGE(BV64_ADD(BV64_MUL(2, BV64_SREM(BV64_ADD(BV64_ADD(BV64_SUB(0, BV32_ZEXT64(local_id_x$1)), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n))), 30), 32)), $c0), BV32_ZEXT64(BV32_ADD(BV32_ADD(BV32_MUL(2, BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n)), BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n)), 59))) then 1 else 0))), BV1_ZEXT32((if BV64_SGE(BV64_ADD(BV64_ADD(BV64_ADD(BV64_MUL(2, BV64_SREM(BV64_ADD(BV64_ADD(BV64_SUB(0, BV32_ZEXT64(local_id_x$1)), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n))), 30), 32)), BV64_SREM(BV64_ADD(BV64_ADD(BV64_SUB(BV64_SUB(BV64_SUB(BV64_MUL(2, BV64_SREM(BV64_ADD(BV64_ADD(BV64_SUB(0, BV32_ZEXT64(local_id_x$1)), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n))), 30), 32)), BV64_MUL(128, BV32_ZEXT64(group_id_y$1))), BV32_ZEXT64(BV32_MUL(2, BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n)))), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n))), $c0), 32584), 32768)), BV64_MUL(2, BV32_ZEXT64(local_id_x$1))), BV64_MUL(64, BV64_SDIV(BV64_ADD(BV64_ADD(BV64_SUB(0, BV32_ZEXT64(local_id_x$1)), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n))), 30), 32))), BV32_ZEXT64(BV32_ADD(BV32_MUL(2, BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n)), 32703))) then 1 else 0))), BV1_ZEXT32((if BV64_ADD(BV32_ZEXT64(group_id_x$1), BV64_MUL(256, BV64_SDIV(BV64_ADD(BV64_ADD(BV64_SUB(BV64_SUB(0, BV64_SREM(BV64_ADD(BV64_ADD(BV64_SUB(0, BV32_ZEXT64(local_id_x$1)), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n))), 30), 32)), BV64_MUL(32, BV32_ZEXT64(group_id_x$1))), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n))), 8190), 8192))) == BV64_SDIV(BV64_ADD(BV64_ADD(BV64_SUB(0, BV32_ZEXT64(local_id_x$1)), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n))), 30), 32) then 1 else 0))), BV1_ZEXT32((if BV64_SREM(BV64_ADD(BV64_SUB(BV64_ADD(BV64_ADD(BV64_MUL(2, BV32_ZEXT64(local_id_x$1)), BV64_MUL(4, BV32_ZEXT64(local_id_y$1))), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n))), $c0), 63), 64) == 0 then 1 else 0)))), BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV1_ZEXT32((if BV32_UGE(BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n), 3) then 1 else 0)), BV1_ZEXT32((if BV32_UGE($n, BV32_ADD(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), 3)) then 1 else 0))), BV1_ZEXT32((if BV64_SGE(BV32_ZEXT64(BV32_ADD(BV32_ADD(BV32_MUL(2, $n), BV32_MUL(4, $tsteps)), BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n))), BV64_ADD($c0, 7)) then 1 else 0))), BV1_ZEXT32((if BV64_SGE(BV32_ZEXT64(BV32_ADD(BV32_ADD(BV32_MUL(4, $tsteps), BV32_MUL(2, BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n))), BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n))), BV64_ADD($c0, 3)) then 1 else 0))), BV1_ZEXT32((if BV64_SGE($c0, BV32_ZEXT64(BV32_ADD(BV32_ADD(BV32_MUL(2, BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n)), BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n)), 1))) then 1 else 0))), BV1_ZEXT32((if BV64_SGE(BV64_ADD($c0, 3), BV32_ZEXT64(BV32_ADD(BV32_MUL(4, $tsteps), BV32_MUL(2, BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n))))) then 1 else 0))), BV1_ZEXT32((if BV64_SGE(BV64_ADD($c0, 4), BV32_ZEXT64(BV32_ADD(BV32_ADD(BV32_MUL(4, $tsteps), BV32_MUL(2, BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n))), BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n)))) then 1 else 0))), BV1_ZEXT32((if BV64_SGE(BV64_ADD(BV32_ZEXT64(BV32_ADD($tsteps, BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n))), BV64_MUL(8192, BV64_SDIV(BV64_ADD(BV64_ADD(BV64_ADD(BV64_SUB(BV64_ADD(BV64_ADD(BV64_ADD(BV64_MUL(128, BV32_ZEXT64(group_id_x$1)), BV64_MUL(128, BV32_ZEXT64(group_id_y$1))), BV64_MUL(4, BV32_ZEXT64(local_id_x$1))), BV64_MUL(32768, BV32_ZEXT64(local_id_y$1))), BV32_ZEXT64(BV32_MUL(2, BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n)))), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n))), BV64_MUL(32767, $c0)), 32893), 32768))), BV64_ADD(BV64_ADD(BV64_ADD(BV64_ADD(BV64_ADD(BV64_MUL(32, BV32_ZEXT64(group_id_x$1)), BV64_MUL(32, BV32_ZEXT64(group_id_y$1))), BV32_ZEXT64(local_id_x$1)), BV64_MUL(8192, BV32_ZEXT64(local_id_y$1))), BV64_MUL(8192, $c0)), 8193)) then 1 else 0))), BV1_ZEXT32((if BV64_SREM(BV64_ADD(BV64_SUB(BV32_SEXT64($tsteps), BV32_ZEXT64(local_id_y$1)), 15), 16) == 0 then 1 else 0))), BV1_ZEXT32((if BV64_SREM(BV64_SUB(BV64_ADD(BV64_MUL(32, BV32_ZEXT64(group_id_x$1)), BV32_ZEXT64(local_id_x$1)), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n))), 8192) == 0 then 1 else 0)))), BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV1_ZEXT32((if BV64_SGE(BV64_ADD(BV64_MUL(32, BV32_ZEXT64(group_id_x$1)), BV32_ZEXT64(local_id_x$1)), 0) then 1 else 0)), BV1_ZEXT32((if BV64_SLE(BV64_ADD(BV64_MUL(32, BV32_ZEXT64(group_id_x$1)), BV32_ZEXT64(local_id_x$1)), 8191) then 1 else 0))), BV1_ZEXT32((if BV32_UGE(BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n), 2) then 1 else 0))), BV1_ZEXT32((if BV32_UGE($n, BV32_ADD(BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n), 1)) then 1 else 0))), BV1_ZEXT32((if BV32_UGE($n, BV32_ADD(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), 2)) then 1 else 0))), BV1_ZEXT32((if BV32_UGE(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), 1) then 1 else 0))), BV1_ZEXT32((if BV64_SGE(BV32_ZEXT64(BV32_ADD(BV32_ADD(BV32_MUL(4, $tsteps), BV32_MUL(2, BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n))), BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n))), BV64_ADD($c0, 6)) then 1 else 0))), BV1_ZEXT32((if BV64_SGE(BV64_ADD($c0, 2), BV32_ZEXT64(BV32_ADD(BV32_MUL(2, BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n)), BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n)))) then 1 else 0))), BV1_ZEXT32((if BV64_SGE(BV64_SREM(BV64_ADD(BV64_ADD(BV32_ZEXT64(BV32_SUB(BV32_MUL(-2, BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n)), BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n))), $c0), 2), 32768), BV64_MUL(128, BV32_ZEXT64(group_id_y$1))) then 1 else 0))), BV1_ZEXT32((if BV64_SGE(BV64_ADD(BV64_MUL(128, BV32_ZEXT64(group_id_y$1)), 124), BV64_SREM(BV64_ADD(BV64_ADD(BV32_ZEXT64(BV32_SUB(BV32_MUL(-2, BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n)), BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n))), $c0), 2), 32768)) then 1 else 0))), BV1_ZEXT32((if BV64_SREM(BV64_ADD(BV64_SUB(BV64_ADD(BV64_MUL(32, BV32_ZEXT64(group_id_x$1)), BV32_ZEXT64(local_id_x$1)), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n))), 1), 8192) == 0 then 1 else 0))), BV1_ZEXT32((if BV64_SREM(BV64_SUB(BV64_ADD(BV64_ADD(BV64_MUL(2, BV32_ZEXT64(local_id_x$1)), BV64_MUL(4, BV32_ZEXT64(local_id_y$1))), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n))), $c0), 64) == 0 then 1 else 0)))), BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV1_ZEXT32((if BV64_SGE(BV64_ADD(BV64_MUL(32, BV32_ZEXT64(group_id_x$1)), BV32_ZEXT64(local_id_x$1)), 0) then 1 else 0)), BV1_ZEXT32((if BV64_SLE(BV64_ADD(BV64_MUL(32, BV32_ZEXT64(group_id_x$1)), BV32_ZEXT64(local_id_x$1)), 8191) then 1 else 0))), BV1_ZEXT32((if BV32_UGE($n, BV32_ADD(BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n), 3)) then 1 else 0))), BV1_ZEXT32((if BV32_UGE(BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n), 0) then 1 else 0))), BV1_ZEXT32((if BV32_UGE(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), 2) then 1 else 0))), BV1_ZEXT32((if BV32_UGE($n, BV32_ADD(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), 1)) then 1 else 0))), BV1_ZEXT32((if BV64_SGE(BV32_ZEXT64(BV32_ADD(BV32_ADD(BV32_MUL(4, $tsteps), BV32_MUL(2, BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n))), BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n))), BV64_ADD($c0, 3)) then 1 else 0))), BV1_ZEXT32((if BV64_SGE($c0, BV32_ZEXT64(BV32_ADD(BV32_ADD(BV32_MUL(2, BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n)), BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n)), 1))) then 1 else 0))), BV1_ZEXT32((if BV64_SGE(BV64_SREM(BV64_SUB(BV64_ADD(BV32_ZEXT64(BV32_SUB(BV32_MUL(-2, BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n)), BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n))), $c0), 1), 32768), BV64_MUL(128, BV32_ZEXT64(group_id_y$1))) then 1 else 0))), BV1_ZEXT32((if BV64_SGE(BV64_ADD(BV64_MUL(128, BV32_ZEXT64(group_id_y$1)), 124), BV64_SREM(BV64_SUB(BV64_ADD(BV32_ZEXT64(BV32_SUB(BV32_MUL(-2, BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n)), BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n))), $c0), 1), 32768)) then 1 else 0))), BV1_ZEXT32((if BV64_SREM(BV64_ADD(BV64_SUB(BV64_ADD(BV64_MUL(32, BV32_ZEXT64(group_id_x$1)), BV32_ZEXT64(local_id_x$1)), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n))), 8191), 8192) == 0 then 1 else 0))), BV1_ZEXT32((if BV64_SREM(BV64_ADD(BV64_SUB(BV64_ADD(BV64_ADD(BV64_MUL(2, BV32_ZEXT64(local_id_x$1)), BV64_MUL(4, BV32_ZEXT64(local_id_y$1))), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n))), $c0), 63), 64) == 0 then 1 else 0)))), BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV1_ZEXT32((if BV32_UGE(BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n), 3) then 1 else 0)), BV1_ZEXT32((if BV32_UGE($n, BV32_ADD(BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n), 2)) then 1 else 0))), BV1_ZEXT32((if BV32_ADD(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), 2) == $n then 1 else 0))), BV1_ZEXT32((if BV64_SGE(BV32_ZEXT64(BV32_ADD(BV32_ADD($n, BV32_MUL(4, $tsteps)), BV32_MUL(2, BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n)))), BV64_ADD($c0, 10)) then 1 else 0))), BV1_ZEXT32((if BV64_SGE(BV64_ADD($c0, 2), BV32_ZEXT64(BV32_ADD($n, BV32_MUL(2, BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n))))) then 1 else 0))), BV1_ZEXT32((if BV64_SLE(BV64_SREM(BV64_SUB(BV64_ADD(BV64_ADD(BV64_ADD(BV64_ADD(BV64_ADD(BV64_ADD(BV32_SEXT64(BV32_MUL(32769, $n)), BV64_MUL(128, BV32_ZEXT64(group_id_x$1))), BV64_MUL(128, BV32_ZEXT64(group_id_y$1))), BV64_MUL(32772, BV32_ZEXT64(local_id_x$1))), BV64_MUL(32768, BV32_ZEXT64(local_id_y$1))), BV32_ZEXT64(BV32_MUL(32766, BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n)))), BV64_MUL(32767, $c0)), 65413), 32768), 125) then 1 else 0))), BV1_ZEXT32((if BV64_SREM(BV64_SUB(BV64_ADD(BV64_MUL(32, BV32_ZEXT64(group_id_x$1)), BV32_ZEXT64(local_id_x$1)), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n))), 8192) == 0 then 1 else 0))), BV1_ZEXT32((if BV64_SREM(BV64_SUB(BV64_SUB(BV64_ADD(BV64_ADD(BV32_SEXT64($n), BV64_MUL(2, BV32_ZEXT64(local_id_x$1))), BV64_MUL(4, BV32_ZEXT64(local_id_y$1))), $c0), 2), 64) == 0 then 1 else 0)))), BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV1_ZEXT32((if BV32_SGE($tsteps, 2) then 1 else 0)), BV1_ZEXT32((if BV32_ZEXT64(group_id_y$1) == 0 then 1 else 0))), BV1_ZEXT32((if BV32_ZEXT64(local_id_y$1) == 0 then 1 else 0))), BV1_ZEXT32((if BV32_UGE(BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n), 3) then 1 else 0))), BV1_ZEXT32((if BV32_UGE($n, BV32_ADD(BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n), 2)) then 1 else 0))), BV1_ZEXT32((if BV32_UGE($n, BV32_ADD(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), 3)) then 1 else 0))), BV1_ZEXT32((if BV32_UGE(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), 1) then 1 else 0))), BV1_ZEXT32((if $c0 == BV32_ZEXT64(BV32_ADD(BV32_MUL(2, BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n)), BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n))) then 1 else 0))), BV1_ZEXT32((if BV64_SREM(BV64_SUB(BV64_ADD(BV64_MUL(32, BV32_ZEXT64(group_id_x$1)), BV32_ZEXT64(local_id_x$1)), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n))), 8192) == 0 then 1 else 0)))), BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV1_ZEXT32((if BV32_ZEXT64(group_id_x$1) == 0 then 1 else 0)), BV1_ZEXT32((if BV64_SGE(BV32_SEXT64($n), BV64_ADD(BV32_ZEXT64(local_id_x$1), 2)) then 1 else 0))), BV1_ZEXT32((if BV64_SGE(BV32_ZEXT64(local_id_x$1), 1) then 1 else 0))), BV1_ZEXT32((if BV64_SLE(BV32_ZEXT64(local_id_x$1), 2) then 1 else 0))), BV1_ZEXT32((if BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n)) == BV32_ZEXT64(local_id_x$1) then 1 else 0))), BV1_ZEXT32((if BV32_ADD(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), 2) == $n then 1 else 0))), BV1_ZEXT32((if BV64_SGE(BV32_SEXT64(BV32_ADD($n, BV32_MUL(4, $tsteps))), BV64_ADD($c0, 4)) then 1 else 0))), BV1_ZEXT32((if BV64_SGE(BV64_ADD($c0, 2), BV64_ADD(BV32_SEXT64($n), BV64_MUL(2, BV32_ZEXT64(local_id_x$1)))) then 1 else 0))), BV1_ZEXT32((if BV64_SLE(BV64_SREM(BV64_ADD(BV64_ADD(BV64_ADD(BV64_ADD(BV32_SEXT64($n), BV64_MUL(128, BV32_ZEXT64(group_id_y$1))), BV64_MUL(2, BV32_ZEXT64(local_id_x$1))), BV64_MUL(32767, $c0)), 123), 32768), 125) then 1 else 0))), BV1_ZEXT32((if BV64_SREM(BV64_SUB(BV64_SUB(BV64_ADD(BV64_ADD(BV32_SEXT64($n), BV64_MUL(2, BV32_ZEXT64(local_id_x$1))), BV64_MUL(4, BV32_ZEXT64(local_id_y$1))), $c0), 2), 64) == 0 then 1 else 0)))), BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV1_ZEXT32((if BV32_ZEXT64(group_id_x$1) == 0 then 1 else 0)), BV1_ZEXT32((if BV32_ZEXT64(local_id_x$1) == 2 then 1 else 0))), BV1_ZEXT32((if BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n) == 2 then 1 else 0))), BV1_ZEXT32((if BV32_UGE($n, BV32_ADD(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), 3)) then 1 else 0))), BV1_ZEXT32((if BV64_SGE($c0, BV32_ZEXT64(BV32_ADD(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), 5))) then 1 else 0))), BV1_ZEXT32((if BV64_SGE($c0, BV32_SEXT64(BV32_ADD(BV32_MUL(4, $tsteps), 1))) then 1 else 0))), BV1_ZEXT32((if BV64_SGE($c0, BV32_ZEXT64(BV32_ADD(BV32_MUL(4, $tsteps), BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n)))) then 1 else 0))), BV1_ZEXT32((if BV64_SGE(BV32_ZEXT64(BV32_ADD(BV32_ADD(BV32_MUL(4, $tsteps), BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n)), 1)), $c0) then 1 else 0))), BV1_ZEXT32((if BV64_SGE(BV64_ADD(BV32_SEXT64($tsteps), BV64_MUL(8192, BV64_SDIV(BV64_ADD(BV64_ADD(BV64_ADD(BV64_MUL(128, BV32_ZEXT64(group_id_y$1)), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n))), BV64_MUL(32767, $c0)), 129), 32768))), BV64_ADD(BV64_ADD(BV64_MUL(32, BV32_ZEXT64(group_id_y$1)), BV64_MUL(8192, $c0)), 1)) then 1 else 0))), BV1_ZEXT32((if BV64_SREM(BV64_ADD(BV64_SUB(BV32_SEXT64($tsteps), BV32_ZEXT64(local_id_y$1)), 15), 16) == 0 then 1 else 0)))), BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV1_ZEXT32((if BV32_UGE(BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n), 3) then 1 else 0)), BV1_ZEXT32((if BV32_ADD(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), 2) == $n then 1 else 0))), BV1_ZEXT32((if BV64_ADD($c0, 6) == BV32_ZEXT64(BV32_ADD(BV32_ADD($n, BV32_MUL(4, $tsteps)), BV32_MUL(2, BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n)))) then 1 else 0))), BV1_ZEXT32((if BV64_SLE(BV64_SREM(BV64_ADD(BV64_ADD(BV64_ADD(BV64_ADD(BV64_ADD(BV64_ADD(BV32_SEXT64(BV32_MUL(8191, $tsteps)), BV64_MUL(32, BV32_ZEXT64(group_id_x$1))), BV64_MUL(32, BV32_ZEXT64(group_id_y$1))), BV64_MUL(8193, BV32_ZEXT64(local_id_x$1))), BV64_MUL(8192, BV32_ZEXT64(local_id_y$1))), BV32_ZEXT64(BV32_MUL(8191, BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n)))), 8224), 8192), 31) then 1 else 0))), BV1_ZEXT32((if BV64_SREM(BV64_SUB(BV64_ADD(BV64_MUL(32, BV32_ZEXT64(group_id_x$1)), BV32_ZEXT64(local_id_x$1)), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n))), 8192) == 0 then 1 else 0))), BV1_ZEXT32((if BV64_SREM(BV64_ADD(BV64_ADD(BV32_SEXT64(BV32_SUB(0, $tsteps)), BV32_ZEXT64(local_id_y$1)), 1), 16) == 0 then 1 else 0)))), BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV1_ZEXT32((if $tsteps == 1 then 1 else 0)), BV1_ZEXT32((if BV32_ZEXT64(group_id_y$1) == 0 then 1 else 0))), BV1_ZEXT32((if BV32_ZEXT64(local_id_y$1) == 0 then 1 else 0))), BV1_ZEXT32((if BV32_UGE(BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n), 3) then 1 else 0))), BV1_ZEXT32((if BV32_UGE($n, BV32_ADD(BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n), 2)) then 1 else 0))), BV1_ZEXT32((if BV32_UGE($n, BV32_ADD(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), 3)) then 1 else 0))), BV1_ZEXT32((if BV32_UGE(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), 1) then 1 else 0))), BV1_ZEXT32((if $c0 == BV32_ZEXT64(BV32_ADD(BV32_MUL(2, BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n)), BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n))) then 1 else 0))), BV1_ZEXT32((if BV64_SREM(BV64_SUB(BV64_ADD(BV64_MUL(32, BV32_ZEXT64(group_id_x$1)), BV32_ZEXT64(local_id_x$1)), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n))), 8192) == 0 then 1 else 0)))), BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV1_ZEXT32((if BV32_SGE($tsteps, 2) then 1 else 0)), BV1_ZEXT32((if BV32_ZEXT64(group_id_x$1) == 0 then 1 else 0))), BV1_ZEXT32((if BV32_ZEXT64(group_id_y$1) == 0 then 1 else 0))), BV1_ZEXT32((if BV32_ZEXT64(local_id_x$1) == 2 then 1 else 0))), BV1_ZEXT32((if BV32_ZEXT64(local_id_y$1) == 0 then 1 else 0))), BV1_ZEXT32((if BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n) == 2 then 1 else 0))), BV1_ZEXT32((if BV32_UGE($n, BV32_ADD(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), 3)) then 1 else 0))), BV1_ZEXT32((if BV32_UGE(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), 1) then 1 else 0))), BV1_ZEXT32((if $c0 == BV32_ZEXT64(BV32_ADD(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), 4)) then 1 else 0)))), BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV1_ZEXT32((if BV32_ZEXT64(group_id_x$1) == 0 then 1 else 0)), BV1_ZEXT32((if BV32_ZEXT64(group_id_y$1) == 0 then 1 else 0))), BV1_ZEXT32((if BV32_ZEXT64(local_id_x$1) == 1 then 1 else 0))), BV1_ZEXT32((if BV32_ZEXT64(local_id_y$1) == 0 then 1 else 0))), BV1_ZEXT32((if BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n) == 1 then 1 else 0))), BV1_ZEXT32((if BV32_UGE($n, BV32_ADD(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), 3)) then 1 else 0))), BV1_ZEXT32((if $c0 == BV32_ZEXT64(BV32_ADD(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), 2)) then 1 else 0)))), BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV1_ZEXT32((if BV32_ZEXT64(group_id_x$1) == 0 then 1 else 0)), BV1_ZEXT32((if BV32_ZEXT64(local_id_x$1) == 2 then 1 else 0))), BV1_ZEXT32((if BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n) == 2 then 1 else 0))), BV1_ZEXT32((if BV32_ADD(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), 2) == $n then 1 else 0))), BV1_ZEXT32((if BV64_ADD($c0, 2) == BV32_SEXT64(BV32_ADD($n, BV32_MUL(4, $tsteps))) then 1 else 0))), BV1_ZEXT32((if BV64_SGE(BV64_SREM(BV64_ADD(BV64_SUB(BV32_SEXT64($tsteps), BV64_MUL(32, BV32_ZEXT64(group_id_y$1))), 8159), 8192), 8160) then 1 else 0))), BV1_ZEXT32((if BV64_SREM(BV64_ADD(BV64_ADD(BV32_SEXT64(BV32_SUB(0, $tsteps)), BV32_ZEXT64(local_id_y$1)), 1), 16) == 0 then 1 else 0)))), BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV1_ZEXT32((if $tsteps == 1 then 1 else 0)), BV1_ZEXT32((if BV32_ZEXT64(group_id_x$1) == 0 then 1 else 0))), BV1_ZEXT32((if BV32_ZEXT64(group_id_y$1) == 0 then 1 else 0))), BV1_ZEXT32((if BV32_ZEXT64(local_id_x$1) == 2 then 1 else 0))), BV1_ZEXT32((if BV32_ZEXT64(local_id_y$1) == 0 then 1 else 0))), BV1_ZEXT32((if BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n) == 2 then 1 else 0))), BV1_ZEXT32((if BV32_UGE($n, BV32_ADD(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), 3)) then 1 else 0))), BV1_ZEXT32((if BV32_UGE(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), 1) then 1 else 0))), BV1_ZEXT32((if $c0 == BV32_ZEXT64(BV32_ADD(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), 4)) then 1 else 0)))) != 0 then 1 else 0) != 0;
    assert {:procedure_wide_invariant} {:do_not_predicate} {:sourceloc_num 4} {:thread 1} (if _WRITE_HAS_OCCURRED_$$A ==> BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV1_ZEXT32((if BV64_SGE(BV64_ADD(BV64_MUL(32, BV32_ZEXT64(group_id_x$1)), BV32_ZEXT64(local_id_x$1)), 0) then 1 else 0)), BV1_ZEXT32((if BV64_SLE(BV64_ADD(BV64_MUL(32, BV32_ZEXT64(group_id_x$1)), BV32_ZEXT64(local_id_x$1)), 8191) then 1 else 0))), BV1_ZEXT32((if BV32_UGE($n, BV32_ADD(BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n), 2)) then 1 else 0))), BV1_ZEXT32((if BV32_UGE(BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n), 1) then 1 else 0))), BV1_ZEXT32((if BV32_UGE($n, BV32_ADD(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), 2)) then 1 else 0))), BV1_ZEXT32((if BV32_UGE(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), 1) then 1 else 0))), BV1_ZEXT32((if BV64_SGE(BV32_ZEXT64(BV32_ADD(BV32_ADD(BV32_MUL(4, $tsteps), BV32_MUL(2, BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n))), BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n))), BV64_ADD($c0, 4)) then 1 else 0))), BV1_ZEXT32((if BV64_SGE($c0, BV32_ZEXT64(BV32_ADD(BV32_MUL(2, BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n)), BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n)))) then 1 else 0))), BV1_ZEXT32((if BV64_SGE(BV64_SREM(BV64_ADD(BV32_ZEXT64(BV32_SUB(BV32_MUL(-2, BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n)), BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n))), $c0), 32768), BV64_MUL(128, BV32_ZEXT64(group_id_y$1))) then 1 else 0))), BV1_ZEXT32((if BV64_SGE(BV64_ADD(BV64_MUL(128, BV32_ZEXT64(group_id_y$1)), 124), BV64_SREM(BV64_ADD(BV32_ZEXT64(BV32_SUB(BV32_MUL(-2, BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n)), BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n))), $c0), 32768)) then 1 else 0))), BV1_ZEXT32((if BV64_SREM(BV64_SUB(BV64_ADD(BV64_MUL(32, BV32_ZEXT64(group_id_x$1)), BV32_ZEXT64(local_id_x$1)), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n))), 8192) == 0 then 1 else 0))), BV1_ZEXT32((if BV64_SREM(BV64_SUB(BV64_ADD(BV64_ADD(BV64_MUL(2, BV32_ZEXT64(local_id_x$1)), BV64_MUL(4, BV32_ZEXT64(local_id_y$1))), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n))), $c0), 64) == 0 then 1 else 0))) != 0 then 1 else 0) != 0;
    assert {:block_sourceloc} {:sourceloc_num 50} p32$1 ==> true;
    v20$1 := (if p32$1 then BV64_SLT(BV32_SEXT64(BV32_SUB($tsteps, 1)), BV64_SUB(BV64_SDIV(BV64_ADD($c0, 1), 4), 1)) else v20$1);
    v20$2 := (if p32$2 then BV64_SLT(BV32_SEXT64(BV32_SUB($tsteps, 1)), BV64_SUB(BV64_SDIV(BV64_ADD($c0, 1), 4), 1)) else v20$2);
    p33$1 := false;
    p33$2 := false;
    p34$1 := false;
    p34$2 := false;
    p35$1 := false;
    p35$2 := false;
    p36$1 := false;
    p36$2 := false;
    p37$1 := false;
    p37$2 := false;
    p38$1 := false;
    p38$2 := false;
    p39$1 := false;
    p39$2 := false;
    p40$1 := false;
    p40$2 := false;
    p41$1 := false;
    p41$2 := false;
    p42$1 := false;
    p42$2 := false;
    p43$1 := false;
    p43$2 := false;
    p44$1 := false;
    p44$2 := false;
    p45$1 := false;
    p45$2 := false;
    p46$1 := false;
    p46$2 := false;
    p34$1 := (if p32$1 && v20$1 then v20$1 else p34$1);
    p34$2 := (if p32$2 && v20$2 then v20$2 else p34$2);
    p33$1 := (if p32$1 && !v20$1 then !v20$1 else p33$1);
    p33$2 := (if p32$2 && !v20$2 then !v20$2 else p33$2);
    $13$1 := (if p33$1 then BV64_SUB(BV64_SDIV(BV64_ADD($c0, 1), 4), 1) else $13$1);
    $13$2 := (if p33$2 then BV64_SUB(BV64_SDIV(BV64_ADD($c0, 1), 4), 1) else $13$2);
    $13$1 := (if p34$1 then BV32_SEXT64(BV32_SUB($tsteps, 1)) else $13$1);
    $13$2 := (if p34$2 then BV32_SEXT64(BV32_SUB($tsteps, 1)) else $13$2);
    v21$1 := (if p32$1 then BV64_SLT($13$1, BV64_ADD(BV64_SUB(0, $c1.0$1), BV64_SDIV(BV64_SUB(BV64_ADD($c0, BV64_MUL(2, $c1.0$1)), 1), 4))) else v21$1);
    v21$2 := (if p32$2 then BV64_SLT($13$2, BV64_ADD(BV64_SUB(0, $c1.0$2), BV64_SDIV(BV64_SUB(BV64_ADD($c0, BV64_MUL(2, $c1.0$2)), 1), 4))) else v21$2);
    p36$1 := (if p32$1 && v21$1 then v21$1 else p36$1);
    p36$2 := (if p32$2 && v21$2 then v21$2 else p36$2);
    p35$1 := (if p32$1 && !v21$1 then !v21$1 else p35$1);
    p35$2 := (if p32$2 && !v21$2 then !v21$2 else p35$2);
    $15$1 := (if p35$1 then BV64_ADD(BV64_SUB(0, $c1.0$1), BV64_SDIV(BV64_SUB(BV64_ADD($c0, BV64_MUL(2, $c1.0$1)), 1), 4)) else $15$1);
    $15$2 := (if p35$2 then BV64_ADD(BV64_SUB(0, $c1.0$2), BV64_SDIV(BV64_SUB(BV64_ADD($c0, BV64_MUL(2, $c1.0$2)), 1), 4)) else $15$2);
    v22$1 := (if p36$1 then BV64_SLT(BV32_SEXT64(BV32_SUB($tsteps, 1)), BV64_SUB(BV64_SDIV(BV64_ADD($c0, 1), 4), 1)) else v22$1);
    v22$2 := (if p36$2 then BV64_SLT(BV32_SEXT64(BV32_SUB($tsteps, 1)), BV64_SUB(BV64_SDIV(BV64_ADD($c0, 1), 4), 1)) else v22$2);
    p38$1 := (if p36$1 && v22$1 then v22$1 else p38$1);
    p38$2 := (if p36$2 && v22$2 then v22$2 else p38$2);
    p37$1 := (if p36$1 && !v22$1 then !v22$1 else p37$1);
    p37$2 := (if p36$2 && !v22$2 then !v22$2 else p37$2);
    $14$1 := (if p37$1 then BV64_SUB(BV64_SDIV(BV64_ADD($c0, 1), 4), 1) else $14$1);
    $14$2 := (if p37$2 then BV64_SUB(BV64_SDIV(BV64_ADD($c0, 1), 4), 1) else $14$2);
    $14$1 := (if p38$1 then BV32_SEXT64(BV32_SUB($tsteps, 1)) else $14$1);
    $14$2 := (if p38$2 then BV32_SEXT64(BV32_SUB($tsteps, 1)) else $14$2);
    $15$1 := (if p36$1 then $14$1 else $15$1);
    $15$2 := (if p36$2 then $14$2 else $15$2);
    v23$1 := (if p32$1 then BV64_SLE($c2.0$1, $15$1) else v23$1);
    v23$2 := (if p32$2 then BV64_SLE($c2.0$2, $15$2) else v23$2);
    p39$1 := (if p32$1 && v23$1 then v23$1 else p39$1);
    p39$2 := (if p32$2 && v23$2 then v23$2 else p39$2);
    p32$1 := (if p32$1 && !v23$1 then v23$1 else p32$1);
    p32$2 := (if p32$2 && !v23$2 then v23$2 else p32$2);
    v24$1 := (if p39$1 then BV64_SLT(BV64_ADD(BV64_SUB(BV64_SUB(BV64_ADD(BV64_SUB(BV64_SUB(BV32_SEXT64(BV32_SUB(0, $n)), BV64_MUL(2, v2$1)), BV64_MUL(4, v3$1)), $c0), BV64_MUL(2, $c1.0$1)), BV64_MUL(4, $c2.0$1)), 1), 0) else v24$1);
    v24$2 := (if p39$2 then BV64_SLT(BV64_ADD(BV64_SUB(BV64_SUB(BV64_ADD(BV64_SUB(BV64_SUB(BV32_SEXT64(BV32_SUB(0, $n)), BV64_MUL(2, v2$2)), BV64_MUL(4, v3$2)), $c0), BV64_MUL(2, $c1.0$2)), BV64_MUL(4, $c2.0$2)), 1), 0) else v24$2);
    p41$1 := (if p39$1 && v24$1 then v24$1 else p41$1);
    p41$2 := (if p39$2 && v24$2 then v24$2 else p41$2);
    p40$1 := (if p39$1 && !v24$1 then !v24$1 else p40$1);
    p40$2 := (if p39$2 && !v24$2 then !v24$2 else p40$2);
    $16$1 := (if p40$1 then BV64_SDIV(BV64_ADD(BV64_SUB(BV64_SUB(BV64_ADD(BV64_SUB(BV64_SUB(BV32_SEXT64(BV32_SUB(0, $n)), BV64_MUL(2, v2$1)), BV64_MUL(4, v3$1)), $c0), BV64_MUL(2, $c1.0$1)), BV64_MUL(4, $c2.0$1)), 1), 64) else $16$1);
    $16$2 := (if p40$2 then BV64_SDIV(BV64_ADD(BV64_SUB(BV64_SUB(BV64_ADD(BV64_SUB(BV64_SUB(BV32_SEXT64(BV32_SUB(0, $n)), BV64_MUL(2, v2$2)), BV64_MUL(4, v3$2)), $c0), BV64_MUL(2, $c1.0$2)), BV64_MUL(4, $c2.0$2)), 1), 64) else $16$2);
    $16$1 := (if p41$1 then BV64_SUB(0, BV64_SDIV(BV64_SUB(BV64_ADD(BV64_SUB(0, BV64_ADD(BV64_SUB(BV64_SUB(BV64_ADD(BV64_SUB(BV64_SUB(BV32_SEXT64(BV32_SUB(0, $n)), BV64_MUL(2, v2$1)), BV64_MUL(4, v3$1)), $c0), BV64_MUL(2, $c1.0$1)), BV64_MUL(4, $c2.0$1)), 1)), 64), 1), 64)) else $16$1);
    $16$2 := (if p41$2 then BV64_SUB(0, BV64_SDIV(BV64_SUB(BV64_ADD(BV64_SUB(0, BV64_ADD(BV64_SUB(BV64_SUB(BV64_ADD(BV64_SUB(BV64_SUB(BV32_SEXT64(BV32_SUB(0, $n)), BV64_MUL(2, v2$2)), BV64_MUL(4, v3$2)), $c0), BV64_MUL(2, $c1.0$2)), BV64_MUL(4, $c2.0$2)), 1)), 64), 1), 64)) else $16$2);
    v25$1 := (if p39$1 then BV64_SGT(v3$1, BV64_ADD(BV64_ADD(v3$1, BV64_MUL(16, $16$1)), 16)) else v25$1);
    v25$2 := (if p39$2 then BV64_SGT(v3$2, BV64_ADD(BV64_ADD(v3$2, BV64_MUL(16, $16$2)), 16)) else v25$2);
    p42$1 := (if p39$1 && v25$1 then v25$1 else p42$1);
    p42$2 := (if p39$2 && v25$2 then v25$2 else p42$2);
    p43$1 := (if p39$1 && !v25$1 then !v25$1 else p43$1);
    p43$2 := (if p39$2 && !v25$2 then !v25$2 else p43$2);
    $17$1 := (if p42$1 then v3$1 else $17$1);
    $17$2 := (if p42$2 then v3$2 else $17$2);
    v26$1 := (if p43$1 then BV64_SLT(BV64_ADD(BV64_SUB(BV64_SUB(BV64_ADD(BV64_SUB(BV64_SUB(BV32_SEXT64(BV32_SUB(0, $n)), BV64_MUL(2, v2$1)), BV64_MUL(4, v3$1)), $c0), BV64_MUL(2, $c1.0$1)), BV64_MUL(4, $c2.0$1)), 1), 0) else v26$1);
    v26$2 := (if p43$2 then BV64_SLT(BV64_ADD(BV64_SUB(BV64_SUB(BV64_ADD(BV64_SUB(BV64_SUB(BV32_SEXT64(BV32_SUB(0, $n)), BV64_MUL(2, v2$2)), BV64_MUL(4, v3$2)), $c0), BV64_MUL(2, $c1.0$2)), BV64_MUL(4, $c2.0$2)), 1), 0) else v26$2);
    p45$1 := (if p43$1 && v26$1 then v26$1 else p45$1);
    p45$2 := (if p43$2 && v26$2 then v26$2 else p45$2);
    p44$1 := (if p43$1 && !v26$1 then !v26$1 else p44$1);
    p44$2 := (if p43$2 && !v26$2 then !v26$2 else p44$2);
    $18$1 := (if p44$1 then BV64_SDIV(BV64_ADD(BV64_SUB(BV64_SUB(BV64_ADD(BV64_SUB(BV64_SUB(BV32_SEXT64(BV32_SUB(0, $n)), BV64_MUL(2, v2$1)), BV64_MUL(4, v3$1)), $c0), BV64_MUL(2, $c1.0$1)), BV64_MUL(4, $c2.0$1)), 1), 64) else $18$1);
    $18$2 := (if p44$2 then BV64_SDIV(BV64_ADD(BV64_SUB(BV64_SUB(BV64_ADD(BV64_SUB(BV64_SUB(BV32_SEXT64(BV32_SUB(0, $n)), BV64_MUL(2, v2$2)), BV64_MUL(4, v3$2)), $c0), BV64_MUL(2, $c1.0$2)), BV64_MUL(4, $c2.0$2)), 1), 64) else $18$2);
    $18$1 := (if p45$1 then BV64_SUB(0, BV64_SDIV(BV64_SUB(BV64_ADD(BV64_SUB(0, BV64_ADD(BV64_SUB(BV64_SUB(BV64_ADD(BV64_SUB(BV64_SUB(BV32_SEXT64(BV32_SUB(0, $n)), BV64_MUL(2, v2$1)), BV64_MUL(4, v3$1)), $c0), BV64_MUL(2, $c1.0$1)), BV64_MUL(4, $c2.0$1)), 1)), 64), 1), 64)) else $18$1);
    $18$2 := (if p45$2 then BV64_SUB(0, BV64_SDIV(BV64_SUB(BV64_ADD(BV64_SUB(0, BV64_ADD(BV64_SUB(BV64_SUB(BV64_ADD(BV64_SUB(BV64_SUB(BV32_SEXT64(BV32_SUB(0, $n)), BV64_MUL(2, v2$2)), BV64_MUL(4, v3$2)), $c0), BV64_MUL(2, $c1.0$2)), BV64_MUL(4, $c2.0$2)), 1)), 64), 1), 64)) else $18$2);
    $17$1 := (if p43$1 then BV64_ADD(BV64_ADD(v3$1, BV64_MUL(16, $18$1)), 16) else $17$1);
    $17$2 := (if p43$2 then BV64_ADD(BV64_ADD(v3$2, BV64_MUL(16, $18$2)), 16) else $17$2);
    $c4.0$1 := (if p39$1 then $17$1 else $c4.0$1);
    $c4.0$2 := (if p39$2 then $17$2 else $c4.0$2);
    p46$1 := (if p39$1 then true else p46$1);
    p46$2 := (if p39$2 then true else p46$2);
    _READ_HAS_OCCURRED_$$A$ghost$$64 := _READ_HAS_OCCURRED_$$A;
    _WRITE_HAS_OCCURRED_$$A$ghost$$64 := _WRITE_HAS_OCCURRED_$$A;
    assume {:captureState "loop_entry_state_2_0"} true;
    goto $64;

  $64:
    assume {:captureState "loop_head_state_2"} true;
    
    
    
    
assert  my_inv (  true ,  true ,  true ,  true ,  true ,  true ,  true ,  true ,  true ,  true ,  true ,  true ,  true ,  true ,  true ,  true ,  true ,  true ,  true ,  true ,  true ,  true ,  true ,  true ,  true ,  true ,  true ,  true ,  true ,  true ,  true ,  true ,  true ,  true ,  true ,  true ,  true ,  true ,  true ,  true ,  (  _READ_HAS_OCCURRED_$$A ==> _WATCHED_OFFSET mod 1 == 0 mod 1 || _WATCHED_OFFSET mod 1 == 0 mod 1 || _WATCHED_OFFSET mod 1 == 0 mod 1 || _WATCHED_OFFSET mod 1 == 0 mod 1 || _WATCHED_OFFSET mod 1 == 0 mod 1 || _WATCHED_OFFSET mod 1 == 0 mod 1 || _WATCHED_OFFSET mod 1 == 0 mod 1 || _WATCHED_OFFSET mod 1 == 0 mod 1 || _WATCHED_OFFSET mod 1 == 0 mod 1 ) ,  (  _WRITE_HAS_OCCURRED_$$A ==> _WATCHED_OFFSET mod 1 == 0 mod 1 ) ,  (  !p39$1 ==> _READ_HAS_OCCURRED_$$A$ghost$$64 == _READ_HAS_OCCURRED_$$A ) ,  (  !p39$1 ==> _WRITE_HAS_OCCURRED_$$A$ghost$$64 == _WRITE_HAS_OCCURRED_$$A )  ); 


    assume {:predicate "p46"} {:dominator_predicate "p39"} true;
    assert p46$1 ==> p32$1;
    assert p46$2 ==> p32$2;
    assert p32$1 ==> p6$1;
    assert p32$2 ==> p6$2;
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
assert  my_inv (  true ,  true ,  true ,  true ,  true ,  true ,  true ,  true ,  true ,  true ,  true ,  true ,  true ,  true ,  true ,  true ,  true ,  true ,  true ,  ( p46$1 ==> $c4.0$1 mod 16 == $17$1 mod 16 )  && ( p46$2 ==> $c4.0$2 mod 16 == $17$2 mod 16 ) ,  ( p46$1 ==> BV32_SLE($c4.0$1, $17$1) )  && ( p46$2 ==> BV32_SLE($c4.0$2, $17$2) ) ,  ( p46$1 ==> BV32_SGE($c4.0$1, $17$1) )  && ( p46$2 ==> BV32_SGE($c4.0$2, $17$2) ) ,  ( p46$1 ==> BV32_ULE($c4.0$1, $17$1) )  && ( p46$2 ==> BV32_ULE($c4.0$2, $17$2) ) ,  ( p46$1 ==> BV32_UGE($c4.0$1, $17$1) )  && ( p46$2 ==> BV32_UGE($c4.0$2, $17$2) ) ,  ( p46$1 ==> p46$1 ==> BV64_SLT($c1.0$1, $3$1) && BV64_SGE(BV32_SEXT64($n), BV64_ADD(BV64_ADD(BV32_ZEXT64(local_id_x$1), $c1.0$1), 2)) && BV64_SGE(BV64_ADD(BV32_ZEXT64(local_id_x$1), $c1.0$1), 1) && BV64_SLE($c2.0$1, $15$1) )  && ( p46$2 ==> p46$2 ==> BV64_SLT($c1.0$2, $3$2) && BV64_SGE(BV32_SEXT64($n), BV64_ADD(BV64_ADD(BV32_ZEXT64(local_id_x$2), $c1.0$2), 2)) && BV64_SGE(BV64_ADD(BV32_ZEXT64(local_id_x$2), $c1.0$2), 1) && BV64_SLE($c2.0$2, $15$2) ) ,  (  BV64_SLT($c1.0$1, $3$1) && BV64_SGE(BV32_SEXT64($n), BV64_ADD(BV64_ADD(BV32_ZEXT64(local_id_x$1), $c1.0$1), 2)) && BV64_SGE(BV64_ADD(BV32_ZEXT64(local_id_x$1), $c1.0$1), 1) && BV64_SLE($c2.0$1, $15$1) && BV64_SLE($c4.0$1, $21$1) ==> p46$1 )  && (  BV64_SLT($c1.0$2, $3$2) && BV64_SGE(BV32_SEXT64($n), BV64_ADD(BV64_ADD(BV32_ZEXT64(local_id_x$2), $c1.0$2), 2)) && BV64_SGE(BV64_ADD(BV32_ZEXT64(local_id_x$2), $c1.0$2), 1) && BV64_SLE($c2.0$2, $15$2) && BV64_SLE($c4.0$2, $21$2) ==> p46$2 ) ,  (  _READ_HAS_OCCURRED_$$A ==> BV64_SLT($c1.0$1, $3$1) ) ,  (  _READ_HAS_OCCURRED_$$A ==> BV64_SGE(BV32_SEXT64($n), BV64_ADD(BV64_ADD(BV32_ZEXT64(local_id_x$1), $c1.0$1), 2)) ) ,  (  _READ_HAS_OCCURRED_$$A ==> BV64_SGE(BV64_ADD(BV32_ZEXT64(local_id_x$1), $c1.0$1), 1) ) ,  (  _READ_HAS_OCCURRED_$$A ==> BV64_SLE($c2.0$1, $15$1) ) ,  (  _WRITE_HAS_OCCURRED_$$A ==> BV64_SLT($c1.0$1, $3$1) ) ,  (  _WRITE_HAS_OCCURRED_$$A ==> BV64_SGE(BV32_SEXT64($n), BV64_ADD(BV64_ADD(BV32_ZEXT64(local_id_x$1), $c1.0$1), 2)) ) ,  (  _WRITE_HAS_OCCURRED_$$A ==> BV64_SGE(BV64_ADD(BV32_ZEXT64(local_id_x$1), $c1.0$1), 1) ) ,  (  _WRITE_HAS_OCCURRED_$$A ==> BV64_SLE($c2.0$1, $15$1) ) ,  true ,  true ,  true ,  true ,  true ,  true ,  true ,  true ,  true ,  true  ); 


    assert {:procedure_wide_invariant} {:do_not_predicate} {:sourceloc_num 5} {:thread 1} (if _READ_HAS_OCCURRED_$$A ==> BV32_OR(BV32_OR(BV32_OR(BV32_OR(BV32_OR(BV32_OR(BV32_OR(BV32_OR(BV32_OR(BV32_OR(BV32_OR(BV32_OR(BV32_OR(BV32_OR(BV32_OR(BV32_OR(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV1_ZEXT32((if BV32_UGE(BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n), 3) then 1 else 0)), BV1_ZEXT32((if BV32_UGE($n, BV32_ADD(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), 3)) then 1 else 0))), BV1_ZEXT32((if BV64_SGE(BV32_ZEXT64(BV32_ADD(BV32_ADD(BV32_MUL(4, $tsteps), BV32_MUL(2, BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n))), BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n))), BV64_ADD($c0, 5)) then 1 else 0))), BV1_ZEXT32((if BV64_SGE(BV32_SEXT64($n), BV64_ADD(BV64_ADD(BV64_SREM(BV64_ADD(BV64_ADD(BV64_ADD(BV64_MUL(32, BV32_ZEXT64(group_id_x$1)), BV32_ZEXT64(local_id_x$1)), BV32_ZEXT64(BV32_MUL(8191, BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n)))), 1), 8192), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n))), 1)) then 1 else 0))), BV1_ZEXT32((if BV64_SLE(BV64_SREM(BV64_ADD(BV64_ADD(BV64_ADD(BV64_MUL(32, BV32_ZEXT64(group_id_x$1)), BV32_ZEXT64(local_id_x$1)), BV32_ZEXT64(BV32_MUL(8191, BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n)))), 1), 8192), 1) then 1 else 0))), BV1_ZEXT32((if BV64_SGE(BV64_ADD($c0, 1), BV64_ADD(BV64_ADD(BV64_MUL(2, BV64_SREM(BV64_ADD(BV64_ADD(BV64_ADD(BV64_MUL(32, BV32_ZEXT64(group_id_x$1)), BV32_ZEXT64(local_id_x$1)), BV32_ZEXT64(BV32_MUL(8191, BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n)))), 1), 8192)), BV32_ZEXT64(BV32_MUL(2, BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n)))), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n)))) then 1 else 0))), BV1_ZEXT32((if BV64_SGE(BV64_MUL(64, (if BV64_SLT(BV64_ADD(BV64_ADD(BV64_ADD(BV64_ADD(BV64_MUL(2, BV32_ZEXT64(local_id_x$1)), BV64_MUL(4, BV32_ZEXT64(local_id_y$1))), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n))), BV64_MUL(63, $c0)), 1), 0) then BV64_SUB(0, BV64_SDIV(BV64_SUB(BV64_ADD(BV64_SUB(0, BV64_ADD(BV64_ADD(BV64_ADD(BV64_ADD(BV64_MUL(2, BV32_ZEXT64(local_id_x$1)), BV64_MUL(4, BV32_ZEXT64(local_id_y$1))), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n))), BV64_MUL(63, $c0)), 1)), 64), 1), 64)) else BV64_SDIV(BV64_ADD(BV64_ADD(BV64_ADD(BV64_ADD(BV64_MUL(2, BV32_ZEXT64(local_id_x$1)), BV64_MUL(4, BV32_ZEXT64(local_id_y$1))), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n))), BV64_MUL(63, $c0)), 1), 64))), BV64_ADD(BV64_ADD(BV64_ADD(BV64_MUL(2, BV32_ZEXT64(local_id_x$1)), BV64_MUL(4, BV32_ZEXT64(local_id_y$1))), BV64_MUL(63, $c0)), 1)) then 1 else 0))), BV1_ZEXT32((if BV64_SGE(BV64_SREM(BV64_ADD(BV64_ADD(BV64_ADD(BV64_MUL(32, BV32_ZEXT64(group_id_x$1)), BV32_ZEXT64(local_id_x$1)), BV32_ZEXT64(BV32_MUL(8191, BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n)))), 1), 8192), BV64_SREM(BV64_ADD(BV64_ADD(BV64_ADD(BV64_ADD(BV64_MUL(2, BV32_ZEXT64(local_id_x$1)), BV64_MUL(4, BV32_ZEXT64(local_id_y$1))), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n))), BV64_MUL(63, $c0)), 1), 64)) then 1 else 0))), BV1_ZEXT32((if BV64_SGE(BV64_ADD(BV32_ZEXT64(local_id_y$1), BV64_MUL(8192, BV64_SDIV(BV64_ADD(BV64_ADD(BV64_ADD(BV64_ADD(BV64_ADD(BV64_ADD(BV64_ADD(BV64_SUB(0, BV64_MUL(2, BV64_SREM(BV64_ADD(BV64_ADD(BV64_ADD(BV64_MUL(32, BV32_ZEXT64(group_id_x$1)), BV32_ZEXT64(local_id_x$1)), BV32_ZEXT64(BV32_MUL(8191, BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n)))), 1), 8192))), BV64_MUL(128, BV32_ZEXT64(group_id_x$1))), BV64_MUL(128, BV32_ZEXT64(group_id_y$1))), BV64_MUL(4, BV32_ZEXT64(local_id_x$1))), BV32_ZEXT64(BV32_MUL(32766, BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n)))), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n))), BV64_MUL(32767, $c0)), 127), 32768))), BV64_ADD(BV64_ADD(BV64_ADD(BV64_ADD(BV64_ADD(BV64_MUL(16, BV32_ZEXT64(group_id_x$1)), BV64_MUL(32, BV32_ZEXT64(group_id_y$1))), BV32_ZEXT64(BV32_MUL(4096, BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n)))), BV64_MUL(8176, $c0)), BV64_MUL(4096, BV64_SDIV(BV64_ADD(BV64_ADD(BV64_ADD(BV64_MUL(32, BV32_ZEXT64(group_id_x$1)), BV32_ZEXT64(local_id_x$1)), BV32_ZEXT64(BV32_MUL(8191, BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n)))), 1), 8192))), BV64_MUL(16, BV64_SDIV(BV64_ADD(BV64_ADD(BV64_ADD(BV64_ADD(BV64_MUL(2, BV32_ZEXT64(local_id_x$1)), BV64_MUL(4, BV32_ZEXT64(local_id_y$1))), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n))), BV64_MUL(63, $c0)), 1), 64)))) then 1 else 0))), BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV1_ZEXT32((if BV32_ZEXT64(group_id_x$1) == 0 then 1 else 0)), BV1_ZEXT32((if BV64_SGE(BV32_SEXT64($n), BV64_ADD(BV32_ZEXT64(local_id_x$1), 2)) then 1 else 0))), BV1_ZEXT32((if BV64_SGE(BV32_ZEXT64(local_id_x$1), 1) then 1 else 0))), BV1_ZEXT32((if BV64_SGE(BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n)), BV32_ZEXT64(local_id_x$1)) then 1 else 0))), BV1_ZEXT32((if BV32_ULE(BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n), 2) then 1 else 0))), BV1_ZEXT32((if BV32_UGE($n, BV32_ADD(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), 3)) then 1 else 0))), BV1_ZEXT32((if BV64_SGE($c0, BV64_ADD(BV64_ADD(BV64_MUL(2, BV32_ZEXT64(local_id_x$1)), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n))), 1)) then 1 else 0))), BV1_ZEXT32((if BV64_SGE(BV32_ZEXT64(BV32_ADD(BV32_MUL(4, $tsteps), BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n))), BV64_ADD($c0, 1)) then 1 else 0))), BV1_ZEXT32((if BV64_SGE(BV64_MUL(64, (if BV64_SLT(BV64_ADD(BV64_ADD(BV64_ADD(BV64_ADD(BV64_MUL(2, BV32_ZEXT64(local_id_x$1)), BV64_MUL(4, BV32_ZEXT64(local_id_y$1))), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n))), BV64_MUL(63, $c0)), 1), 0) then BV64_SUB(0, BV64_SDIV(BV64_SUB(BV64_ADD(BV64_SUB(0, BV64_ADD(BV64_ADD(BV64_ADD(BV64_ADD(BV64_MUL(2, BV32_ZEXT64(local_id_x$1)), BV64_MUL(4, BV32_ZEXT64(local_id_y$1))), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n))), BV64_MUL(63, $c0)), 1)), 64), 1), 64)) else BV64_SDIV(BV64_ADD(BV64_ADD(BV64_ADD(BV64_ADD(BV64_MUL(2, BV32_ZEXT64(local_id_x$1)), BV64_MUL(4, BV32_ZEXT64(local_id_y$1))), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n))), BV64_MUL(63, $c0)), 1), 64))), BV64_ADD(BV64_ADD(BV64_ADD(BV64_MUL(2, BV32_ZEXT64(local_id_x$1)), BV64_MUL(4, BV32_ZEXT64(local_id_y$1))), BV64_MUL(63, $c0)), 1)) then 1 else 0))), BV1_ZEXT32((if BV64_SGE(BV64_ADD(BV32_ZEXT64(local_id_x$1), 1), BV64_ADD(BV64_SREM(BV64_ADD(BV64_ADD(BV64_ADD(BV64_ADD(BV64_MUL(2, BV32_ZEXT64(local_id_x$1)), BV64_MUL(4, BV32_ZEXT64(local_id_y$1))), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n))), BV64_MUL(63, $c0)), 1), 64), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n)))) then 1 else 0))), BV1_ZEXT32((if BV64_SGE(BV64_ADD(BV32_ZEXT64(local_id_y$1), BV64_MUL(8192, BV64_SDIV(BV64_ADD(BV64_ADD(BV64_ADD(BV64_ADD(BV64_MUL(128, BV32_ZEXT64(group_id_y$1)), BV64_MUL(2, BV32_ZEXT64(local_id_x$1))), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n))), BV64_MUL(32767, $c0)), 125), 32768))), BV64_ADD(BV64_ADD(BV64_MUL(32, BV32_ZEXT64(group_id_y$1)), BV64_MUL(8176, $c0)), BV64_MUL(16, BV64_SDIV(BV64_ADD(BV64_ADD(BV64_ADD(BV64_ADD(BV64_MUL(2, BV32_ZEXT64(local_id_x$1)), BV64_MUL(4, BV32_ZEXT64(local_id_y$1))), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n))), BV64_MUL(63, $c0)), 1), 64)))) then 1 else 0)))), BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV1_ZEXT32((if BV64_SGE(BV64_ADD(BV64_MUL(32, BV32_ZEXT64(group_id_x$1)), BV32_ZEXT64(local_id_x$1)), 0) then 1 else 0)), BV1_ZEXT32((if BV64_SLE(BV64_ADD(BV64_MUL(32, BV32_ZEXT64(group_id_x$1)), BV32_ZEXT64(local_id_x$1)), 8191) then 1 else 0))), BV1_ZEXT32((if BV32_UGE($n, BV32_ADD(BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n), 3)) then 1 else 0))), BV1_ZEXT32((if BV32_UGE(BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n), 0) then 1 else 0))), BV1_ZEXT32((if BV64_SGE(BV64_ADD(BV64_ADD(BV32_ZEXT64(local_id_x$1), BV64_MUL(2, BV32_ZEXT64(local_id_y$1))), BV64_MUL(32, (if BV64_SLT(BV64_ADD(BV64_ADD(BV64_SUB(BV64_SUB(BV64_MUL(-2, BV32_ZEXT64(local_id_x$1)), BV64_MUL(4, BV32_ZEXT64(local_id_y$1))), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n))), $c0), 62), 0) then BV64_SUB(0, BV64_SDIV(BV64_SUB(BV64_ADD(BV64_SUB(0, BV64_ADD(BV64_ADD(BV64_SUB(BV64_SUB(BV64_MUL(-2, BV32_ZEXT64(local_id_x$1)), BV64_MUL(4, BV32_ZEXT64(local_id_y$1))), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n))), $c0), 62)), 64), 1), 64)) else BV64_SDIV(BV64_ADD(BV64_ADD(BV64_SUB(BV64_SUB(BV64_MUL(-2, BV32_ZEXT64(local_id_x$1)), BV64_MUL(4, BV32_ZEXT64(local_id_y$1))), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n))), $c0), 62), 64)))), BV32_ZEXT64(BV32_ADD(BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n), 1))) then 1 else 0))), BV1_ZEXT32((if BV64_SGE(BV32_ZEXT64(BV32_ADD(BV32_MUL(2, $tsteps), BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n))), BV64_ADD(BV64_ADD(BV64_ADD(BV32_ZEXT64(local_id_x$1), BV64_MUL(2, BV32_ZEXT64(local_id_y$1))), BV64_MUL(32, (if BV64_SLT(BV64_ADD(BV64_ADD(BV64_SUB(BV64_SUB(BV64_MUL(-2, BV32_ZEXT64(local_id_x$1)), BV64_MUL(4, BV32_ZEXT64(local_id_y$1))), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n))), $c0), 62), 0) then BV64_SUB(0, BV64_SDIV(BV64_SUB(BV64_ADD(BV64_SUB(0, BV64_ADD(BV64_ADD(BV64_SUB(BV64_SUB(BV64_MUL(-2, BV32_ZEXT64(local_id_x$1)), BV64_MUL(4, BV32_ZEXT64(local_id_y$1))), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n))), $c0), 62)), 64), 1), 64)) else BV64_SDIV(BV64_ADD(BV64_ADD(BV64_SUB(BV64_SUB(BV64_MUL(-2, BV32_ZEXT64(local_id_x$1)), BV64_MUL(4, BV32_ZEXT64(local_id_y$1))), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n))), $c0), 62), 64)))), 1)) then 1 else 0))), BV1_ZEXT32((if BV64_SGE(BV64_ADD(BV64_ADD(BV64_ADD(BV32_SEXT64($n), BV64_MUL(2, BV32_ZEXT64(local_id_x$1))), BV64_MUL(4, BV32_ZEXT64(local_id_y$1))), BV64_MUL(64, (if BV64_SLT(BV64_ADD(BV64_ADD(BV64_SUB(BV64_SUB(BV64_MUL(-2, BV32_ZEXT64(local_id_x$1)), BV64_MUL(4, BV32_ZEXT64(local_id_y$1))), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n))), $c0), 62), 0) then BV64_SUB(0, BV64_SDIV(BV64_SUB(BV64_ADD(BV64_SUB(0, BV64_ADD(BV64_ADD(BV64_SUB(BV64_SUB(BV64_MUL(-2, BV32_ZEXT64(local_id_x$1)), BV64_MUL(4, BV32_ZEXT64(local_id_y$1))), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n))), $c0), 62)), 64), 1), 64)) else BV64_SDIV(BV64_ADD(BV64_ADD(BV64_SUB(BV64_SUB(BV64_MUL(-2, BV32_ZEXT64(local_id_x$1)), BV64_MUL(4, BV32_ZEXT64(local_id_y$1))), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n))), $c0), 62), 64)))), BV64_ADD($c0, 2)) then 1 else 0))), BV1_ZEXT32((if BV64_SGE($c0, BV64_ADD(BV64_ADD(BV64_ADD(BV64_MUL(2, BV32_ZEXT64(local_id_x$1)), BV64_MUL(4, BV32_ZEXT64(local_id_y$1))), BV64_MUL(64, (if BV64_SLT(BV64_ADD(BV64_ADD(BV64_SUB(BV64_SUB(BV64_MUL(-2, BV32_ZEXT64(local_id_x$1)), BV64_MUL(4, BV32_ZEXT64(local_id_y$1))), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n))), $c0), 62), 0) then BV64_SUB(0, BV64_SDIV(BV64_SUB(BV64_ADD(BV64_SUB(0, BV64_ADD(BV64_ADD(BV64_SUB(BV64_SUB(BV64_MUL(-2, BV32_ZEXT64(local_id_x$1)), BV64_MUL(4, BV32_ZEXT64(local_id_y$1))), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n))), $c0), 62)), 64), 1), 64)) else BV64_SDIV(BV64_ADD(BV64_ADD(BV64_SUB(BV64_SUB(BV64_MUL(-2, BV32_ZEXT64(local_id_x$1)), BV64_MUL(4, BV32_ZEXT64(local_id_y$1))), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n))), $c0), 62), 64)))), 1)) then 1 else 0))), BV1_ZEXT32((if BV64_SGE($c0, BV64_ADD(BV64_ADD(BV64_ADD(BV64_MUL(2, BV32_ZEXT64(local_id_x$1)), BV64_MUL(4, BV32_ZEXT64(local_id_y$1))), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n))), BV64_MUL(64, (if BV64_SLT(BV64_ADD(BV64_ADD(BV64_SUB(BV64_SUB(BV64_MUL(-2, BV32_ZEXT64(local_id_x$1)), BV64_MUL(4, BV32_ZEXT64(local_id_y$1))), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n))), $c0), 62), 0) then BV64_SUB(0, BV64_SDIV(BV64_SUB(BV64_ADD(BV64_SUB(0, BV64_ADD(BV64_ADD(BV64_SUB(BV64_SUB(BV64_MUL(-2, BV32_ZEXT64(local_id_x$1)), BV64_MUL(4, BV32_ZEXT64(local_id_y$1))), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n))), $c0), 62)), 64), 1), 64)) else BV64_SDIV(BV64_ADD(BV64_ADD(BV64_SUB(BV64_SUB(BV64_MUL(-2, BV32_ZEXT64(local_id_x$1)), BV64_MUL(4, BV32_ZEXT64(local_id_y$1))), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n))), $c0), 62), 64))))) then 1 else 0))), BV1_ZEXT32((if BV64_SGE(BV64_SREM(BV64_ADD(BV64_ADD(BV64_SUB(BV64_ADD(BV64_ADD(BV64_MUL(-64, BV32_ZEXT64(group_id_y$1)), BV32_ZEXT64(local_id_x$1)), BV64_MUL(2, BV32_ZEXT64(local_id_y$1))), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n))), BV64_MUL(32, BV64_SDIV(BV64_ADD(BV64_ADD(BV64_SUB(BV64_SUB(BV64_MUL(-2, BV32_ZEXT64(local_id_x$1)), BV64_MUL(4, BV32_ZEXT64(local_id_y$1))), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n))), $c0), 126), 64))), 16288), 16384), 16321) then 1 else 0))), BV1_ZEXT32((if BV64_SREM(BV64_ADD(BV64_SUB(BV64_ADD(BV64_MUL(32, BV32_ZEXT64(group_id_x$1)), BV32_ZEXT64(local_id_x$1)), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n))), 8191), 8192) == 0 then 1 else 0)))), BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV1_ZEXT32((if BV64_SLE(BV32_ZEXT64(local_id_x$1), 31) then 1 else 0)), BV1_ZEXT32((if BV64_SGE(BV32_ZEXT64(local_id_x$1), -8160) then 1 else 0))), BV1_ZEXT32((if BV32_UGE(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), 2) then 1 else 0))), BV1_ZEXT32((if BV32_UGE($n, BV32_ADD(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), 1)) then 1 else 0))), BV1_ZEXT32((if BV64_SGE(BV32_SEXT64($n), BV64_ADD(BV64_ADD(BV32_ZEXT64(local_id_x$1), BV64_MUL(32, (if BV64_SLT(BV64_ADD(BV64_ADD(BV64_SUB(0, BV32_ZEXT64(local_id_x$1)), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n))), 30), 0) then BV64_SUB(0, BV64_SDIV(BV64_SUB(BV64_ADD(BV64_SUB(0, BV64_ADD(BV64_ADD(BV64_SUB(0, BV32_ZEXT64(local_id_x$1)), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n))), 30)), 32), 1), 32)) else BV64_SDIV(BV64_ADD(BV64_ADD(BV64_SUB(0, BV32_ZEXT64(local_id_x$1)), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n))), 30), 32)))), 2)) then 1 else 0))), BV1_ZEXT32((if BV64_SGE(BV64_ADD(BV32_ZEXT64(local_id_x$1), BV64_MUL(32, (if BV64_SLT(BV64_ADD(BV64_ADD(BV64_SUB(0, BV32_ZEXT64(local_id_x$1)), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n))), 30), 0) then BV64_SUB(0, BV64_SDIV(BV64_SUB(BV64_ADD(BV64_SUB(0, BV64_ADD(BV64_ADD(BV64_SUB(0, BV32_ZEXT64(local_id_x$1)), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n))), 30)), 32), 1), 32)) else BV64_SDIV(BV64_ADD(BV64_ADD(BV64_SUB(0, BV32_ZEXT64(local_id_x$1)), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n))), 30), 32)))), 1) then 1 else 0))), BV1_ZEXT32((if BV64_SGE(BV64_SREM(BV64_ADD(BV64_ADD(BV64_SUB(0, BV32_ZEXT64(local_id_x$1)), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n))), 30), 32), 30) then 1 else 0))), BV1_ZEXT32((if BV64_SGE(BV32_ZEXT64(BV32_ADD(BV32_ADD(BV32_ADD(BV32_MUL(4, $tsteps), BV32_MUL(2, BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n))), BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n)), 55)), BV64_ADD(BV64_MUL(2, BV64_SREM(BV64_ADD(BV64_ADD(BV64_SUB(0, BV32_ZEXT64(local_id_x$1)), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n))), 30), 32)), $c0)) then 1 else 0))), BV1_ZEXT32((if BV64_SGE(BV64_ADD(BV64_MUL(2, BV64_SREM(BV64_ADD(BV64_ADD(BV64_SUB(0, BV32_ZEXT64(local_id_x$1)), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n))), 30), 32)), $c0), BV32_ZEXT64(BV32_ADD(BV32_ADD(BV32_MUL(2, BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n)), BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n)), 59))) then 1 else 0))), BV1_ZEXT32((if BV64_SGE(BV64_ADD(BV64_ADD(BV64_ADD(BV64_MUL(2, BV64_SREM(BV64_ADD(BV64_ADD(BV64_SUB(0, BV32_ZEXT64(local_id_x$1)), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n))), 30), 32)), BV64_SREM(BV64_ADD(BV64_ADD(BV64_SUB(BV64_SUB(BV64_SUB(BV64_MUL(2, BV64_SREM(BV64_ADD(BV64_ADD(BV64_SUB(0, BV32_ZEXT64(local_id_x$1)), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n))), 30), 32)), BV64_MUL(128, BV32_ZEXT64(group_id_y$1))), BV32_ZEXT64(BV32_MUL(2, BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n)))), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n))), $c0), 32584), 32768)), BV64_MUL(2, BV32_ZEXT64(local_id_x$1))), BV64_MUL(64, BV64_SDIV(BV64_ADD(BV64_ADD(BV64_SUB(0, BV32_ZEXT64(local_id_x$1)), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n))), 30), 32))), BV32_ZEXT64(BV32_ADD(BV32_MUL(2, BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n)), 32703))) then 1 else 0))), BV1_ZEXT32((if BV64_ADD(BV32_ZEXT64(group_id_x$1), BV64_MUL(256, BV64_SDIV(BV64_ADD(BV64_ADD(BV64_SUB(BV64_SUB(0, BV64_SREM(BV64_ADD(BV64_ADD(BV64_SUB(0, BV32_ZEXT64(local_id_x$1)), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n))), 30), 32)), BV64_MUL(32, BV32_ZEXT64(group_id_x$1))), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n))), 8190), 8192))) == BV64_SDIV(BV64_ADD(BV64_ADD(BV64_SUB(0, BV32_ZEXT64(local_id_x$1)), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n))), 30), 32) then 1 else 0))), BV1_ZEXT32((if BV64_SREM(BV64_ADD(BV64_SUB(BV64_ADD(BV64_ADD(BV64_MUL(2, BV32_ZEXT64(local_id_x$1)), BV64_MUL(4, BV32_ZEXT64(local_id_y$1))), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n))), $c0), 63), 64) == 0 then 1 else 0)))), BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV1_ZEXT32((if BV32_UGE(BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n), 3) then 1 else 0)), BV1_ZEXT32((if BV32_UGE($n, BV32_ADD(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), 3)) then 1 else 0))), BV1_ZEXT32((if BV64_SGE(BV32_ZEXT64(BV32_ADD(BV32_ADD(BV32_MUL(2, $n), BV32_MUL(4, $tsteps)), BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n))), BV64_ADD($c0, 7)) then 1 else 0))), BV1_ZEXT32((if BV64_SGE(BV32_ZEXT64(BV32_ADD(BV32_ADD(BV32_MUL(4, $tsteps), BV32_MUL(2, BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n))), BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n))), BV64_ADD($c0, 3)) then 1 else 0))), BV1_ZEXT32((if BV64_SGE($c0, BV32_ZEXT64(BV32_ADD(BV32_ADD(BV32_MUL(2, BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n)), BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n)), 1))) then 1 else 0))), BV1_ZEXT32((if BV64_SGE(BV64_ADD($c0, 3), BV32_ZEXT64(BV32_ADD(BV32_MUL(4, $tsteps), BV32_MUL(2, BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n))))) then 1 else 0))), BV1_ZEXT32((if BV64_SGE(BV64_ADD($c0, 4), BV32_ZEXT64(BV32_ADD(BV32_ADD(BV32_MUL(4, $tsteps), BV32_MUL(2, BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n))), BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n)))) then 1 else 0))), BV1_ZEXT32((if BV64_SGE(BV64_ADD(BV32_ZEXT64(BV32_ADD($tsteps, BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n))), BV64_MUL(8192, BV64_SDIV(BV64_ADD(BV64_ADD(BV64_ADD(BV64_SUB(BV64_ADD(BV64_ADD(BV64_ADD(BV64_MUL(128, BV32_ZEXT64(group_id_x$1)), BV64_MUL(128, BV32_ZEXT64(group_id_y$1))), BV64_MUL(4, BV32_ZEXT64(local_id_x$1))), BV64_MUL(32768, BV32_ZEXT64(local_id_y$1))), BV32_ZEXT64(BV32_MUL(2, BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n)))), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n))), BV64_MUL(32767, $c0)), 32893), 32768))), BV64_ADD(BV64_ADD(BV64_ADD(BV64_ADD(BV64_ADD(BV64_MUL(32, BV32_ZEXT64(group_id_x$1)), BV64_MUL(32, BV32_ZEXT64(group_id_y$1))), BV32_ZEXT64(local_id_x$1)), BV64_MUL(8192, BV32_ZEXT64(local_id_y$1))), BV64_MUL(8192, $c0)), 8193)) then 1 else 0))), BV1_ZEXT32((if BV64_SREM(BV64_ADD(BV64_SUB(BV32_SEXT64($tsteps), BV32_ZEXT64(local_id_y$1)), 15), 16) == 0 then 1 else 0))), BV1_ZEXT32((if BV64_SREM(BV64_SUB(BV64_ADD(BV64_MUL(32, BV32_ZEXT64(group_id_x$1)), BV32_ZEXT64(local_id_x$1)), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n))), 8192) == 0 then 1 else 0)))), BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV1_ZEXT32((if BV64_SGE(BV64_ADD(BV64_MUL(32, BV32_ZEXT64(group_id_x$1)), BV32_ZEXT64(local_id_x$1)), 0) then 1 else 0)), BV1_ZEXT32((if BV64_SLE(BV64_ADD(BV64_MUL(32, BV32_ZEXT64(group_id_x$1)), BV32_ZEXT64(local_id_x$1)), 8191) then 1 else 0))), BV1_ZEXT32((if BV32_UGE(BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n), 2) then 1 else 0))), BV1_ZEXT32((if BV32_UGE($n, BV32_ADD(BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n), 1)) then 1 else 0))), BV1_ZEXT32((if BV32_UGE($n, BV32_ADD(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), 2)) then 1 else 0))), BV1_ZEXT32((if BV32_UGE(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), 1) then 1 else 0))), BV1_ZEXT32((if BV64_SGE(BV32_ZEXT64(BV32_ADD(BV32_ADD(BV32_MUL(4, $tsteps), BV32_MUL(2, BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n))), BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n))), BV64_ADD($c0, 6)) then 1 else 0))), BV1_ZEXT32((if BV64_SGE(BV64_ADD($c0, 2), BV32_ZEXT64(BV32_ADD(BV32_MUL(2, BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n)), BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n)))) then 1 else 0))), BV1_ZEXT32((if BV64_SGE(BV64_SREM(BV64_ADD(BV64_ADD(BV32_ZEXT64(BV32_SUB(BV32_MUL(-2, BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n)), BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n))), $c0), 2), 32768), BV64_MUL(128, BV32_ZEXT64(group_id_y$1))) then 1 else 0))), BV1_ZEXT32((if BV64_SGE(BV64_ADD(BV64_MUL(128, BV32_ZEXT64(group_id_y$1)), 124), BV64_SREM(BV64_ADD(BV64_ADD(BV32_ZEXT64(BV32_SUB(BV32_MUL(-2, BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n)), BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n))), $c0), 2), 32768)) then 1 else 0))), BV1_ZEXT32((if BV64_SREM(BV64_ADD(BV64_SUB(BV64_ADD(BV64_MUL(32, BV32_ZEXT64(group_id_x$1)), BV32_ZEXT64(local_id_x$1)), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n))), 1), 8192) == 0 then 1 else 0))), BV1_ZEXT32((if BV64_SREM(BV64_SUB(BV64_ADD(BV64_ADD(BV64_MUL(2, BV32_ZEXT64(local_id_x$1)), BV64_MUL(4, BV32_ZEXT64(local_id_y$1))), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n))), $c0), 64) == 0 then 1 else 0)))), BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV1_ZEXT32((if BV64_SGE(BV64_ADD(BV64_MUL(32, BV32_ZEXT64(group_id_x$1)), BV32_ZEXT64(local_id_x$1)), 0) then 1 else 0)), BV1_ZEXT32((if BV64_SLE(BV64_ADD(BV64_MUL(32, BV32_ZEXT64(group_id_x$1)), BV32_ZEXT64(local_id_x$1)), 8191) then 1 else 0))), BV1_ZEXT32((if BV32_UGE($n, BV32_ADD(BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n), 3)) then 1 else 0))), BV1_ZEXT32((if BV32_UGE(BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n), 0) then 1 else 0))), BV1_ZEXT32((if BV32_UGE(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), 2) then 1 else 0))), BV1_ZEXT32((if BV32_UGE($n, BV32_ADD(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), 1)) then 1 else 0))), BV1_ZEXT32((if BV64_SGE(BV32_ZEXT64(BV32_ADD(BV32_ADD(BV32_MUL(4, $tsteps), BV32_MUL(2, BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n))), BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n))), BV64_ADD($c0, 3)) then 1 else 0))), BV1_ZEXT32((if BV64_SGE($c0, BV32_ZEXT64(BV32_ADD(BV32_ADD(BV32_MUL(2, BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n)), BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n)), 1))) then 1 else 0))), BV1_ZEXT32((if BV64_SGE(BV64_SREM(BV64_SUB(BV64_ADD(BV32_ZEXT64(BV32_SUB(BV32_MUL(-2, BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n)), BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n))), $c0), 1), 32768), BV64_MUL(128, BV32_ZEXT64(group_id_y$1))) then 1 else 0))), BV1_ZEXT32((if BV64_SGE(BV64_ADD(BV64_MUL(128, BV32_ZEXT64(group_id_y$1)), 124), BV64_SREM(BV64_SUB(BV64_ADD(BV32_ZEXT64(BV32_SUB(BV32_MUL(-2, BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n)), BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n))), $c0), 1), 32768)) then 1 else 0))), BV1_ZEXT32((if BV64_SREM(BV64_ADD(BV64_SUB(BV64_ADD(BV64_MUL(32, BV32_ZEXT64(group_id_x$1)), BV32_ZEXT64(local_id_x$1)), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n))), 8191), 8192) == 0 then 1 else 0))), BV1_ZEXT32((if BV64_SREM(BV64_ADD(BV64_SUB(BV64_ADD(BV64_ADD(BV64_MUL(2, BV32_ZEXT64(local_id_x$1)), BV64_MUL(4, BV32_ZEXT64(local_id_y$1))), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n))), $c0), 63), 64) == 0 then 1 else 0)))), BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV1_ZEXT32((if BV32_UGE(BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n), 3) then 1 else 0)), BV1_ZEXT32((if BV32_UGE($n, BV32_ADD(BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n), 2)) then 1 else 0))), BV1_ZEXT32((if BV32_ADD(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), 2) == $n then 1 else 0))), BV1_ZEXT32((if BV64_SGE(BV32_ZEXT64(BV32_ADD(BV32_ADD($n, BV32_MUL(4, $tsteps)), BV32_MUL(2, BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n)))), BV64_ADD($c0, 10)) then 1 else 0))), BV1_ZEXT32((if BV64_SGE(BV64_ADD($c0, 2), BV32_ZEXT64(BV32_ADD($n, BV32_MUL(2, BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n))))) then 1 else 0))), BV1_ZEXT32((if BV64_SLE(BV64_SREM(BV64_SUB(BV64_ADD(BV64_ADD(BV64_ADD(BV64_ADD(BV64_ADD(BV64_ADD(BV32_SEXT64(BV32_MUL(32769, $n)), BV64_MUL(128, BV32_ZEXT64(group_id_x$1))), BV64_MUL(128, BV32_ZEXT64(group_id_y$1))), BV64_MUL(32772, BV32_ZEXT64(local_id_x$1))), BV64_MUL(32768, BV32_ZEXT64(local_id_y$1))), BV32_ZEXT64(BV32_MUL(32766, BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n)))), BV64_MUL(32767, $c0)), 65413), 32768), 125) then 1 else 0))), BV1_ZEXT32((if BV64_SREM(BV64_SUB(BV64_ADD(BV64_MUL(32, BV32_ZEXT64(group_id_x$1)), BV32_ZEXT64(local_id_x$1)), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n))), 8192) == 0 then 1 else 0))), BV1_ZEXT32((if BV64_SREM(BV64_SUB(BV64_SUB(BV64_ADD(BV64_ADD(BV32_SEXT64($n), BV64_MUL(2, BV32_ZEXT64(local_id_x$1))), BV64_MUL(4, BV32_ZEXT64(local_id_y$1))), $c0), 2), 64) == 0 then 1 else 0)))), BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV1_ZEXT32((if BV32_SGE($tsteps, 2) then 1 else 0)), BV1_ZEXT32((if BV32_ZEXT64(group_id_y$1) == 0 then 1 else 0))), BV1_ZEXT32((if BV32_ZEXT64(local_id_y$1) == 0 then 1 else 0))), BV1_ZEXT32((if BV32_UGE(BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n), 3) then 1 else 0))), BV1_ZEXT32((if BV32_UGE($n, BV32_ADD(BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n), 2)) then 1 else 0))), BV1_ZEXT32((if BV32_UGE($n, BV32_ADD(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), 3)) then 1 else 0))), BV1_ZEXT32((if BV32_UGE(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), 1) then 1 else 0))), BV1_ZEXT32((if $c0 == BV32_ZEXT64(BV32_ADD(BV32_MUL(2, BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n)), BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n))) then 1 else 0))), BV1_ZEXT32((if BV64_SREM(BV64_SUB(BV64_ADD(BV64_MUL(32, BV32_ZEXT64(group_id_x$1)), BV32_ZEXT64(local_id_x$1)), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n))), 8192) == 0 then 1 else 0)))), BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV1_ZEXT32((if BV32_ZEXT64(group_id_x$1) == 0 then 1 else 0)), BV1_ZEXT32((if BV64_SGE(BV32_SEXT64($n), BV64_ADD(BV32_ZEXT64(local_id_x$1), 2)) then 1 else 0))), BV1_ZEXT32((if BV64_SGE(BV32_ZEXT64(local_id_x$1), 1) then 1 else 0))), BV1_ZEXT32((if BV64_SLE(BV32_ZEXT64(local_id_x$1), 2) then 1 else 0))), BV1_ZEXT32((if BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n)) == BV32_ZEXT64(local_id_x$1) then 1 else 0))), BV1_ZEXT32((if BV32_ADD(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), 2) == $n then 1 else 0))), BV1_ZEXT32((if BV64_SGE(BV32_SEXT64(BV32_ADD($n, BV32_MUL(4, $tsteps))), BV64_ADD($c0, 4)) then 1 else 0))), BV1_ZEXT32((if BV64_SGE(BV64_ADD($c0, 2), BV64_ADD(BV32_SEXT64($n), BV64_MUL(2, BV32_ZEXT64(local_id_x$1)))) then 1 else 0))), BV1_ZEXT32((if BV64_SLE(BV64_SREM(BV64_ADD(BV64_ADD(BV64_ADD(BV64_ADD(BV32_SEXT64($n), BV64_MUL(128, BV32_ZEXT64(group_id_y$1))), BV64_MUL(2, BV32_ZEXT64(local_id_x$1))), BV64_MUL(32767, $c0)), 123), 32768), 125) then 1 else 0))), BV1_ZEXT32((if BV64_SREM(BV64_SUB(BV64_SUB(BV64_ADD(BV64_ADD(BV32_SEXT64($n), BV64_MUL(2, BV32_ZEXT64(local_id_x$1))), BV64_MUL(4, BV32_ZEXT64(local_id_y$1))), $c0), 2), 64) == 0 then 1 else 0)))), BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV1_ZEXT32((if BV32_ZEXT64(group_id_x$1) == 0 then 1 else 0)), BV1_ZEXT32((if BV32_ZEXT64(local_id_x$1) == 2 then 1 else 0))), BV1_ZEXT32((if BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n) == 2 then 1 else 0))), BV1_ZEXT32((if BV32_UGE($n, BV32_ADD(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), 3)) then 1 else 0))), BV1_ZEXT32((if BV64_SGE($c0, BV32_ZEXT64(BV32_ADD(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), 5))) then 1 else 0))), BV1_ZEXT32((if BV64_SGE($c0, BV32_SEXT64(BV32_ADD(BV32_MUL(4, $tsteps), 1))) then 1 else 0))), BV1_ZEXT32((if BV64_SGE($c0, BV32_ZEXT64(BV32_ADD(BV32_MUL(4, $tsteps), BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n)))) then 1 else 0))), BV1_ZEXT32((if BV64_SGE(BV32_ZEXT64(BV32_ADD(BV32_ADD(BV32_MUL(4, $tsteps), BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n)), 1)), $c0) then 1 else 0))), BV1_ZEXT32((if BV64_SGE(BV64_ADD(BV32_SEXT64($tsteps), BV64_MUL(8192, BV64_SDIV(BV64_ADD(BV64_ADD(BV64_ADD(BV64_MUL(128, BV32_ZEXT64(group_id_y$1)), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n))), BV64_MUL(32767, $c0)), 129), 32768))), BV64_ADD(BV64_ADD(BV64_MUL(32, BV32_ZEXT64(group_id_y$1)), BV64_MUL(8192, $c0)), 1)) then 1 else 0))), BV1_ZEXT32((if BV64_SREM(BV64_ADD(BV64_SUB(BV32_SEXT64($tsteps), BV32_ZEXT64(local_id_y$1)), 15), 16) == 0 then 1 else 0)))), BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV1_ZEXT32((if BV32_UGE(BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n), 3) then 1 else 0)), BV1_ZEXT32((if BV32_ADD(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), 2) == $n then 1 else 0))), BV1_ZEXT32((if BV64_ADD($c0, 6) == BV32_ZEXT64(BV32_ADD(BV32_ADD($n, BV32_MUL(4, $tsteps)), BV32_MUL(2, BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n)))) then 1 else 0))), BV1_ZEXT32((if BV64_SLE(BV64_SREM(BV64_ADD(BV64_ADD(BV64_ADD(BV64_ADD(BV64_ADD(BV64_ADD(BV32_SEXT64(BV32_MUL(8191, $tsteps)), BV64_MUL(32, BV32_ZEXT64(group_id_x$1))), BV64_MUL(32, BV32_ZEXT64(group_id_y$1))), BV64_MUL(8193, BV32_ZEXT64(local_id_x$1))), BV64_MUL(8192, BV32_ZEXT64(local_id_y$1))), BV32_ZEXT64(BV32_MUL(8191, BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n)))), 8224), 8192), 31) then 1 else 0))), BV1_ZEXT32((if BV64_SREM(BV64_SUB(BV64_ADD(BV64_MUL(32, BV32_ZEXT64(group_id_x$1)), BV32_ZEXT64(local_id_x$1)), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n))), 8192) == 0 then 1 else 0))), BV1_ZEXT32((if BV64_SREM(BV64_ADD(BV64_ADD(BV32_SEXT64(BV32_SUB(0, $tsteps)), BV32_ZEXT64(local_id_y$1)), 1), 16) == 0 then 1 else 0)))), BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV1_ZEXT32((if $tsteps == 1 then 1 else 0)), BV1_ZEXT32((if BV32_ZEXT64(group_id_y$1) == 0 then 1 else 0))), BV1_ZEXT32((if BV32_ZEXT64(local_id_y$1) == 0 then 1 else 0))), BV1_ZEXT32((if BV32_UGE(BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n), 3) then 1 else 0))), BV1_ZEXT32((if BV32_UGE($n, BV32_ADD(BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n), 2)) then 1 else 0))), BV1_ZEXT32((if BV32_UGE($n, BV32_ADD(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), 3)) then 1 else 0))), BV1_ZEXT32((if BV32_UGE(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), 1) then 1 else 0))), BV1_ZEXT32((if $c0 == BV32_ZEXT64(BV32_ADD(BV32_MUL(2, BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n)), BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n))) then 1 else 0))), BV1_ZEXT32((if BV64_SREM(BV64_SUB(BV64_ADD(BV64_MUL(32, BV32_ZEXT64(group_id_x$1)), BV32_ZEXT64(local_id_x$1)), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n))), 8192) == 0 then 1 else 0)))), BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV1_ZEXT32((if BV32_SGE($tsteps, 2) then 1 else 0)), BV1_ZEXT32((if BV32_ZEXT64(group_id_x$1) == 0 then 1 else 0))), BV1_ZEXT32((if BV32_ZEXT64(group_id_y$1) == 0 then 1 else 0))), BV1_ZEXT32((if BV32_ZEXT64(local_id_x$1) == 2 then 1 else 0))), BV1_ZEXT32((if BV32_ZEXT64(local_id_y$1) == 0 then 1 else 0))), BV1_ZEXT32((if BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n) == 2 then 1 else 0))), BV1_ZEXT32((if BV32_UGE($n, BV32_ADD(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), 3)) then 1 else 0))), BV1_ZEXT32((if BV32_UGE(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), 1) then 1 else 0))), BV1_ZEXT32((if $c0 == BV32_ZEXT64(BV32_ADD(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), 4)) then 1 else 0)))), BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV1_ZEXT32((if BV32_ZEXT64(group_id_x$1) == 0 then 1 else 0)), BV1_ZEXT32((if BV32_ZEXT64(group_id_y$1) == 0 then 1 else 0))), BV1_ZEXT32((if BV32_ZEXT64(local_id_x$1) == 1 then 1 else 0))), BV1_ZEXT32((if BV32_ZEXT64(local_id_y$1) == 0 then 1 else 0))), BV1_ZEXT32((if BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n) == 1 then 1 else 0))), BV1_ZEXT32((if BV32_UGE($n, BV32_ADD(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), 3)) then 1 else 0))), BV1_ZEXT32((if $c0 == BV32_ZEXT64(BV32_ADD(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), 2)) then 1 else 0)))), BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV1_ZEXT32((if BV32_ZEXT64(group_id_x$1) == 0 then 1 else 0)), BV1_ZEXT32((if BV32_ZEXT64(local_id_x$1) == 2 then 1 else 0))), BV1_ZEXT32((if BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n) == 2 then 1 else 0))), BV1_ZEXT32((if BV32_ADD(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), 2) == $n then 1 else 0))), BV1_ZEXT32((if BV64_ADD($c0, 2) == BV32_SEXT64(BV32_ADD($n, BV32_MUL(4, $tsteps))) then 1 else 0))), BV1_ZEXT32((if BV64_SGE(BV64_SREM(BV64_ADD(BV64_SUB(BV32_SEXT64($tsteps), BV64_MUL(32, BV32_ZEXT64(group_id_y$1))), 8159), 8192), 8160) then 1 else 0))), BV1_ZEXT32((if BV64_SREM(BV64_ADD(BV64_ADD(BV32_SEXT64(BV32_SUB(0, $tsteps)), BV32_ZEXT64(local_id_y$1)), 1), 16) == 0 then 1 else 0)))), BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV1_ZEXT32((if $tsteps == 1 then 1 else 0)), BV1_ZEXT32((if BV32_ZEXT64(group_id_x$1) == 0 then 1 else 0))), BV1_ZEXT32((if BV32_ZEXT64(group_id_y$1) == 0 then 1 else 0))), BV1_ZEXT32((if BV32_ZEXT64(local_id_x$1) == 2 then 1 else 0))), BV1_ZEXT32((if BV32_ZEXT64(local_id_y$1) == 0 then 1 else 0))), BV1_ZEXT32((if BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n) == 2 then 1 else 0))), BV1_ZEXT32((if BV32_UGE($n, BV32_ADD(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), 3)) then 1 else 0))), BV1_ZEXT32((if BV32_UGE(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), 1) then 1 else 0))), BV1_ZEXT32((if $c0 == BV32_ZEXT64(BV32_ADD(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), 4)) then 1 else 0)))) != 0 then 1 else 0) != 0;
    assert {:procedure_wide_invariant} {:do_not_predicate} {:sourceloc_num 4} {:thread 1} (if _WRITE_HAS_OCCURRED_$$A ==> BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV1_ZEXT32((if BV64_SGE(BV64_ADD(BV64_MUL(32, BV32_ZEXT64(group_id_x$1)), BV32_ZEXT64(local_id_x$1)), 0) then 1 else 0)), BV1_ZEXT32((if BV64_SLE(BV64_ADD(BV64_MUL(32, BV32_ZEXT64(group_id_x$1)), BV32_ZEXT64(local_id_x$1)), 8191) then 1 else 0))), BV1_ZEXT32((if BV32_UGE($n, BV32_ADD(BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n), 2)) then 1 else 0))), BV1_ZEXT32((if BV32_UGE(BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n), 1) then 1 else 0))), BV1_ZEXT32((if BV32_UGE($n, BV32_ADD(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), 2)) then 1 else 0))), BV1_ZEXT32((if BV32_UGE(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), 1) then 1 else 0))), BV1_ZEXT32((if BV64_SGE(BV32_ZEXT64(BV32_ADD(BV32_ADD(BV32_MUL(4, $tsteps), BV32_MUL(2, BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n))), BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n))), BV64_ADD($c0, 4)) then 1 else 0))), BV1_ZEXT32((if BV64_SGE($c0, BV32_ZEXT64(BV32_ADD(BV32_MUL(2, BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n)), BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n)))) then 1 else 0))), BV1_ZEXT32((if BV64_SGE(BV64_SREM(BV64_ADD(BV32_ZEXT64(BV32_SUB(BV32_MUL(-2, BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n)), BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n))), $c0), 32768), BV64_MUL(128, BV32_ZEXT64(group_id_y$1))) then 1 else 0))), BV1_ZEXT32((if BV64_SGE(BV64_ADD(BV64_MUL(128, BV32_ZEXT64(group_id_y$1)), 124), BV64_SREM(BV64_ADD(BV32_ZEXT64(BV32_SUB(BV32_MUL(-2, BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n)), BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n))), $c0), 32768)) then 1 else 0))), BV1_ZEXT32((if BV64_SREM(BV64_SUB(BV64_ADD(BV64_MUL(32, BV32_ZEXT64(group_id_x$1)), BV32_ZEXT64(local_id_x$1)), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n))), 8192) == 0 then 1 else 0))), BV1_ZEXT32((if BV64_SREM(BV64_SUB(BV64_ADD(BV64_ADD(BV64_MUL(2, BV32_ZEXT64(local_id_x$1)), BV64_MUL(4, BV32_ZEXT64(local_id_y$1))), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n))), $c0), 64) == 0 then 1 else 0))) != 0 then 1 else 0) != 0;
    assert {:block_sourceloc} {:sourceloc_num 70} p46$1 ==> true;
    v27$1 := (if p46$1 then BV64_SLT(31, BV64_SUB(BV64_SUB(BV32_SEXT64($tsteps), $c2.0$1), 1)) else v27$1);
    v27$2 := (if p46$2 then BV64_SLT(31, BV64_SUB(BV64_SUB(BV32_SEXT64($tsteps), $c2.0$2), 1)) else v27$2);
    p47$1 := false;
    p47$2 := false;
    p48$1 := false;
    p48$2 := false;
    p49$1 := false;
    p49$2 := false;
    p50$1 := false;
    p50$2 := false;
    p51$1 := false;
    p51$2 := false;
    p52$1 := false;
    p52$2 := false;
    p53$1 := false;
    p53$2 := false;
    p48$1 := (if p46$1 && v27$1 then v27$1 else p48$1);
    p48$2 := (if p46$2 && v27$2 then v27$2 else p48$2);
    p47$1 := (if p46$1 && !v27$1 then !v27$1 else p47$1);
    p47$2 := (if p46$2 && !v27$2 then !v27$2 else p47$2);
    $19$1 := (if p47$1 then BV64_SUB(BV64_SUB(BV32_SEXT64($tsteps), $c2.0$1), 1) else $19$1);
    $19$2 := (if p47$2 then BV64_SUB(BV64_SUB(BV32_SEXT64($tsteps), $c2.0$2), 1) else $19$2);
    $19$1 := (if p48$1 then 31 else $19$1);
    $19$2 := (if p48$2 then 31 else $19$2);
    v28$1 := (if p46$1 then BV64_SLT($19$1, BV64_ADD(BV64_SUB(BV64_SUB(BV64_SUB(0, v2$1), $c1.0$1), $c2.0$1), BV64_SDIV(BV64_SUB(BV64_ADD(BV64_ADD(BV64_MUL(2, v2$1), $c0), BV64_MUL(2, $c1.0$1)), 1), 4))) else v28$1);
    v28$2 := (if p46$2 then BV64_SLT($19$2, BV64_ADD(BV64_SUB(BV64_SUB(BV64_SUB(0, v2$2), $c1.0$2), $c2.0$2), BV64_SDIV(BV64_SUB(BV64_ADD(BV64_ADD(BV64_MUL(2, v2$2), $c0), BV64_MUL(2, $c1.0$2)), 1), 4))) else v28$2);
    p50$1 := (if p46$1 && v28$1 then v28$1 else p50$1);
    p50$2 := (if p46$2 && v28$2 then v28$2 else p50$2);
    p49$1 := (if p46$1 && !v28$1 then !v28$1 else p49$1);
    p49$2 := (if p46$2 && !v28$2 then !v28$2 else p49$2);
    $21$1 := (if p49$1 then BV64_ADD(BV64_SUB(BV64_SUB(BV64_SUB(0, v2$1), $c1.0$1), $c2.0$1), BV64_SDIV(BV64_SUB(BV64_ADD(BV64_ADD(BV64_MUL(2, v2$1), $c0), BV64_MUL(2, $c1.0$1)), 1), 4)) else $21$1);
    $21$2 := (if p49$2 then BV64_ADD(BV64_SUB(BV64_SUB(BV64_SUB(0, v2$2), $c1.0$2), $c2.0$2), BV64_SDIV(BV64_SUB(BV64_ADD(BV64_ADD(BV64_MUL(2, v2$2), $c0), BV64_MUL(2, $c1.0$2)), 1), 4)) else $21$2);
    v29$1 := (if p50$1 then BV64_SLT(31, BV64_SUB(BV64_SUB(BV32_SEXT64($tsteps), $c2.0$1), 1)) else v29$1);
    v29$2 := (if p50$2 then BV64_SLT(31, BV64_SUB(BV64_SUB(BV32_SEXT64($tsteps), $c2.0$2), 1)) else v29$2);
    p52$1 := (if p50$1 && v29$1 then v29$1 else p52$1);
    p52$2 := (if p50$2 && v29$2 then v29$2 else p52$2);
    p51$1 := (if p50$1 && !v29$1 then !v29$1 else p51$1);
    p51$2 := (if p50$2 && !v29$2 then !v29$2 else p51$2);
    $20$1 := (if p51$1 then BV64_SUB(BV64_SUB(BV32_SEXT64($tsteps), $c2.0$1), 1) else $20$1);
    $20$2 := (if p51$2 then BV64_SUB(BV64_SUB(BV32_SEXT64($tsteps), $c2.0$2), 1) else $20$2);
    $20$1 := (if p52$1 then 31 else $20$1);
    $20$2 := (if p52$2 then 31 else $20$2);
    $21$1 := (if p50$1 then $20$1 else $21$1);
    $21$2 := (if p50$2 then $20$2 else $21$2);
    v30$1 := (if p46$1 then BV64_SLE($c4.0$1, $21$1) else v30$1);
    v30$2 := (if p46$2 then BV64_SLE($c4.0$2, $21$2) else v30$2);
    p53$1 := (if p46$1 && v30$1 then v30$1 else p53$1);
    p53$2 := (if p46$2 && v30$2 then v30$2 else p53$2);
    p46$1 := (if p46$1 && !v30$1 then v30$1 else p46$1);
    p46$2 := (if p46$2 && !v30$2 then v30$2 else p46$2);
    call {:sourceloc} {:sourceloc_num 81} _LOG_READ_$$A(p53$1, BV_EXTRACT(BV64_ADD(BV64_MUL(BV64_SUB(BV64_ADD(v2$1, $c1.0$1), 1), BV32_SEXT64($n)), BV64_SUB(BV64_SUB(BV64_SUB(BV64_SUB(BV64_ADD(BV64_MUL(-2, v2$1), $c0), BV64_MUL(2, $c1.0$1)), BV64_MUL(4, $c2.0$1)), BV64_MUL(4, $c4.0$1)), 1)), 32, 0), $$A[BV_EXTRACT(BV64_ADD(BV64_MUL(BV64_SUB(BV64_ADD(v2$1, $c1.0$1), 1), BV32_SEXT64($n)), BV64_SUB(BV64_SUB(BV64_SUB(BV64_SUB(BV64_ADD(BV64_MUL(-2, v2$1), $c0), BV64_MUL(2, $c1.0$1)), BV64_MUL(4, $c2.0$1)), BV64_MUL(4, $c4.0$1)), 1)), 32, 0)]);
    assume {:do_not_predicate} {:check_id "check_state_0"} {:captureState "check_state_0"} {:sourceloc} {:sourceloc_num 81} true;
    call {:check_id "check_state_0"} {:sourceloc} {:sourceloc_num 81} _CHECK_READ_$$A(p53$2, BV_EXTRACT(BV64_ADD(BV64_MUL(BV64_SUB(BV64_ADD(v2$2, $c1.0$2), 1), BV32_SEXT64($n)), BV64_SUB(BV64_SUB(BV64_SUB(BV64_SUB(BV64_ADD(BV64_MUL(-2, v2$2), $c0), BV64_MUL(2, $c1.0$2)), BV64_MUL(4, $c2.0$2)), BV64_MUL(4, $c4.0$2)), 1)), 32, 0), $$A[BV_EXTRACT(BV64_ADD(BV64_MUL(BV64_SUB(BV64_ADD(v2$2, $c1.0$2), 1), BV32_SEXT64($n)), BV64_SUB(BV64_SUB(BV64_SUB(BV64_SUB(BV64_ADD(BV64_MUL(-2, v2$2), $c0), BV64_MUL(2, $c1.0$2)), BV64_MUL(4, $c2.0$2)), BV64_MUL(4, $c4.0$2)), 1)), 32, 0)]);
    assume {:captureState "call_return_state_0"} {:procedureName "_CHECK_READ_$$A"} true;
    v31$1 := (if p53$1 then $$A[BV_EXTRACT(BV64_ADD(BV64_MUL(BV64_SUB(BV64_ADD(v2$1, $c1.0$1), 1), BV32_SEXT64($n)), BV64_SUB(BV64_SUB(BV64_SUB(BV64_SUB(BV64_ADD(BV64_MUL(-2, v2$1), $c0), BV64_MUL(2, $c1.0$1)), BV64_MUL(4, $c2.0$1)), BV64_MUL(4, $c4.0$1)), 1)), 32, 0)] else v31$1);
    v31$2 := (if p53$2 then $$A[BV_EXTRACT(BV64_ADD(BV64_MUL(BV64_SUB(BV64_ADD(v2$2, $c1.0$2), 1), BV32_SEXT64($n)), BV64_SUB(BV64_SUB(BV64_SUB(BV64_SUB(BV64_ADD(BV64_MUL(-2, v2$2), $c0), BV64_MUL(2, $c1.0$2)), BV64_MUL(4, $c2.0$2)), BV64_MUL(4, $c4.0$2)), 1)), 32, 0)] else v31$2);
    call {:sourceloc} {:sourceloc_num 82} _LOG_READ_$$A(p53$1, BV_EXTRACT(BV64_ADD(BV64_MUL(BV64_SUB(BV64_ADD(v2$1, $c1.0$1), 1), BV32_SEXT64($n)), BV64_SUB(BV64_SUB(BV64_SUB(BV64_ADD(BV64_MUL(-2, v2$1), $c0), BV64_MUL(2, $c1.0$1)), BV64_MUL(4, $c2.0$1)), BV64_MUL(4, $c4.0$1))), 32, 0), $$A[BV_EXTRACT(BV64_ADD(BV64_MUL(BV64_SUB(BV64_ADD(v2$1, $c1.0$1), 1), BV32_SEXT64($n)), BV64_SUB(BV64_SUB(BV64_SUB(BV64_ADD(BV64_MUL(-2, v2$1), $c0), BV64_MUL(2, $c1.0$1)), BV64_MUL(4, $c2.0$1)), BV64_MUL(4, $c4.0$1))), 32, 0)]);
    assume {:do_not_predicate} {:check_id "check_state_1"} {:captureState "check_state_1"} {:sourceloc} {:sourceloc_num 82} true;
    call {:check_id "check_state_1"} {:sourceloc} {:sourceloc_num 82} _CHECK_READ_$$A(p53$2, BV_EXTRACT(BV64_ADD(BV64_MUL(BV64_SUB(BV64_ADD(v2$2, $c1.0$2), 1), BV32_SEXT64($n)), BV64_SUB(BV64_SUB(BV64_SUB(BV64_ADD(BV64_MUL(-2, v2$2), $c0), BV64_MUL(2, $c1.0$2)), BV64_MUL(4, $c2.0$2)), BV64_MUL(4, $c4.0$2))), 32, 0), $$A[BV_EXTRACT(BV64_ADD(BV64_MUL(BV64_SUB(BV64_ADD(v2$2, $c1.0$2), 1), BV32_SEXT64($n)), BV64_SUB(BV64_SUB(BV64_SUB(BV64_ADD(BV64_MUL(-2, v2$2), $c0), BV64_MUL(2, $c1.0$2)), BV64_MUL(4, $c2.0$2)), BV64_MUL(4, $c4.0$2))), 32, 0)]);
    assume {:captureState "call_return_state_0"} {:procedureName "_CHECK_READ_$$A"} true;
    v32$1 := (if p53$1 then $$A[BV_EXTRACT(BV64_ADD(BV64_MUL(BV64_SUB(BV64_ADD(v2$1, $c1.0$1), 1), BV32_SEXT64($n)), BV64_SUB(BV64_SUB(BV64_SUB(BV64_ADD(BV64_MUL(-2, v2$1), $c0), BV64_MUL(2, $c1.0$1)), BV64_MUL(4, $c2.0$1)), BV64_MUL(4, $c4.0$1))), 32, 0)] else v32$1);
    v32$2 := (if p53$2 then $$A[BV_EXTRACT(BV64_ADD(BV64_MUL(BV64_SUB(BV64_ADD(v2$2, $c1.0$2), 1), BV32_SEXT64($n)), BV64_SUB(BV64_SUB(BV64_SUB(BV64_ADD(BV64_MUL(-2, v2$2), $c0), BV64_MUL(2, $c1.0$2)), BV64_MUL(4, $c2.0$2)), BV64_MUL(4, $c4.0$2))), 32, 0)] else v32$2);
    call {:sourceloc} {:sourceloc_num 83} _LOG_READ_$$A(p53$1, BV_EXTRACT(BV64_ADD(BV64_MUL(BV64_SUB(BV64_ADD(v2$1, $c1.0$1), 1), BV32_SEXT64($n)), BV64_ADD(BV64_SUB(BV64_SUB(BV64_SUB(BV64_ADD(BV64_MUL(-2, v2$1), $c0), BV64_MUL(2, $c1.0$1)), BV64_MUL(4, $c2.0$1)), BV64_MUL(4, $c4.0$1)), 1)), 32, 0), $$A[BV_EXTRACT(BV64_ADD(BV64_MUL(BV64_SUB(BV64_ADD(v2$1, $c1.0$1), 1), BV32_SEXT64($n)), BV64_ADD(BV64_SUB(BV64_SUB(BV64_SUB(BV64_ADD(BV64_MUL(-2, v2$1), $c0), BV64_MUL(2, $c1.0$1)), BV64_MUL(4, $c2.0$1)), BV64_MUL(4, $c4.0$1)), 1)), 32, 0)]);
    assume {:do_not_predicate} {:check_id "check_state_2"} {:captureState "check_state_2"} {:sourceloc} {:sourceloc_num 83} true;
    call {:check_id "check_state_2"} {:sourceloc} {:sourceloc_num 83} _CHECK_READ_$$A(p53$2, BV_EXTRACT(BV64_ADD(BV64_MUL(BV64_SUB(BV64_ADD(v2$2, $c1.0$2), 1), BV32_SEXT64($n)), BV64_ADD(BV64_SUB(BV64_SUB(BV64_SUB(BV64_ADD(BV64_MUL(-2, v2$2), $c0), BV64_MUL(2, $c1.0$2)), BV64_MUL(4, $c2.0$2)), BV64_MUL(4, $c4.0$2)), 1)), 32, 0), $$A[BV_EXTRACT(BV64_ADD(BV64_MUL(BV64_SUB(BV64_ADD(v2$2, $c1.0$2), 1), BV32_SEXT64($n)), BV64_ADD(BV64_SUB(BV64_SUB(BV64_SUB(BV64_ADD(BV64_MUL(-2, v2$2), $c0), BV64_MUL(2, $c1.0$2)), BV64_MUL(4, $c2.0$2)), BV64_MUL(4, $c4.0$2)), 1)), 32, 0)]);
    assume {:captureState "call_return_state_0"} {:procedureName "_CHECK_READ_$$A"} true;
    v33$1 := (if p53$1 then $$A[BV_EXTRACT(BV64_ADD(BV64_MUL(BV64_SUB(BV64_ADD(v2$1, $c1.0$1), 1), BV32_SEXT64($n)), BV64_ADD(BV64_SUB(BV64_SUB(BV64_SUB(BV64_ADD(BV64_MUL(-2, v2$1), $c0), BV64_MUL(2, $c1.0$1)), BV64_MUL(4, $c2.0$1)), BV64_MUL(4, $c4.0$1)), 1)), 32, 0)] else v33$1);
    v33$2 := (if p53$2 then $$A[BV_EXTRACT(BV64_ADD(BV64_MUL(BV64_SUB(BV64_ADD(v2$2, $c1.0$2), 1), BV32_SEXT64($n)), BV64_ADD(BV64_SUB(BV64_SUB(BV64_SUB(BV64_ADD(BV64_MUL(-2, v2$2), $c0), BV64_MUL(2, $c1.0$2)), BV64_MUL(4, $c2.0$2)), BV64_MUL(4, $c4.0$2)), 1)), 32, 0)] else v33$2);
    call {:sourceloc} {:sourceloc_num 84} _LOG_READ_$$A(p53$1, BV_EXTRACT(BV64_ADD(BV64_MUL(BV64_ADD(v2$1, $c1.0$1), BV32_SEXT64($n)), BV64_SUB(BV64_SUB(BV64_SUB(BV64_SUB(BV64_ADD(BV64_MUL(-2, v2$1), $c0), BV64_MUL(2, $c1.0$1)), BV64_MUL(4, $c2.0$1)), BV64_MUL(4, $c4.0$1)), 1)), 32, 0), $$A[BV_EXTRACT(BV64_ADD(BV64_MUL(BV64_ADD(v2$1, $c1.0$1), BV32_SEXT64($n)), BV64_SUB(BV64_SUB(BV64_SUB(BV64_SUB(BV64_ADD(BV64_MUL(-2, v2$1), $c0), BV64_MUL(2, $c1.0$1)), BV64_MUL(4, $c2.0$1)), BV64_MUL(4, $c4.0$1)), 1)), 32, 0)]);
    assume {:do_not_predicate} {:check_id "check_state_3"} {:captureState "check_state_3"} {:sourceloc} {:sourceloc_num 84} true;
    call {:check_id "check_state_3"} {:sourceloc} {:sourceloc_num 84} _CHECK_READ_$$A(p53$2, BV_EXTRACT(BV64_ADD(BV64_MUL(BV64_ADD(v2$2, $c1.0$2), BV32_SEXT64($n)), BV64_SUB(BV64_SUB(BV64_SUB(BV64_SUB(BV64_ADD(BV64_MUL(-2, v2$2), $c0), BV64_MUL(2, $c1.0$2)), BV64_MUL(4, $c2.0$2)), BV64_MUL(4, $c4.0$2)), 1)), 32, 0), $$A[BV_EXTRACT(BV64_ADD(BV64_MUL(BV64_ADD(v2$2, $c1.0$2), BV32_SEXT64($n)), BV64_SUB(BV64_SUB(BV64_SUB(BV64_SUB(BV64_ADD(BV64_MUL(-2, v2$2), $c0), BV64_MUL(2, $c1.0$2)), BV64_MUL(4, $c2.0$2)), BV64_MUL(4, $c4.0$2)), 1)), 32, 0)]);
    assume {:captureState "call_return_state_0"} {:procedureName "_CHECK_READ_$$A"} true;
    v34$1 := (if p53$1 then $$A[BV_EXTRACT(BV64_ADD(BV64_MUL(BV64_ADD(v2$1, $c1.0$1), BV32_SEXT64($n)), BV64_SUB(BV64_SUB(BV64_SUB(BV64_SUB(BV64_ADD(BV64_MUL(-2, v2$1), $c0), BV64_MUL(2, $c1.0$1)), BV64_MUL(4, $c2.0$1)), BV64_MUL(4, $c4.0$1)), 1)), 32, 0)] else v34$1);
    v34$2 := (if p53$2 then $$A[BV_EXTRACT(BV64_ADD(BV64_MUL(BV64_ADD(v2$2, $c1.0$2), BV32_SEXT64($n)), BV64_SUB(BV64_SUB(BV64_SUB(BV64_SUB(BV64_ADD(BV64_MUL(-2, v2$2), $c0), BV64_MUL(2, $c1.0$2)), BV64_MUL(4, $c2.0$2)), BV64_MUL(4, $c4.0$2)), 1)), 32, 0)] else v34$2);
    call {:sourceloc} {:sourceloc_num 85} _LOG_READ_$$A(p53$1, BV_EXTRACT(BV64_ADD(BV64_MUL(BV64_ADD(v2$1, $c1.0$1), BV32_SEXT64($n)), BV64_SUB(BV64_SUB(BV64_SUB(BV64_ADD(BV64_MUL(-2, v2$1), $c0), BV64_MUL(2, $c1.0$1)), BV64_MUL(4, $c2.0$1)), BV64_MUL(4, $c4.0$1))), 32, 0), $$A[BV_EXTRACT(BV64_ADD(BV64_MUL(BV64_ADD(v2$1, $c1.0$1), BV32_SEXT64($n)), BV64_SUB(BV64_SUB(BV64_SUB(BV64_ADD(BV64_MUL(-2, v2$1), $c0), BV64_MUL(2, $c1.0$1)), BV64_MUL(4, $c2.0$1)), BV64_MUL(4, $c4.0$1))), 32, 0)]);
    assume {:do_not_predicate} {:check_id "check_state_4"} {:captureState "check_state_4"} {:sourceloc} {:sourceloc_num 85} true;
    call {:check_id "check_state_4"} {:sourceloc} {:sourceloc_num 85} _CHECK_READ_$$A(p53$2, BV_EXTRACT(BV64_ADD(BV64_MUL(BV64_ADD(v2$2, $c1.0$2), BV32_SEXT64($n)), BV64_SUB(BV64_SUB(BV64_SUB(BV64_ADD(BV64_MUL(-2, v2$2), $c0), BV64_MUL(2, $c1.0$2)), BV64_MUL(4, $c2.0$2)), BV64_MUL(4, $c4.0$2))), 32, 0), $$A[BV_EXTRACT(BV64_ADD(BV64_MUL(BV64_ADD(v2$2, $c1.0$2), BV32_SEXT64($n)), BV64_SUB(BV64_SUB(BV64_SUB(BV64_ADD(BV64_MUL(-2, v2$2), $c0), BV64_MUL(2, $c1.0$2)), BV64_MUL(4, $c2.0$2)), BV64_MUL(4, $c4.0$2))), 32, 0)]);
    assume {:captureState "call_return_state_0"} {:procedureName "_CHECK_READ_$$A"} true;
    v35$1 := (if p53$1 then $$A[BV_EXTRACT(BV64_ADD(BV64_MUL(BV64_ADD(v2$1, $c1.0$1), BV32_SEXT64($n)), BV64_SUB(BV64_SUB(BV64_SUB(BV64_ADD(BV64_MUL(-2, v2$1), $c0), BV64_MUL(2, $c1.0$1)), BV64_MUL(4, $c2.0$1)), BV64_MUL(4, $c4.0$1))), 32, 0)] else v35$1);
    v35$2 := (if p53$2 then $$A[BV_EXTRACT(BV64_ADD(BV64_MUL(BV64_ADD(v2$2, $c1.0$2), BV32_SEXT64($n)), BV64_SUB(BV64_SUB(BV64_SUB(BV64_ADD(BV64_MUL(-2, v2$2), $c0), BV64_MUL(2, $c1.0$2)), BV64_MUL(4, $c2.0$2)), BV64_MUL(4, $c4.0$2))), 32, 0)] else v35$2);
    call {:sourceloc} {:sourceloc_num 86} _LOG_READ_$$A(p53$1, BV_EXTRACT(BV64_ADD(BV64_MUL(BV64_ADD(v2$1, $c1.0$1), BV32_SEXT64($n)), BV64_ADD(BV64_SUB(BV64_SUB(BV64_SUB(BV64_ADD(BV64_MUL(-2, v2$1), $c0), BV64_MUL(2, $c1.0$1)), BV64_MUL(4, $c2.0$1)), BV64_MUL(4, $c4.0$1)), 1)), 32, 0), $$A[BV_EXTRACT(BV64_ADD(BV64_MUL(BV64_ADD(v2$1, $c1.0$1), BV32_SEXT64($n)), BV64_ADD(BV64_SUB(BV64_SUB(BV64_SUB(BV64_ADD(BV64_MUL(-2, v2$1), $c0), BV64_MUL(2, $c1.0$1)), BV64_MUL(4, $c2.0$1)), BV64_MUL(4, $c4.0$1)), 1)), 32, 0)]);
    assume {:do_not_predicate} {:check_id "check_state_5"} {:captureState "check_state_5"} {:sourceloc} {:sourceloc_num 86} true;
    call {:check_id "check_state_5"} {:sourceloc} {:sourceloc_num 86} _CHECK_READ_$$A(p53$2, BV_EXTRACT(BV64_ADD(BV64_MUL(BV64_ADD(v2$2, $c1.0$2), BV32_SEXT64($n)), BV64_ADD(BV64_SUB(BV64_SUB(BV64_SUB(BV64_ADD(BV64_MUL(-2, v2$2), $c0), BV64_MUL(2, $c1.0$2)), BV64_MUL(4, $c2.0$2)), BV64_MUL(4, $c4.0$2)), 1)), 32, 0), $$A[BV_EXTRACT(BV64_ADD(BV64_MUL(BV64_ADD(v2$2, $c1.0$2), BV32_SEXT64($n)), BV64_ADD(BV64_SUB(BV64_SUB(BV64_SUB(BV64_ADD(BV64_MUL(-2, v2$2), $c0), BV64_MUL(2, $c1.0$2)), BV64_MUL(4, $c2.0$2)), BV64_MUL(4, $c4.0$2)), 1)), 32, 0)]);
    assume {:captureState "call_return_state_0"} {:procedureName "_CHECK_READ_$$A"} true;
    v36$1 := (if p53$1 then $$A[BV_EXTRACT(BV64_ADD(BV64_MUL(BV64_ADD(v2$1, $c1.0$1), BV32_SEXT64($n)), BV64_ADD(BV64_SUB(BV64_SUB(BV64_SUB(BV64_ADD(BV64_MUL(-2, v2$1), $c0), BV64_MUL(2, $c1.0$1)), BV64_MUL(4, $c2.0$1)), BV64_MUL(4, $c4.0$1)), 1)), 32, 0)] else v36$1);
    v36$2 := (if p53$2 then $$A[BV_EXTRACT(BV64_ADD(BV64_MUL(BV64_ADD(v2$2, $c1.0$2), BV32_SEXT64($n)), BV64_ADD(BV64_SUB(BV64_SUB(BV64_SUB(BV64_ADD(BV64_MUL(-2, v2$2), $c0), BV64_MUL(2, $c1.0$2)), BV64_MUL(4, $c2.0$2)), BV64_MUL(4, $c4.0$2)), 1)), 32, 0)] else v36$2);
    call {:sourceloc} {:sourceloc_num 87} _LOG_READ_$$A(p53$1, BV_EXTRACT(BV64_ADD(BV64_MUL(BV64_ADD(BV64_ADD(v2$1, $c1.0$1), 1), BV32_SEXT64($n)), BV64_SUB(BV64_SUB(BV64_SUB(BV64_SUB(BV64_ADD(BV64_MUL(-2, v2$1), $c0), BV64_MUL(2, $c1.0$1)), BV64_MUL(4, $c2.0$1)), BV64_MUL(4, $c4.0$1)), 1)), 32, 0), $$A[BV_EXTRACT(BV64_ADD(BV64_MUL(BV64_ADD(BV64_ADD(v2$1, $c1.0$1), 1), BV32_SEXT64($n)), BV64_SUB(BV64_SUB(BV64_SUB(BV64_SUB(BV64_ADD(BV64_MUL(-2, v2$1), $c0), BV64_MUL(2, $c1.0$1)), BV64_MUL(4, $c2.0$1)), BV64_MUL(4, $c4.0$1)), 1)), 32, 0)]);
    assume {:do_not_predicate} {:check_id "check_state_6"} {:captureState "check_state_6"} {:sourceloc} {:sourceloc_num 87} true;
    call {:check_id "check_state_6"} {:sourceloc} {:sourceloc_num 87} _CHECK_READ_$$A(p53$2, BV_EXTRACT(BV64_ADD(BV64_MUL(BV64_ADD(BV64_ADD(v2$2, $c1.0$2), 1), BV32_SEXT64($n)), BV64_SUB(BV64_SUB(BV64_SUB(BV64_SUB(BV64_ADD(BV64_MUL(-2, v2$2), $c0), BV64_MUL(2, $c1.0$2)), BV64_MUL(4, $c2.0$2)), BV64_MUL(4, $c4.0$2)), 1)), 32, 0), $$A[BV_EXTRACT(BV64_ADD(BV64_MUL(BV64_ADD(BV64_ADD(v2$2, $c1.0$2), 1), BV32_SEXT64($n)), BV64_SUB(BV64_SUB(BV64_SUB(BV64_SUB(BV64_ADD(BV64_MUL(-2, v2$2), $c0), BV64_MUL(2, $c1.0$2)), BV64_MUL(4, $c2.0$2)), BV64_MUL(4, $c4.0$2)), 1)), 32, 0)]);
    assume {:captureState "call_return_state_0"} {:procedureName "_CHECK_READ_$$A"} true;
    v37$1 := (if p53$1 then $$A[BV_EXTRACT(BV64_ADD(BV64_MUL(BV64_ADD(BV64_ADD(v2$1, $c1.0$1), 1), BV32_SEXT64($n)), BV64_SUB(BV64_SUB(BV64_SUB(BV64_SUB(BV64_ADD(BV64_MUL(-2, v2$1), $c0), BV64_MUL(2, $c1.0$1)), BV64_MUL(4, $c2.0$1)), BV64_MUL(4, $c4.0$1)), 1)), 32, 0)] else v37$1);
    v37$2 := (if p53$2 then $$A[BV_EXTRACT(BV64_ADD(BV64_MUL(BV64_ADD(BV64_ADD(v2$2, $c1.0$2), 1), BV32_SEXT64($n)), BV64_SUB(BV64_SUB(BV64_SUB(BV64_SUB(BV64_ADD(BV64_MUL(-2, v2$2), $c0), BV64_MUL(2, $c1.0$2)), BV64_MUL(4, $c2.0$2)), BV64_MUL(4, $c4.0$2)), 1)), 32, 0)] else v37$2);
    call {:sourceloc} {:sourceloc_num 88} _LOG_READ_$$A(p53$1, BV_EXTRACT(BV64_ADD(BV64_MUL(BV64_ADD(BV64_ADD(v2$1, $c1.0$1), 1), BV32_SEXT64($n)), BV64_SUB(BV64_SUB(BV64_SUB(BV64_ADD(BV64_MUL(-2, v2$1), $c0), BV64_MUL(2, $c1.0$1)), BV64_MUL(4, $c2.0$1)), BV64_MUL(4, $c4.0$1))), 32, 0), $$A[BV_EXTRACT(BV64_ADD(BV64_MUL(BV64_ADD(BV64_ADD(v2$1, $c1.0$1), 1), BV32_SEXT64($n)), BV64_SUB(BV64_SUB(BV64_SUB(BV64_ADD(BV64_MUL(-2, v2$1), $c0), BV64_MUL(2, $c1.0$1)), BV64_MUL(4, $c2.0$1)), BV64_MUL(4, $c4.0$1))), 32, 0)]);
    assume {:do_not_predicate} {:check_id "check_state_7"} {:captureState "check_state_7"} {:sourceloc} {:sourceloc_num 88} true;
    call {:check_id "check_state_7"} {:sourceloc} {:sourceloc_num 88} _CHECK_READ_$$A(p53$2, BV_EXTRACT(BV64_ADD(BV64_MUL(BV64_ADD(BV64_ADD(v2$2, $c1.0$2), 1), BV32_SEXT64($n)), BV64_SUB(BV64_SUB(BV64_SUB(BV64_ADD(BV64_MUL(-2, v2$2), $c0), BV64_MUL(2, $c1.0$2)), BV64_MUL(4, $c2.0$2)), BV64_MUL(4, $c4.0$2))), 32, 0), $$A[BV_EXTRACT(BV64_ADD(BV64_MUL(BV64_ADD(BV64_ADD(v2$2, $c1.0$2), 1), BV32_SEXT64($n)), BV64_SUB(BV64_SUB(BV64_SUB(BV64_ADD(BV64_MUL(-2, v2$2), $c0), BV64_MUL(2, $c1.0$2)), BV64_MUL(4, $c2.0$2)), BV64_MUL(4, $c4.0$2))), 32, 0)]);
    assume {:captureState "call_return_state_0"} {:procedureName "_CHECK_READ_$$A"} true;
    v38$1 := (if p53$1 then $$A[BV_EXTRACT(BV64_ADD(BV64_MUL(BV64_ADD(BV64_ADD(v2$1, $c1.0$1), 1), BV32_SEXT64($n)), BV64_SUB(BV64_SUB(BV64_SUB(BV64_ADD(BV64_MUL(-2, v2$1), $c0), BV64_MUL(2, $c1.0$1)), BV64_MUL(4, $c2.0$1)), BV64_MUL(4, $c4.0$1))), 32, 0)] else v38$1);
    v38$2 := (if p53$2 then $$A[BV_EXTRACT(BV64_ADD(BV64_MUL(BV64_ADD(BV64_ADD(v2$2, $c1.0$2), 1), BV32_SEXT64($n)), BV64_SUB(BV64_SUB(BV64_SUB(BV64_ADD(BV64_MUL(-2, v2$2), $c0), BV64_MUL(2, $c1.0$2)), BV64_MUL(4, $c2.0$2)), BV64_MUL(4, $c4.0$2))), 32, 0)] else v38$2);
    call {:sourceloc} {:sourceloc_num 89} _LOG_READ_$$A(p53$1, BV_EXTRACT(BV64_ADD(BV64_MUL(BV64_ADD(BV64_ADD(v2$1, $c1.0$1), 1), BV32_SEXT64($n)), BV64_ADD(BV64_SUB(BV64_SUB(BV64_SUB(BV64_ADD(BV64_MUL(-2, v2$1), $c0), BV64_MUL(2, $c1.0$1)), BV64_MUL(4, $c2.0$1)), BV64_MUL(4, $c4.0$1)), 1)), 32, 0), $$A[BV_EXTRACT(BV64_ADD(BV64_MUL(BV64_ADD(BV64_ADD(v2$1, $c1.0$1), 1), BV32_SEXT64($n)), BV64_ADD(BV64_SUB(BV64_SUB(BV64_SUB(BV64_ADD(BV64_MUL(-2, v2$1), $c0), BV64_MUL(2, $c1.0$1)), BV64_MUL(4, $c2.0$1)), BV64_MUL(4, $c4.0$1)), 1)), 32, 0)]);
    assume {:do_not_predicate} {:check_id "check_state_8"} {:captureState "check_state_8"} {:sourceloc} {:sourceloc_num 89} true;
    call {:check_id "check_state_8"} {:sourceloc} {:sourceloc_num 89} _CHECK_READ_$$A(p53$2, BV_EXTRACT(BV64_ADD(BV64_MUL(BV64_ADD(BV64_ADD(v2$2, $c1.0$2), 1), BV32_SEXT64($n)), BV64_ADD(BV64_SUB(BV64_SUB(BV64_SUB(BV64_ADD(BV64_MUL(-2, v2$2), $c0), BV64_MUL(2, $c1.0$2)), BV64_MUL(4, $c2.0$2)), BV64_MUL(4, $c4.0$2)), 1)), 32, 0), $$A[BV_EXTRACT(BV64_ADD(BV64_MUL(BV64_ADD(BV64_ADD(v2$2, $c1.0$2), 1), BV32_SEXT64($n)), BV64_ADD(BV64_SUB(BV64_SUB(BV64_SUB(BV64_ADD(BV64_MUL(-2, v2$2), $c0), BV64_MUL(2, $c1.0$2)), BV64_MUL(4, $c2.0$2)), BV64_MUL(4, $c4.0$2)), 1)), 32, 0)]);
    assume {:captureState "call_return_state_0"} {:procedureName "_CHECK_READ_$$A"} true;
    v39$1 := (if p53$1 then $$A[BV_EXTRACT(BV64_ADD(BV64_MUL(BV64_ADD(BV64_ADD(v2$1, $c1.0$1), 1), BV32_SEXT64($n)), BV64_ADD(BV64_SUB(BV64_SUB(BV64_SUB(BV64_ADD(BV64_MUL(-2, v2$1), $c0), BV64_MUL(2, $c1.0$1)), BV64_MUL(4, $c2.0$1)), BV64_MUL(4, $c4.0$1)), 1)), 32, 0)] else v39$1);
    v39$2 := (if p53$2 then $$A[BV_EXTRACT(BV64_ADD(BV64_MUL(BV64_ADD(BV64_ADD(v2$2, $c1.0$2), 1), BV32_SEXT64($n)), BV64_ADD(BV64_SUB(BV64_SUB(BV64_SUB(BV64_ADD(BV64_MUL(-2, v2$2), $c0), BV64_MUL(2, $c1.0$2)), BV64_MUL(4, $c2.0$2)), BV64_MUL(4, $c4.0$2)), 1)), 32, 0)] else v39$2);
    call {:sourceloc} {:sourceloc_num 90} _LOG_WRITE_$$A(p53$1, BV_EXTRACT(BV64_ADD(BV64_MUL(BV64_ADD(v2$1, $c1.0$1), BV32_SEXT64($n)), BV64_SUB(BV64_SUB(BV64_SUB(BV64_ADD(BV64_MUL(-2, v2$1), $c0), BV64_MUL(2, $c1.0$1)), BV64_MUL(4, $c2.0$1)), BV64_MUL(4, $c4.0$1))), 32, 0), FDIV64(FADD64(FADD64(FADD64(FADD64(FADD64(FADD64(FADD64(FADD64(v31$1, v32$1), v33$1), v34$1), v35$1), v36$1), v37$1), v38$1), v39$1), 4621256167635550208), $$A[BV_EXTRACT(BV64_ADD(BV64_MUL(BV64_ADD(v2$1, $c1.0$1), BV32_SEXT64($n)), BV64_SUB(BV64_SUB(BV64_SUB(BV64_ADD(BV64_MUL(-2, v2$1), $c0), BV64_MUL(2, $c1.0$1)), BV64_MUL(4, $c2.0$1)), BV64_MUL(4, $c4.0$1))), 32, 0)]);
    call _UPDATE_WRITE_READ_BENIGN_FLAG_$$A(p53$2, BV_EXTRACT(BV64_ADD(BV64_MUL(BV64_ADD(v2$2, $c1.0$2), BV32_SEXT64($n)), BV64_SUB(BV64_SUB(BV64_SUB(BV64_ADD(BV64_MUL(-2, v2$2), $c0), BV64_MUL(2, $c1.0$2)), BV64_MUL(4, $c2.0$2)), BV64_MUL(4, $c4.0$2))), 32, 0));
    assume {:do_not_predicate} {:check_id "check_state_9"} {:captureState "check_state_9"} {:sourceloc} {:sourceloc_num 90} true;
    call {:check_id "check_state_9"} {:sourceloc} {:sourceloc_num 90} _CHECK_WRITE_$$A(p53$2, BV_EXTRACT(BV64_ADD(BV64_MUL(BV64_ADD(v2$2, $c1.0$2), BV32_SEXT64($n)), BV64_SUB(BV64_SUB(BV64_SUB(BV64_ADD(BV64_MUL(-2, v2$2), $c0), BV64_MUL(2, $c1.0$2)), BV64_MUL(4, $c2.0$2)), BV64_MUL(4, $c4.0$2))), 32, 0), FDIV64(FADD64(FADD64(FADD64(FADD64(FADD64(FADD64(FADD64(FADD64(v31$2, v32$2), v33$2), v34$2), v35$2), v36$2), v37$2), v38$2), v39$2), 4621256167635550208));
    assume {:captureState "call_return_state_0"} {:procedureName "_CHECK_WRITE_$$A"} true;
    $$A[BV_EXTRACT(BV64_ADD(BV64_MUL(BV64_ADD(v2$1, $c1.0$1), BV32_SEXT64($n)), BV64_SUB(BV64_SUB(BV64_SUB(BV64_ADD(BV64_MUL(-2, v2$1), $c0), BV64_MUL(2, $c1.0$1)), BV64_MUL(4, $c2.0$1)), BV64_MUL(4, $c4.0$1))), 32, 0)] := (if p53$1 then FDIV64(FADD64(FADD64(FADD64(FADD64(FADD64(FADD64(FADD64(FADD64(v31$1, v32$1), v33$1), v34$1), v35$1), v36$1), v37$1), v38$1), v39$1), 4621256167635550208) else $$A[BV_EXTRACT(BV64_ADD(BV64_MUL(BV64_ADD(v2$1, $c1.0$1), BV32_SEXT64($n)), BV64_SUB(BV64_SUB(BV64_SUB(BV64_ADD(BV64_MUL(-2, v2$1), $c0), BV64_MUL(2, $c1.0$1)), BV64_MUL(4, $c2.0$1)), BV64_MUL(4, $c4.0$1))), 32, 0)]);
    $$A[BV_EXTRACT(BV64_ADD(BV64_MUL(BV64_ADD(v2$2, $c1.0$2), BV32_SEXT64($n)), BV64_SUB(BV64_SUB(BV64_SUB(BV64_ADD(BV64_MUL(-2, v2$2), $c0), BV64_MUL(2, $c1.0$2)), BV64_MUL(4, $c2.0$2)), BV64_MUL(4, $c4.0$2))), 32, 0)] := (if p53$2 then FDIV64(FADD64(FADD64(FADD64(FADD64(FADD64(FADD64(FADD64(FADD64(v31$2, v32$2), v33$2), v34$2), v35$2), v36$2), v37$2), v38$2), v39$2), 4621256167635550208) else $$A[BV_EXTRACT(BV64_ADD(BV64_MUL(BV64_ADD(v2$2, $c1.0$2), BV32_SEXT64($n)), BV64_SUB(BV64_SUB(BV64_SUB(BV64_ADD(BV64_MUL(-2, v2$2), $c0), BV64_MUL(2, $c1.0$2)), BV64_MUL(4, $c2.0$2)), BV64_MUL(4, $c4.0$2))), 32, 0)]);
    $c4.0$1 := (if p53$1 then BV64_ADD($c4.0$1, 16) else $c4.0$1);
    $c4.0$2 := (if p53$2 then BV64_ADD($c4.0$2, 16) else $c4.0$2);
    p46$1 := (if p53$1 then true else p46$1);
    p46$2 := (if p53$2 then true else p46$2);
    goto $64.backedge, $64.tail;

  $64.tail:
    assume !p46$1 && !p46$2;
    $c2.0$1 := (if p39$1 then BV64_ADD($c2.0$1, 8192) else $c2.0$1);
    $c2.0$2 := (if p39$2 then BV64_ADD($c2.0$2, 8192) else $c2.0$2);
    p32$1 := (if p39$1 then true else p32$1);
    p32$2 := (if p39$2 then true else p32$2);
    goto $44.backedge, $44.tail;

  $44.tail:
    assume !p32$1 && !p32$2;
    $c1.0$1 := (if p9$1 then BV64_ADD($c1.0$1, 8192) else $c1.0$1);
    $c1.0$2 := (if p9$2 then BV64_ADD($c1.0$2, 8192) else $c1.0$2);
    p6$1 := (if p9$1 then true else p6$1);
    p6$2 := (if p9$2 then true else p6$2);
    goto $10.backedge, $10.tail;

  $10.tail:
    assume !p6$1 && !p6$2;
    return;

  $10.backedge:
    assume {:backedge} p6$1 || p6$2;
    assume {:captureState "loop_back_edge_state_0_0"} true;
    goto $10;

  $44.backedge:
    assume {:backedge} p32$1 || p32$2;
    assume {:captureState "loop_back_edge_state_1_0"} true;
    goto $44;

  $64.backedge:
    assume {:backedge} p46$1 || p46$2;
    assume {:captureState "loop_back_edge_state_2_0"} true;
    goto $64;
}



axiom (if group_size_z == 1 then 1 else 0) != 0;

axiom (if num_groups_z == 1 then 1 else 0) != 0;

axiom (if group_size_x == 32 then 1 else 0) != 0;

axiom (if group_size_y == 16 then 1 else 0) != 0;

axiom (if num_groups_x == 2 then 1 else 0) != 0;

axiom (if num_groups_y == 2 then 1 else 0) != 0;

axiom (if global_offset_x == 0 then 1 else 0) != 0;

axiom (if global_offset_y == 0 then 1 else 0) != 0;

axiom (if global_offset_z == 0 then 1 else 0) != 0;

const {:local_id_z} local_id_z$1: int;

const {:local_id_z} local_id_z$2: int;

const {:group_id_z} group_id_z$1: int;

const {:group_id_z} group_id_z$2: int;





































































const _WATCHED_VALUE_$$A: int;

procedure {:inline 1} _LOG_READ_$$A(_P: bool, _offset: int, _value: int);
  modifies _READ_HAS_OCCURRED_$$A;



implementation {:inline 1} _LOG_READ_$$A(_P: bool, _offset: int, _value: int)
{

  log_access_entry:
    _READ_HAS_OCCURRED_$$A := (if _P && _TRACKING && _WATCHED_OFFSET == _offset && _WATCHED_VALUE_$$A == _value then true else _READ_HAS_OCCURRED_$$A);
    return;
}



procedure _CHECK_READ_$$A(_P: bool, _offset: int, _value: int);
  requires {:source_name "A"} {:array "$$A"} {:race} {:write_read} !(_P && _WRITE_HAS_OCCURRED_$$A && _WATCHED_OFFSET == _offset && _WRITE_READ_BENIGN_FLAG_$$A);
  requires {:source_name "A"} {:array "$$A"} {:race} {:atomic_read} !(_P && _ATOMIC_HAS_OCCURRED_$$A && _WATCHED_OFFSET == _offset);



var _WRITE_READ_BENIGN_FLAG_$$A: bool;

procedure {:inline 1} _LOG_WRITE_$$A(_P: bool, _offset: int, _value: int, _value_old: int);
  modifies _WRITE_HAS_OCCURRED_$$A, _WRITE_READ_BENIGN_FLAG_$$A;



implementation {:inline 1} _LOG_WRITE_$$A(_P: bool, _offset: int, _value: int, _value_old: int)
{

  log_access_entry:
    _WRITE_HAS_OCCURRED_$$A := (if _P && _TRACKING && _WATCHED_OFFSET == _offset && _WATCHED_VALUE_$$A == _value then true else _WRITE_HAS_OCCURRED_$$A);
    _WRITE_READ_BENIGN_FLAG_$$A := (if _P && _TRACKING && _WATCHED_OFFSET == _offset && _WATCHED_VALUE_$$A == _value then _value != _value_old else _WRITE_READ_BENIGN_FLAG_$$A);
    return;
}



procedure _CHECK_WRITE_$$A(_P: bool, _offset: int, _value: int);
  requires {:source_name "A"} {:array "$$A"} {:race} {:write_write} !(_P && _WRITE_HAS_OCCURRED_$$A && _WATCHED_OFFSET == _offset && _WATCHED_VALUE_$$A != _value);
  requires {:source_name "A"} {:array "$$A"} {:race} {:read_write} !(_P && _READ_HAS_OCCURRED_$$A && _WATCHED_OFFSET == _offset && _WATCHED_VALUE_$$A != _value);
  requires {:source_name "A"} {:array "$$A"} {:race} {:atomic_write} !(_P && _ATOMIC_HAS_OCCURRED_$$A && _WATCHED_OFFSET == _offset);



procedure {:inline 1} _LOG_ATOMIC_$$A(_P: bool, _offset: int);
  modifies _ATOMIC_HAS_OCCURRED_$$A;



implementation {:inline 1} _LOG_ATOMIC_$$A(_P: bool, _offset: int)
{

  log_access_entry:
    _ATOMIC_HAS_OCCURRED_$$A := (if _P && _TRACKING && _WATCHED_OFFSET == _offset then true else _ATOMIC_HAS_OCCURRED_$$A);
    return;
}



procedure _CHECK_ATOMIC_$$A(_P: bool, _offset: int);
  requires {:source_name "A"} {:array "$$A"} {:race} {:write_atomic} !(_P && _WRITE_HAS_OCCURRED_$$A && _WATCHED_OFFSET == _offset);
  requires {:source_name "A"} {:array "$$A"} {:race} {:read_atomic} !(_P && _READ_HAS_OCCURRED_$$A && _WATCHED_OFFSET == _offset);



procedure {:inline 1} _UPDATE_WRITE_READ_BENIGN_FLAG_$$A(_P: bool, _offset: int);
  modifies _WRITE_READ_BENIGN_FLAG_$$A;



implementation {:inline 1} _UPDATE_WRITE_READ_BENIGN_FLAG_$$A(_P: bool, _offset: int)
{

  _UPDATE_BENIGN_FLAG:
    _WRITE_READ_BENIGN_FLAG_$$A := (if _P && _WRITE_HAS_OCCURRED_$$A && _WATCHED_OFFSET == _offset then false else _WRITE_READ_BENIGN_FLAG_$$A);
    return;
}



var _TRACKING: bool;

function  BV32_SGT(x: int, y: int) : bool
{
  x > y
}

function  BV32_SLT(x: int, y: int) : bool
{
  x < y
}




















