type _SIZE_T_TYPE = bv32;

procedure _ATOMIC_OP64(x : [int]int, y : int) returns (z : int, A : [int]int);
var {:source_name "A"} {:global} $$A : [int]int;
axiom {:array_info "$$A"} {:global} {:elem_width 64} {:source_name "A"} {:source_elem_width 64} {:source_dimensions "*"} true;
var {:race_checking} {:global} {:elem_width 64} {:source_elem_width 64} {:source_dimensions "*"} _READ_HAS_OCCURRED_$$A : bool;
var {:race_checking} {:global} {:elem_width 64} {:source_elem_width 64} {:source_dimensions "*"} _WRITE_HAS_OCCURRED_$$A : bool;
var {:race_checking} {:global} {:elem_width 64} {:source_elem_width 64} {:source_dimensions "*"} _ATOMIC_HAS_OCCURRED_$$A : bool;

const _WATCHED_OFFSET : int;
const {:global_offset_x} global_offset_x : int;
const {:global_offset_y} global_offset_y : int;
const {:global_offset_z} global_offset_z : int;
const {:group_id_x} group_id_x : int;
const {:group_id_y} group_id_y : int;
const {:group_size_x} group_size_x : int;
const {:group_size_y} group_size_y : int;
const {:group_size_z} group_size_z : int;
const {:local_id_x} local_id_x : int;
const {:local_id_y} local_id_y : int;
const {:num_groups_x} num_groups_x : int;
const {:num_groups_y} num_groups_y : int;
const {:num_groups_z} num_groups_z : int;
function BV32_SEXT64(int) : int;
function BV_EXTRACT(int, int, int) : int;
function FADD64(int, int) : int;
function FDIV64(int, int) : int;
function {:inline true} BV1_ZEXT32(x : int) : int {
  x
}
function {:inline true} BV32_ADD(x : int, y : int) : int {
  x + y
}
function {:inline true} BV32_AND(x : int, y : int) : int {
  if x == y then x else (if x == 0 || y == 0 then 0 else BV32_AND_UF(x, y))
}
function BV32_AND_UF(int, int) : int;
function {:inline true} BV32_MUL(x : int, y : int) : int {
  x * y
}
function {:inline true} BV32_OR(x : int, y : int) : int {
  if x == y then x else (if x == 0 then y else (if y == 0 then x else BV32_OR_UF(x, y)))
}
function BV32_OR_UF(int, int) : int;
function {:inline true} BV32_SGE(x : int, y : int) : bool {
  x >= y
}
function {:inline true} BV32_SLE(x : int, y : int) : bool {
  x <= y
}
function {:inline true} BV32_SUB(x : int, y : int) : int {
  x - y
}
function {:inline true} BV32_UDIV(x : int, y : int) : int {
  x div y
}
function {:inline true} BV32_UGE(x : int, y : int) : bool {
  x >= y
}
function {:inline true} BV32_ULE(x : int, y : int) : bool {
  x <= y
}
function {:inline true} BV32_UREM(x : int, y : int) : int {
  x mod y
}
function {:inline true} BV32_ZEXT64(x : int) : int {
  x
}
function {:inline true} BV64_ADD(x : int, y : int) : int {
  x + y
}
function {:inline true} BV64_MUL(x : int, y : int) : int {
  x * y
}
function {:inline true} BV64_SDIV(x : int, y : int) : int {
  x div y
}
function {:inline true} BV64_SGE(x : int, y : int) : bool {
  x >= y
}
function {:inline true} BV64_SGT(x : int, y : int) : bool {
  x > y
}
function {:inline true} BV64_SLE(x : int, y : int) : bool {
  x <= y
}
function {:inline true} BV64_SLT(x : int, y : int) : bool {
  x < y
}
function {:inline true} BV64_SREM(x : int, y : int) : int {
  x mod y
}
function {:inline true} BV64_SUB(x : int, y : int) : int {
  x - y
}
procedure {:source_name "kernel0"} {:kernel} $kernel0($n:int, $tsteps:int, $c0:int)
requires {:sourceloc_num 0} (if $n == 64 then 1 else 0) != 0;
requires {:sourceloc_num 1} (if $tsteps == 64 then 1 else 0) != 0;
requires {:sourceloc_num 2} (if BV64_SLT($c0, BV32_SEXT64(BV32_SUB(BV32_ADD(BV32_MUL(3, $n), BV32_MUL(4, $tsteps)), 9))) then 1 else 0) != 0;
requires {:sourceloc_num 3} (if BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV1_ZEXT32((if BV32_SGE($n, 3) then 1 else 0)), BV1_ZEXT32((if BV32_SLE($n, 2147483647) then 1 else 0))), BV1_ZEXT32((if BV32_SGE($tsteps, 1) then 1 else 0))), BV1_ZEXT32((if BV32_SLE($tsteps, 2147483647) then 1 else 0))), BV1_ZEXT32((if BV64_SGE(BV32_SEXT64(BV32_ADD(BV32_MUL(3, $n), BV32_MUL(4, $tsteps))), BV64_ADD($c0, 10)) then 1 else 0))), BV1_ZEXT32((if BV64_SGE($c0, 3) then 1 else 0))) != 0 then 1 else 0) != 0;
requires {:procedure_wide_invariant} {:do_not_predicate} {:sourceloc_num 4} (if (_WRITE_HAS_OCCURRED_$$A ==> BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV1_ZEXT32((if BV64_SGE(BV64_ADD(BV64_MUL(32, BV32_ZEXT64(group_id_x)), BV32_ZEXT64(local_id_x)), 0) then 1 else 0)), BV1_ZEXT32((if BV64_SLE(BV64_ADD(BV64_MUL(32, BV32_ZEXT64(group_id_x)), BV32_ZEXT64(local_id_x)), 8191) then 1 else 0))), BV1_ZEXT32((if BV32_UGE($n, BV32_ADD(BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n), 2)) then 1 else 0))), BV1_ZEXT32((if BV32_UGE(BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n), 1) then 1 else 0))), BV1_ZEXT32((if BV32_UGE($n, BV32_ADD(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), 2)) then 1 else 0))), BV1_ZEXT32((if BV32_UGE(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), 1) then 1 else 0))), BV1_ZEXT32((if BV64_SGE(BV32_ZEXT64(BV32_ADD(BV32_ADD(BV32_MUL(4, $tsteps), BV32_MUL(2, BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n))), BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n))), BV64_ADD($c0, 4)) then 1 else 0))), BV1_ZEXT32((if BV64_SGE($c0, BV32_ZEXT64(BV32_ADD(BV32_MUL(2, BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n)), BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n)))) then 1 else 0))), BV1_ZEXT32((if BV64_SGE(BV64_SREM(BV64_ADD(BV32_ZEXT64(BV32_SUB(BV32_MUL(-2, BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n)), BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n))), $c0), 32768), BV64_MUL(128, BV32_ZEXT64(group_id_y))) then 1 else 0))), BV1_ZEXT32((if BV64_SGE(BV64_ADD(BV64_MUL(128, BV32_ZEXT64(group_id_y)), 124), BV64_SREM(BV64_ADD(BV32_ZEXT64(BV32_SUB(BV32_MUL(-2, BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n)), BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n))), $c0), 32768)) then 1 else 0))), BV1_ZEXT32((if BV64_SREM(BV64_SUB(BV64_ADD(BV64_MUL(32, BV32_ZEXT64(group_id_x)), BV32_ZEXT64(local_id_x)), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n))), 8192) == 0 then 1 else 0))), BV1_ZEXT32((if BV64_SREM(BV64_SUB(BV64_ADD(BV64_ADD(BV64_MUL(2, BV32_ZEXT64(local_id_x)), BV64_MUL(4, BV32_ZEXT64(local_id_y))), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n))), $c0), 64) == 0 then 1 else 0))) != 0) then 1 else 0) != 0;
requires {:procedure_wide_invariant} {:do_not_predicate} {:sourceloc_num 5} (if (_READ_HAS_OCCURRED_$$A ==> BV32_OR(BV32_OR(BV32_OR(BV32_OR(BV32_OR(BV32_OR(BV32_OR(BV32_OR(BV32_OR(BV32_OR(BV32_OR(BV32_OR(BV32_OR(BV32_OR(BV32_OR(BV32_OR(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV1_ZEXT32((if BV32_UGE(BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n), 3) then 1 else 0)), BV1_ZEXT32((if BV32_UGE($n, BV32_ADD(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), 3)) then 1 else 0))), BV1_ZEXT32((if BV64_SGE(BV32_ZEXT64(BV32_ADD(BV32_ADD(BV32_MUL(4, $tsteps), BV32_MUL(2, BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n))), BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n))), BV64_ADD($c0, 5)) then 1 else 0))), BV1_ZEXT32((if BV64_SGE(BV32_SEXT64($n), BV64_ADD(BV64_ADD(BV64_SREM(BV64_ADD(BV64_ADD(BV64_ADD(BV64_MUL(32, BV32_ZEXT64(group_id_x)), BV32_ZEXT64(local_id_x)), BV32_ZEXT64(BV32_MUL(8191, BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n)))), 1), 8192), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n))), 1)) then 1 else 0))), BV1_ZEXT32((if BV64_SLE(BV64_SREM(BV64_ADD(BV64_ADD(BV64_ADD(BV64_MUL(32, BV32_ZEXT64(group_id_x)), BV32_ZEXT64(local_id_x)), BV32_ZEXT64(BV32_MUL(8191, BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n)))), 1), 8192), 1) then 1 else 0))), BV1_ZEXT32((if BV64_SGE(BV64_ADD($c0, 1), BV64_ADD(BV64_ADD(BV64_MUL(2, BV64_SREM(BV64_ADD(BV64_ADD(BV64_ADD(BV64_MUL(32, BV32_ZEXT64(group_id_x)), BV32_ZEXT64(local_id_x)), BV32_ZEXT64(BV32_MUL(8191, BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n)))), 1), 8192)), BV32_ZEXT64(BV32_MUL(2, BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n)))), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n)))) then 1 else 0))), BV1_ZEXT32((if BV64_SGE(BV64_MUL(64, (if BV64_SLT(BV64_ADD(BV64_ADD(BV64_ADD(BV64_ADD(BV64_MUL(2, BV32_ZEXT64(local_id_x)), BV64_MUL(4, BV32_ZEXT64(local_id_y))), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n))), BV64_MUL(63, $c0)), 1), 0) then BV64_SUB(0, BV64_SDIV(BV64_SUB(BV64_ADD(BV64_SUB(0, BV64_ADD(BV64_ADD(BV64_ADD(BV64_ADD(BV64_MUL(2, BV32_ZEXT64(local_id_x)), BV64_MUL(4, BV32_ZEXT64(local_id_y))), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n))), BV64_MUL(63, $c0)), 1)), 64), 1), 64)) else BV64_SDIV(BV64_ADD(BV64_ADD(BV64_ADD(BV64_ADD(BV64_MUL(2, BV32_ZEXT64(local_id_x)), BV64_MUL(4, BV32_ZEXT64(local_id_y))), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n))), BV64_MUL(63, $c0)), 1), 64))), BV64_ADD(BV64_ADD(BV64_ADD(BV64_MUL(2, BV32_ZEXT64(local_id_x)), BV64_MUL(4, BV32_ZEXT64(local_id_y))), BV64_MUL(63, $c0)), 1)) then 1 else 0))), BV1_ZEXT32((if BV64_SGE(BV64_SREM(BV64_ADD(BV64_ADD(BV64_ADD(BV64_MUL(32, BV32_ZEXT64(group_id_x)), BV32_ZEXT64(local_id_x)), BV32_ZEXT64(BV32_MUL(8191, BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n)))), 1), 8192), BV64_SREM(BV64_ADD(BV64_ADD(BV64_ADD(BV64_ADD(BV64_MUL(2, BV32_ZEXT64(local_id_x)), BV64_MUL(4, BV32_ZEXT64(local_id_y))), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n))), BV64_MUL(63, $c0)), 1), 64)) then 1 else 0))), BV1_ZEXT32((if BV64_SGE(BV64_ADD(BV32_ZEXT64(local_id_y), BV64_MUL(8192, BV64_SDIV(BV64_ADD(BV64_ADD(BV64_ADD(BV64_ADD(BV64_ADD(BV64_ADD(BV64_ADD(BV64_SUB(0, BV64_MUL(2, BV64_SREM(BV64_ADD(BV64_ADD(BV64_ADD(BV64_MUL(32, BV32_ZEXT64(group_id_x)), BV32_ZEXT64(local_id_x)), BV32_ZEXT64(BV32_MUL(8191, BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n)))), 1), 8192))), BV64_MUL(128, BV32_ZEXT64(group_id_x))), BV64_MUL(128, BV32_ZEXT64(group_id_y))), BV64_MUL(4, BV32_ZEXT64(local_id_x))), BV32_ZEXT64(BV32_MUL(32766, BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n)))), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n))), BV64_MUL(32767, $c0)), 127), 32768))), BV64_ADD(BV64_ADD(BV64_ADD(BV64_ADD(BV64_ADD(BV64_MUL(16, BV32_ZEXT64(group_id_x)), BV64_MUL(32, BV32_ZEXT64(group_id_y))), BV32_ZEXT64(BV32_MUL(4096, BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n)))), BV64_MUL(8176, $c0)), BV64_MUL(4096, BV64_SDIV(BV64_ADD(BV64_ADD(BV64_ADD(BV64_MUL(32, BV32_ZEXT64(group_id_x)), BV32_ZEXT64(local_id_x)), BV32_ZEXT64(BV32_MUL(8191, BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n)))), 1), 8192))), BV64_MUL(16, BV64_SDIV(BV64_ADD(BV64_ADD(BV64_ADD(BV64_ADD(BV64_MUL(2, BV32_ZEXT64(local_id_x)), BV64_MUL(4, BV32_ZEXT64(local_id_y))), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n))), BV64_MUL(63, $c0)), 1), 64)))) then 1 else 0))), BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV1_ZEXT32((if BV32_ZEXT64(group_id_x) == 0 then 1 else 0)), BV1_ZEXT32((if BV64_SGE(BV32_SEXT64($n), BV64_ADD(BV32_ZEXT64(local_id_x), 2)) then 1 else 0))), BV1_ZEXT32((if BV64_SGE(BV32_ZEXT64(local_id_x), 1) then 1 else 0))), BV1_ZEXT32((if BV64_SGE(BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n)), BV32_ZEXT64(local_id_x)) then 1 else 0))), BV1_ZEXT32((if BV32_ULE(BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n), 2) then 1 else 0))), BV1_ZEXT32((if BV32_UGE($n, BV32_ADD(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), 3)) then 1 else 0))), BV1_ZEXT32((if BV64_SGE($c0, BV64_ADD(BV64_ADD(BV64_MUL(2, BV32_ZEXT64(local_id_x)), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n))), 1)) then 1 else 0))), BV1_ZEXT32((if BV64_SGE(BV32_ZEXT64(BV32_ADD(BV32_MUL(4, $tsteps), BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n))), BV64_ADD($c0, 1)) then 1 else 0))), BV1_ZEXT32((if BV64_SGE(BV64_MUL(64, (if BV64_SLT(BV64_ADD(BV64_ADD(BV64_ADD(BV64_ADD(BV64_MUL(2, BV32_ZEXT64(local_id_x)), BV64_MUL(4, BV32_ZEXT64(local_id_y))), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n))), BV64_MUL(63, $c0)), 1), 0) then BV64_SUB(0, BV64_SDIV(BV64_SUB(BV64_ADD(BV64_SUB(0, BV64_ADD(BV64_ADD(BV64_ADD(BV64_ADD(BV64_MUL(2, BV32_ZEXT64(local_id_x)), BV64_MUL(4, BV32_ZEXT64(local_id_y))), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n))), BV64_MUL(63, $c0)), 1)), 64), 1), 64)) else BV64_SDIV(BV64_ADD(BV64_ADD(BV64_ADD(BV64_ADD(BV64_MUL(2, BV32_ZEXT64(local_id_x)), BV64_MUL(4, BV32_ZEXT64(local_id_y))), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n))), BV64_MUL(63, $c0)), 1), 64))), BV64_ADD(BV64_ADD(BV64_ADD(BV64_MUL(2, BV32_ZEXT64(local_id_x)), BV64_MUL(4, BV32_ZEXT64(local_id_y))), BV64_MUL(63, $c0)), 1)) then 1 else 0))), BV1_ZEXT32((if BV64_SGE(BV64_ADD(BV32_ZEXT64(local_id_x), 1), BV64_ADD(BV64_SREM(BV64_ADD(BV64_ADD(BV64_ADD(BV64_ADD(BV64_MUL(2, BV32_ZEXT64(local_id_x)), BV64_MUL(4, BV32_ZEXT64(local_id_y))), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n))), BV64_MUL(63, $c0)), 1), 64), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n)))) then 1 else 0))), BV1_ZEXT32((if BV64_SGE(BV64_ADD(BV32_ZEXT64(local_id_y), BV64_MUL(8192, BV64_SDIV(BV64_ADD(BV64_ADD(BV64_ADD(BV64_ADD(BV64_MUL(128, BV32_ZEXT64(group_id_y)), BV64_MUL(2, BV32_ZEXT64(local_id_x))), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n))), BV64_MUL(32767, $c0)), 125), 32768))), BV64_ADD(BV64_ADD(BV64_MUL(32, BV32_ZEXT64(group_id_y)), BV64_MUL(8176, $c0)), BV64_MUL(16, BV64_SDIV(BV64_ADD(BV64_ADD(BV64_ADD(BV64_ADD(BV64_MUL(2, BV32_ZEXT64(local_id_x)), BV64_MUL(4, BV32_ZEXT64(local_id_y))), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n))), BV64_MUL(63, $c0)), 1), 64)))) then 1 else 0)))), BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV1_ZEXT32((if BV64_SGE(BV64_ADD(BV64_MUL(32, BV32_ZEXT64(group_id_x)), BV32_ZEXT64(local_id_x)), 0) then 1 else 0)), BV1_ZEXT32((if BV64_SLE(BV64_ADD(BV64_MUL(32, BV32_ZEXT64(group_id_x)), BV32_ZEXT64(local_id_x)), 8191) then 1 else 0))), BV1_ZEXT32((if BV32_UGE($n, BV32_ADD(BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n), 3)) then 1 else 0))), BV1_ZEXT32((if BV32_UGE(BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n), 0) then 1 else 0))), BV1_ZEXT32((if BV64_SGE(BV64_ADD(BV64_ADD(BV32_ZEXT64(local_id_x), BV64_MUL(2, BV32_ZEXT64(local_id_y))), BV64_MUL(32, (if BV64_SLT(BV64_ADD(BV64_ADD(BV64_SUB(BV64_SUB(BV64_MUL(-2, BV32_ZEXT64(local_id_x)), BV64_MUL(4, BV32_ZEXT64(local_id_y))), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n))), $c0), 62), 0) then BV64_SUB(0, BV64_SDIV(BV64_SUB(BV64_ADD(BV64_SUB(0, BV64_ADD(BV64_ADD(BV64_SUB(BV64_SUB(BV64_MUL(-2, BV32_ZEXT64(local_id_x)), BV64_MUL(4, BV32_ZEXT64(local_id_y))), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n))), $c0), 62)), 64), 1), 64)) else BV64_SDIV(BV64_ADD(BV64_ADD(BV64_SUB(BV64_SUB(BV64_MUL(-2, BV32_ZEXT64(local_id_x)), BV64_MUL(4, BV32_ZEXT64(local_id_y))), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n))), $c0), 62), 64)))), BV32_ZEXT64(BV32_ADD(BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n), 1))) then 1 else 0))), BV1_ZEXT32((if BV64_SGE(BV32_ZEXT64(BV32_ADD(BV32_MUL(2, $tsteps), BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n))), BV64_ADD(BV64_ADD(BV64_ADD(BV32_ZEXT64(local_id_x), BV64_MUL(2, BV32_ZEXT64(local_id_y))), BV64_MUL(32, (if BV64_SLT(BV64_ADD(BV64_ADD(BV64_SUB(BV64_SUB(BV64_MUL(-2, BV32_ZEXT64(local_id_x)), BV64_MUL(4, BV32_ZEXT64(local_id_y))), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n))), $c0), 62), 0) then BV64_SUB(0, BV64_SDIV(BV64_SUB(BV64_ADD(BV64_SUB(0, BV64_ADD(BV64_ADD(BV64_SUB(BV64_SUB(BV64_MUL(-2, BV32_ZEXT64(local_id_x)), BV64_MUL(4, BV32_ZEXT64(local_id_y))), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n))), $c0), 62)), 64), 1), 64)) else BV64_SDIV(BV64_ADD(BV64_ADD(BV64_SUB(BV64_SUB(BV64_MUL(-2, BV32_ZEXT64(local_id_x)), BV64_MUL(4, BV32_ZEXT64(local_id_y))), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n))), $c0), 62), 64)))), 1)) then 1 else 0))), BV1_ZEXT32((if BV64_SGE(BV64_ADD(BV64_ADD(BV64_ADD(BV32_SEXT64($n), BV64_MUL(2, BV32_ZEXT64(local_id_x))), BV64_MUL(4, BV32_ZEXT64(local_id_y))), BV64_MUL(64, (if BV64_SLT(BV64_ADD(BV64_ADD(BV64_SUB(BV64_SUB(BV64_MUL(-2, BV32_ZEXT64(local_id_x)), BV64_MUL(4, BV32_ZEXT64(local_id_y))), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n))), $c0), 62), 0) then BV64_SUB(0, BV64_SDIV(BV64_SUB(BV64_ADD(BV64_SUB(0, BV64_ADD(BV64_ADD(BV64_SUB(BV64_SUB(BV64_MUL(-2, BV32_ZEXT64(local_id_x)), BV64_MUL(4, BV32_ZEXT64(local_id_y))), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n))), $c0), 62)), 64), 1), 64)) else BV64_SDIV(BV64_ADD(BV64_ADD(BV64_SUB(BV64_SUB(BV64_MUL(-2, BV32_ZEXT64(local_id_x)), BV64_MUL(4, BV32_ZEXT64(local_id_y))), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n))), $c0), 62), 64)))), BV64_ADD($c0, 2)) then 1 else 0))), BV1_ZEXT32((if BV64_SGE($c0, BV64_ADD(BV64_ADD(BV64_ADD(BV64_MUL(2, BV32_ZEXT64(local_id_x)), BV64_MUL(4, BV32_ZEXT64(local_id_y))), BV64_MUL(64, (if BV64_SLT(BV64_ADD(BV64_ADD(BV64_SUB(BV64_SUB(BV64_MUL(-2, BV32_ZEXT64(local_id_x)), BV64_MUL(4, BV32_ZEXT64(local_id_y))), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n))), $c0), 62), 0) then BV64_SUB(0, BV64_SDIV(BV64_SUB(BV64_ADD(BV64_SUB(0, BV64_ADD(BV64_ADD(BV64_SUB(BV64_SUB(BV64_MUL(-2, BV32_ZEXT64(local_id_x)), BV64_MUL(4, BV32_ZEXT64(local_id_y))), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n))), $c0), 62)), 64), 1), 64)) else BV64_SDIV(BV64_ADD(BV64_ADD(BV64_SUB(BV64_SUB(BV64_MUL(-2, BV32_ZEXT64(local_id_x)), BV64_MUL(4, BV32_ZEXT64(local_id_y))), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n))), $c0), 62), 64)))), 1)) then 1 else 0))), BV1_ZEXT32((if BV64_SGE($c0, BV64_ADD(BV64_ADD(BV64_ADD(BV64_MUL(2, BV32_ZEXT64(local_id_x)), BV64_MUL(4, BV32_ZEXT64(local_id_y))), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n))), BV64_MUL(64, (if BV64_SLT(BV64_ADD(BV64_ADD(BV64_SUB(BV64_SUB(BV64_MUL(-2, BV32_ZEXT64(local_id_x)), BV64_MUL(4, BV32_ZEXT64(local_id_y))), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n))), $c0), 62), 0) then BV64_SUB(0, BV64_SDIV(BV64_SUB(BV64_ADD(BV64_SUB(0, BV64_ADD(BV64_ADD(BV64_SUB(BV64_SUB(BV64_MUL(-2, BV32_ZEXT64(local_id_x)), BV64_MUL(4, BV32_ZEXT64(local_id_y))), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n))), $c0), 62)), 64), 1), 64)) else BV64_SDIV(BV64_ADD(BV64_ADD(BV64_SUB(BV64_SUB(BV64_MUL(-2, BV32_ZEXT64(local_id_x)), BV64_MUL(4, BV32_ZEXT64(local_id_y))), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n))), $c0), 62), 64))))) then 1 else 0))), BV1_ZEXT32((if BV64_SGE(BV64_SREM(BV64_ADD(BV64_ADD(BV64_SUB(BV64_ADD(BV64_ADD(BV64_MUL(-64, BV32_ZEXT64(group_id_y)), BV32_ZEXT64(local_id_x)), BV64_MUL(2, BV32_ZEXT64(local_id_y))), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n))), BV64_MUL(32, BV64_SDIV(BV64_ADD(BV64_ADD(BV64_SUB(BV64_SUB(BV64_MUL(-2, BV32_ZEXT64(local_id_x)), BV64_MUL(4, BV32_ZEXT64(local_id_y))), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n))), $c0), 126), 64))), 16288), 16384), 16321) then 1 else 0))), BV1_ZEXT32((if BV64_SREM(BV64_ADD(BV64_SUB(BV64_ADD(BV64_MUL(32, BV32_ZEXT64(group_id_x)), BV32_ZEXT64(local_id_x)), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n))), 8191), 8192) == 0 then 1 else 0)))), BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV1_ZEXT32((if BV64_SLE(BV32_ZEXT64(local_id_x), 31) then 1 else 0)), BV1_ZEXT32((if BV64_SGE(BV32_ZEXT64(local_id_x), -8160) then 1 else 0))), BV1_ZEXT32((if BV32_UGE(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), 2) then 1 else 0))), BV1_ZEXT32((if BV32_UGE($n, BV32_ADD(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), 1)) then 1 else 0))), BV1_ZEXT32((if BV64_SGE(BV32_SEXT64($n), BV64_ADD(BV64_ADD(BV32_ZEXT64(local_id_x), BV64_MUL(32, (if BV64_SLT(BV64_ADD(BV64_ADD(BV64_SUB(0, BV32_ZEXT64(local_id_x)), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n))), 30), 0) then BV64_SUB(0, BV64_SDIV(BV64_SUB(BV64_ADD(BV64_SUB(0, BV64_ADD(BV64_ADD(BV64_SUB(0, BV32_ZEXT64(local_id_x)), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n))), 30)), 32), 1), 32)) else BV64_SDIV(BV64_ADD(BV64_ADD(BV64_SUB(0, BV32_ZEXT64(local_id_x)), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n))), 30), 32)))), 2)) then 1 else 0))), BV1_ZEXT32((if BV64_SGE(BV64_ADD(BV32_ZEXT64(local_id_x), BV64_MUL(32, (if BV64_SLT(BV64_ADD(BV64_ADD(BV64_SUB(0, BV32_ZEXT64(local_id_x)), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n))), 30), 0) then BV64_SUB(0, BV64_SDIV(BV64_SUB(BV64_ADD(BV64_SUB(0, BV64_ADD(BV64_ADD(BV64_SUB(0, BV32_ZEXT64(local_id_x)), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n))), 30)), 32), 1), 32)) else BV64_SDIV(BV64_ADD(BV64_ADD(BV64_SUB(0, BV32_ZEXT64(local_id_x)), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n))), 30), 32)))), 1) then 1 else 0))), BV1_ZEXT32((if BV64_SGE(BV64_SREM(BV64_ADD(BV64_ADD(BV64_SUB(0, BV32_ZEXT64(local_id_x)), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n))), 30), 32), 30) then 1 else 0))), BV1_ZEXT32((if BV64_SGE(BV32_ZEXT64(BV32_ADD(BV32_ADD(BV32_ADD(BV32_MUL(4, $tsteps), BV32_MUL(2, BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n))), BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n)), 55)), BV64_ADD(BV64_MUL(2, BV64_SREM(BV64_ADD(BV64_ADD(BV64_SUB(0, BV32_ZEXT64(local_id_x)), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n))), 30), 32)), $c0)) then 1 else 0))), BV1_ZEXT32((if BV64_SGE(BV64_ADD(BV64_MUL(2, BV64_SREM(BV64_ADD(BV64_ADD(BV64_SUB(0, BV32_ZEXT64(local_id_x)), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n))), 30), 32)), $c0), BV32_ZEXT64(BV32_ADD(BV32_ADD(BV32_MUL(2, BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n)), BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n)), 59))) then 1 else 0))), BV1_ZEXT32((if BV64_SGE(BV64_ADD(BV64_ADD(BV64_ADD(BV64_MUL(2, BV64_SREM(BV64_ADD(BV64_ADD(BV64_SUB(0, BV32_ZEXT64(local_id_x)), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n))), 30), 32)), BV64_SREM(BV64_ADD(BV64_ADD(BV64_SUB(BV64_SUB(BV64_SUB(BV64_MUL(2, BV64_SREM(BV64_ADD(BV64_ADD(BV64_SUB(0, BV32_ZEXT64(local_id_x)), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n))), 30), 32)), BV64_MUL(128, BV32_ZEXT64(group_id_y))), BV32_ZEXT64(BV32_MUL(2, BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n)))), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n))), $c0), 32584), 32768)), BV64_MUL(2, BV32_ZEXT64(local_id_x))), BV64_MUL(64, BV64_SDIV(BV64_ADD(BV64_ADD(BV64_SUB(0, BV32_ZEXT64(local_id_x)), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n))), 30), 32))), BV32_ZEXT64(BV32_ADD(BV32_MUL(2, BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n)), 32703))) then 1 else 0))), BV1_ZEXT32((if BV64_ADD(BV32_ZEXT64(group_id_x), BV64_MUL(256, BV64_SDIV(BV64_ADD(BV64_ADD(BV64_SUB(BV64_SUB(0, BV64_SREM(BV64_ADD(BV64_ADD(BV64_SUB(0, BV32_ZEXT64(local_id_x)), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n))), 30), 32)), BV64_MUL(32, BV32_ZEXT64(group_id_x))), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n))), 8190), 8192))) == BV64_SDIV(BV64_ADD(BV64_ADD(BV64_SUB(0, BV32_ZEXT64(local_id_x)), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n))), 30), 32) then 1 else 0))), BV1_ZEXT32((if BV64_SREM(BV64_ADD(BV64_SUB(BV64_ADD(BV64_ADD(BV64_MUL(2, BV32_ZEXT64(local_id_x)), BV64_MUL(4, BV32_ZEXT64(local_id_y))), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n))), $c0), 63), 64) == 0 then 1 else 0)))), BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV1_ZEXT32((if BV32_UGE(BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n), 3) then 1 else 0)), BV1_ZEXT32((if BV32_UGE($n, BV32_ADD(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), 3)) then 1 else 0))), BV1_ZEXT32((if BV64_SGE(BV32_ZEXT64(BV32_ADD(BV32_ADD(BV32_MUL(2, $n), BV32_MUL(4, $tsteps)), BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n))), BV64_ADD($c0, 7)) then 1 else 0))), BV1_ZEXT32((if BV64_SGE(BV32_ZEXT64(BV32_ADD(BV32_ADD(BV32_MUL(4, $tsteps), BV32_MUL(2, BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n))), BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n))), BV64_ADD($c0, 3)) then 1 else 0))), BV1_ZEXT32((if BV64_SGE($c0, BV32_ZEXT64(BV32_ADD(BV32_ADD(BV32_MUL(2, BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n)), BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n)), 1))) then 1 else 0))), BV1_ZEXT32((if BV64_SGE(BV64_ADD($c0, 3), BV32_ZEXT64(BV32_ADD(BV32_MUL(4, $tsteps), BV32_MUL(2, BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n))))) then 1 else 0))), BV1_ZEXT32((if BV64_SGE(BV64_ADD($c0, 4), BV32_ZEXT64(BV32_ADD(BV32_ADD(BV32_MUL(4, $tsteps), BV32_MUL(2, BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n))), BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n)))) then 1 else 0))), BV1_ZEXT32((if BV64_SGE(BV64_ADD(BV32_ZEXT64(BV32_ADD($tsteps, BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n))), BV64_MUL(8192, BV64_SDIV(BV64_ADD(BV64_ADD(BV64_ADD(BV64_SUB(BV64_ADD(BV64_ADD(BV64_ADD(BV64_MUL(128, BV32_ZEXT64(group_id_x)), BV64_MUL(128, BV32_ZEXT64(group_id_y))), BV64_MUL(4, BV32_ZEXT64(local_id_x))), BV64_MUL(32768, BV32_ZEXT64(local_id_y))), BV32_ZEXT64(BV32_MUL(2, BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n)))), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n))), BV64_MUL(32767, $c0)), 32893), 32768))), BV64_ADD(BV64_ADD(BV64_ADD(BV64_ADD(BV64_ADD(BV64_MUL(32, BV32_ZEXT64(group_id_x)), BV64_MUL(32, BV32_ZEXT64(group_id_y))), BV32_ZEXT64(local_id_x)), BV64_MUL(8192, BV32_ZEXT64(local_id_y))), BV64_MUL(8192, $c0)), 8193)) then 1 else 0))), BV1_ZEXT32((if BV64_SREM(BV64_ADD(BV64_SUB(BV32_SEXT64($tsteps), BV32_ZEXT64(local_id_y)), 15), 16) == 0 then 1 else 0))), BV1_ZEXT32((if BV64_SREM(BV64_SUB(BV64_ADD(BV64_MUL(32, BV32_ZEXT64(group_id_x)), BV32_ZEXT64(local_id_x)), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n))), 8192) == 0 then 1 else 0)))), BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV1_ZEXT32((if BV64_SGE(BV64_ADD(BV64_MUL(32, BV32_ZEXT64(group_id_x)), BV32_ZEXT64(local_id_x)), 0) then 1 else 0)), BV1_ZEXT32((if BV64_SLE(BV64_ADD(BV64_MUL(32, BV32_ZEXT64(group_id_x)), BV32_ZEXT64(local_id_x)), 8191) then 1 else 0))), BV1_ZEXT32((if BV32_UGE(BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n), 2) then 1 else 0))), BV1_ZEXT32((if BV32_UGE($n, BV32_ADD(BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n), 1)) then 1 else 0))), BV1_ZEXT32((if BV32_UGE($n, BV32_ADD(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), 2)) then 1 else 0))), BV1_ZEXT32((if BV32_UGE(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), 1) then 1 else 0))), BV1_ZEXT32((if BV64_SGE(BV32_ZEXT64(BV32_ADD(BV32_ADD(BV32_MUL(4, $tsteps), BV32_MUL(2, BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n))), BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n))), BV64_ADD($c0, 6)) then 1 else 0))), BV1_ZEXT32((if BV64_SGE(BV64_ADD($c0, 2), BV32_ZEXT64(BV32_ADD(BV32_MUL(2, BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n)), BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n)))) then 1 else 0))), BV1_ZEXT32((if BV64_SGE(BV64_SREM(BV64_ADD(BV64_ADD(BV32_ZEXT64(BV32_SUB(BV32_MUL(-2, BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n)), BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n))), $c0), 2), 32768), BV64_MUL(128, BV32_ZEXT64(group_id_y))) then 1 else 0))), BV1_ZEXT32((if BV64_SGE(BV64_ADD(BV64_MUL(128, BV32_ZEXT64(group_id_y)), 124), BV64_SREM(BV64_ADD(BV64_ADD(BV32_ZEXT64(BV32_SUB(BV32_MUL(-2, BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n)), BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n))), $c0), 2), 32768)) then 1 else 0))), BV1_ZEXT32((if BV64_SREM(BV64_ADD(BV64_SUB(BV64_ADD(BV64_MUL(32, BV32_ZEXT64(group_id_x)), BV32_ZEXT64(local_id_x)), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n))), 1), 8192) == 0 then 1 else 0))), BV1_ZEXT32((if BV64_SREM(BV64_SUB(BV64_ADD(BV64_ADD(BV64_MUL(2, BV32_ZEXT64(local_id_x)), BV64_MUL(4, BV32_ZEXT64(local_id_y))), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n))), $c0), 64) == 0 then 1 else 0)))), BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV1_ZEXT32((if BV64_SGE(BV64_ADD(BV64_MUL(32, BV32_ZEXT64(group_id_x)), BV32_ZEXT64(local_id_x)), 0) then 1 else 0)), BV1_ZEXT32((if BV64_SLE(BV64_ADD(BV64_MUL(32, BV32_ZEXT64(group_id_x)), BV32_ZEXT64(local_id_x)), 8191) then 1 else 0))), BV1_ZEXT32((if BV32_UGE($n, BV32_ADD(BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n), 3)) then 1 else 0))), BV1_ZEXT32((if BV32_UGE(BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n), 0) then 1 else 0))), BV1_ZEXT32((if BV32_UGE(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), 2) then 1 else 0))), BV1_ZEXT32((if BV32_UGE($n, BV32_ADD(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), 1)) then 1 else 0))), BV1_ZEXT32((if BV64_SGE(BV32_ZEXT64(BV32_ADD(BV32_ADD(BV32_MUL(4, $tsteps), BV32_MUL(2, BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n))), BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n))), BV64_ADD($c0, 3)) then 1 else 0))), BV1_ZEXT32((if BV64_SGE($c0, BV32_ZEXT64(BV32_ADD(BV32_ADD(BV32_MUL(2, BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n)), BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n)), 1))) then 1 else 0))), BV1_ZEXT32((if BV64_SGE(BV64_SREM(BV64_SUB(BV64_ADD(BV32_ZEXT64(BV32_SUB(BV32_MUL(-2, BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n)), BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n))), $c0), 1), 32768), BV64_MUL(128, BV32_ZEXT64(group_id_y))) then 1 else 0))), BV1_ZEXT32((if BV64_SGE(BV64_ADD(BV64_MUL(128, BV32_ZEXT64(group_id_y)), 124), BV64_SREM(BV64_SUB(BV64_ADD(BV32_ZEXT64(BV32_SUB(BV32_MUL(-2, BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n)), BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n))), $c0), 1), 32768)) then 1 else 0))), BV1_ZEXT32((if BV64_SREM(BV64_ADD(BV64_SUB(BV64_ADD(BV64_MUL(32, BV32_ZEXT64(group_id_x)), BV32_ZEXT64(local_id_x)), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n))), 8191), 8192) == 0 then 1 else 0))), BV1_ZEXT32((if BV64_SREM(BV64_ADD(BV64_SUB(BV64_ADD(BV64_ADD(BV64_MUL(2, BV32_ZEXT64(local_id_x)), BV64_MUL(4, BV32_ZEXT64(local_id_y))), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n))), $c0), 63), 64) == 0 then 1 else 0)))), BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV1_ZEXT32((if BV32_UGE(BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n), 3) then 1 else 0)), BV1_ZEXT32((if BV32_UGE($n, BV32_ADD(BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n), 2)) then 1 else 0))), BV1_ZEXT32((if BV32_ADD(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), 2) == $n then 1 else 0))), BV1_ZEXT32((if BV64_SGE(BV32_ZEXT64(BV32_ADD(BV32_ADD($n, BV32_MUL(4, $tsteps)), BV32_MUL(2, BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n)))), BV64_ADD($c0, 10)) then 1 else 0))), BV1_ZEXT32((if BV64_SGE(BV64_ADD($c0, 2), BV32_ZEXT64(BV32_ADD($n, BV32_MUL(2, BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n))))) then 1 else 0))), BV1_ZEXT32((if BV64_SLE(BV64_SREM(BV64_SUB(BV64_ADD(BV64_ADD(BV64_ADD(BV64_ADD(BV64_ADD(BV64_ADD(BV32_SEXT64(BV32_MUL(32769, $n)), BV64_MUL(128, BV32_ZEXT64(group_id_x))), BV64_MUL(128, BV32_ZEXT64(group_id_y))), BV64_MUL(32772, BV32_ZEXT64(local_id_x))), BV64_MUL(32768, BV32_ZEXT64(local_id_y))), BV32_ZEXT64(BV32_MUL(32766, BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n)))), BV64_MUL(32767, $c0)), 65413), 32768), 125) then 1 else 0))), BV1_ZEXT32((if BV64_SREM(BV64_SUB(BV64_ADD(BV64_MUL(32, BV32_ZEXT64(group_id_x)), BV32_ZEXT64(local_id_x)), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n))), 8192) == 0 then 1 else 0))), BV1_ZEXT32((if BV64_SREM(BV64_SUB(BV64_SUB(BV64_ADD(BV64_ADD(BV32_SEXT64($n), BV64_MUL(2, BV32_ZEXT64(local_id_x))), BV64_MUL(4, BV32_ZEXT64(local_id_y))), $c0), 2), 64) == 0 then 1 else 0)))), BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV1_ZEXT32((if BV32_SGE($tsteps, 2) then 1 else 0)), BV1_ZEXT32((if BV32_ZEXT64(group_id_y) == 0 then 1 else 0))), BV1_ZEXT32((if BV32_ZEXT64(local_id_y) == 0 then 1 else 0))), BV1_ZEXT32((if BV32_UGE(BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n), 3) then 1 else 0))), BV1_ZEXT32((if BV32_UGE($n, BV32_ADD(BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n), 2)) then 1 else 0))), BV1_ZEXT32((if BV32_UGE($n, BV32_ADD(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), 3)) then 1 else 0))), BV1_ZEXT32((if BV32_UGE(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), 1) then 1 else 0))), BV1_ZEXT32((if $c0 == BV32_ZEXT64(BV32_ADD(BV32_MUL(2, BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n)), BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n))) then 1 else 0))), BV1_ZEXT32((if BV64_SREM(BV64_SUB(BV64_ADD(BV64_MUL(32, BV32_ZEXT64(group_id_x)), BV32_ZEXT64(local_id_x)), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n))), 8192) == 0 then 1 else 0)))), BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV1_ZEXT32((if BV32_ZEXT64(group_id_x) == 0 then 1 else 0)), BV1_ZEXT32((if BV64_SGE(BV32_SEXT64($n), BV64_ADD(BV32_ZEXT64(local_id_x), 2)) then 1 else 0))), BV1_ZEXT32((if BV64_SGE(BV32_ZEXT64(local_id_x), 1) then 1 else 0))), BV1_ZEXT32((if BV64_SLE(BV32_ZEXT64(local_id_x), 2) then 1 else 0))), BV1_ZEXT32((if BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n)) == BV32_ZEXT64(local_id_x) then 1 else 0))), BV1_ZEXT32((if BV32_ADD(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), 2) == $n then 1 else 0))), BV1_ZEXT32((if BV64_SGE(BV32_SEXT64(BV32_ADD($n, BV32_MUL(4, $tsteps))), BV64_ADD($c0, 4)) then 1 else 0))), BV1_ZEXT32((if BV64_SGE(BV64_ADD($c0, 2), BV64_ADD(BV32_SEXT64($n), BV64_MUL(2, BV32_ZEXT64(local_id_x)))) then 1 else 0))), BV1_ZEXT32((if BV64_SLE(BV64_SREM(BV64_ADD(BV64_ADD(BV64_ADD(BV64_ADD(BV32_SEXT64($n), BV64_MUL(128, BV32_ZEXT64(group_id_y))), BV64_MUL(2, BV32_ZEXT64(local_id_x))), BV64_MUL(32767, $c0)), 123), 32768), 125) then 1 else 0))), BV1_ZEXT32((if BV64_SREM(BV64_SUB(BV64_SUB(BV64_ADD(BV64_ADD(BV32_SEXT64($n), BV64_MUL(2, BV32_ZEXT64(local_id_x))), BV64_MUL(4, BV32_ZEXT64(local_id_y))), $c0), 2), 64) == 0 then 1 else 0)))), BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV1_ZEXT32((if BV32_ZEXT64(group_id_x) == 0 then 1 else 0)), BV1_ZEXT32((if BV32_ZEXT64(local_id_x) == 2 then 1 else 0))), BV1_ZEXT32((if BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n) == 2 then 1 else 0))), BV1_ZEXT32((if BV32_UGE($n, BV32_ADD(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), 3)) then 1 else 0))), BV1_ZEXT32((if BV64_SGE($c0, BV32_ZEXT64(BV32_ADD(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), 5))) then 1 else 0))), BV1_ZEXT32((if BV64_SGE($c0, BV32_SEXT64(BV32_ADD(BV32_MUL(4, $tsteps), 1))) then 1 else 0))), BV1_ZEXT32((if BV64_SGE($c0, BV32_ZEXT64(BV32_ADD(BV32_MUL(4, $tsteps), BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n)))) then 1 else 0))), BV1_ZEXT32((if BV64_SGE(BV32_ZEXT64(BV32_ADD(BV32_ADD(BV32_MUL(4, $tsteps), BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n)), 1)), $c0) then 1 else 0))), BV1_ZEXT32((if BV64_SGE(BV64_ADD(BV32_SEXT64($tsteps), BV64_MUL(8192, BV64_SDIV(BV64_ADD(BV64_ADD(BV64_ADD(BV64_MUL(128, BV32_ZEXT64(group_id_y)), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n))), BV64_MUL(32767, $c0)), 129), 32768))), BV64_ADD(BV64_ADD(BV64_MUL(32, BV32_ZEXT64(group_id_y)), BV64_MUL(8192, $c0)), 1)) then 1 else 0))), BV1_ZEXT32((if BV64_SREM(BV64_ADD(BV64_SUB(BV32_SEXT64($tsteps), BV32_ZEXT64(local_id_y)), 15), 16) == 0 then 1 else 0)))), BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV1_ZEXT32((if BV32_UGE(BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n), 3) then 1 else 0)), BV1_ZEXT32((if BV32_ADD(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), 2) == $n then 1 else 0))), BV1_ZEXT32((if BV64_ADD($c0, 6) == BV32_ZEXT64(BV32_ADD(BV32_ADD($n, BV32_MUL(4, $tsteps)), BV32_MUL(2, BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n)))) then 1 else 0))), BV1_ZEXT32((if BV64_SLE(BV64_SREM(BV64_ADD(BV64_ADD(BV64_ADD(BV64_ADD(BV64_ADD(BV64_ADD(BV32_SEXT64(BV32_MUL(8191, $tsteps)), BV64_MUL(32, BV32_ZEXT64(group_id_x))), BV64_MUL(32, BV32_ZEXT64(group_id_y))), BV64_MUL(8193, BV32_ZEXT64(local_id_x))), BV64_MUL(8192, BV32_ZEXT64(local_id_y))), BV32_ZEXT64(BV32_MUL(8191, BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n)))), 8224), 8192), 31) then 1 else 0))), BV1_ZEXT32((if BV64_SREM(BV64_SUB(BV64_ADD(BV64_MUL(32, BV32_ZEXT64(group_id_x)), BV32_ZEXT64(local_id_x)), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n))), 8192) == 0 then 1 else 0))), BV1_ZEXT32((if BV64_SREM(BV64_ADD(BV64_ADD(BV32_SEXT64(BV32_SUB(0, $tsteps)), BV32_ZEXT64(local_id_y)), 1), 16) == 0 then 1 else 0)))), BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV1_ZEXT32((if $tsteps == 1 then 1 else 0)), BV1_ZEXT32((if BV32_ZEXT64(group_id_y) == 0 then 1 else 0))), BV1_ZEXT32((if BV32_ZEXT64(local_id_y) == 0 then 1 else 0))), BV1_ZEXT32((if BV32_UGE(BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n), 3) then 1 else 0))), BV1_ZEXT32((if BV32_UGE($n, BV32_ADD(BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n), 2)) then 1 else 0))), BV1_ZEXT32((if BV32_UGE($n, BV32_ADD(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), 3)) then 1 else 0))), BV1_ZEXT32((if BV32_UGE(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), 1) then 1 else 0))), BV1_ZEXT32((if $c0 == BV32_ZEXT64(BV32_ADD(BV32_MUL(2, BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n)), BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n))) then 1 else 0))), BV1_ZEXT32((if BV64_SREM(BV64_SUB(BV64_ADD(BV64_MUL(32, BV32_ZEXT64(group_id_x)), BV32_ZEXT64(local_id_x)), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n))), 8192) == 0 then 1 else 0)))), BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV1_ZEXT32((if BV32_SGE($tsteps, 2) then 1 else 0)), BV1_ZEXT32((if BV32_ZEXT64(group_id_x) == 0 then 1 else 0))), BV1_ZEXT32((if BV32_ZEXT64(group_id_y) == 0 then 1 else 0))), BV1_ZEXT32((if BV32_ZEXT64(local_id_x) == 2 then 1 else 0))), BV1_ZEXT32((if BV32_ZEXT64(local_id_y) == 0 then 1 else 0))), BV1_ZEXT32((if BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n) == 2 then 1 else 0))), BV1_ZEXT32((if BV32_UGE($n, BV32_ADD(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), 3)) then 1 else 0))), BV1_ZEXT32((if BV32_UGE(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), 1) then 1 else 0))), BV1_ZEXT32((if $c0 == BV32_ZEXT64(BV32_ADD(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), 4)) then 1 else 0)))), BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV1_ZEXT32((if BV32_ZEXT64(group_id_x) == 0 then 1 else 0)), BV1_ZEXT32((if BV32_ZEXT64(group_id_y) == 0 then 1 else 0))), BV1_ZEXT32((if BV32_ZEXT64(local_id_x) == 1 then 1 else 0))), BV1_ZEXT32((if BV32_ZEXT64(local_id_y) == 0 then 1 else 0))), BV1_ZEXT32((if BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n) == 1 then 1 else 0))), BV1_ZEXT32((if BV32_UGE($n, BV32_ADD(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), 3)) then 1 else 0))), BV1_ZEXT32((if $c0 == BV32_ZEXT64(BV32_ADD(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), 2)) then 1 else 0)))), BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV1_ZEXT32((if BV32_ZEXT64(group_id_x) == 0 then 1 else 0)), BV1_ZEXT32((if BV32_ZEXT64(local_id_x) == 2 then 1 else 0))), BV1_ZEXT32((if BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n) == 2 then 1 else 0))), BV1_ZEXT32((if BV32_ADD(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), 2) == $n then 1 else 0))), BV1_ZEXT32((if BV64_ADD($c0, 2) == BV32_SEXT64(BV32_ADD($n, BV32_MUL(4, $tsteps))) then 1 else 0))), BV1_ZEXT32((if BV64_SGE(BV64_SREM(BV64_ADD(BV64_SUB(BV32_SEXT64($tsteps), BV64_MUL(32, BV32_ZEXT64(group_id_y))), 8159), 8192), 8160) then 1 else 0))), BV1_ZEXT32((if BV64_SREM(BV64_ADD(BV64_ADD(BV32_SEXT64(BV32_SUB(0, $tsteps)), BV32_ZEXT64(local_id_y)), 1), 16) == 0 then 1 else 0)))), BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV1_ZEXT32((if $tsteps == 1 then 1 else 0)), BV1_ZEXT32((if BV32_ZEXT64(group_id_x) == 0 then 1 else 0))), BV1_ZEXT32((if BV32_ZEXT64(group_id_y) == 0 then 1 else 0))), BV1_ZEXT32((if BV32_ZEXT64(local_id_x) == 2 then 1 else 0))), BV1_ZEXT32((if BV32_ZEXT64(local_id_y) == 0 then 1 else 0))), BV1_ZEXT32((if BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n) == 2 then 1 else 0))), BV1_ZEXT32((if BV32_UGE($n, BV32_ADD(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), 3)) then 1 else 0))), BV1_ZEXT32((if BV32_UGE(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), 1) then 1 else 0))), BV1_ZEXT32((if $c0 == BV32_ZEXT64(BV32_ADD(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), 4)) then 1 else 0)))) != 0) then 1 else 0) != 0;
{
  var $0:int;
  var $1:int;
  var $2:int;
  var $c1.0:int;
  var $3:int;
  var $4:int;
  var $5:int;
  var $6:int;
  var $7:int;
  var $8:int;
  var $9:int;
  var $10:int;
  var $11:int;
  var $12:int;
  var $c2.0:int;
  var $13:int;
  var $14:int;
  var $15:int;
  var $16:int;
  var $17:int;
  var $18:int;
  var $c4.0:int;
  var $19:int;
  var $20:int;
  var $21:int;
  var v0:int;
  var v1:int;
  var v2:int;
  var v3:int;
  var v4:bool;
  var v5:bool;
  var v6:bool;
  var v7:bool;
  var v8:bool;
  var v9:bool;
  var v10:bool;
  var v11:bool;
  var v12:bool;
  var v13:bool;
  var v14:bool;
  var v15:bool;
  var v16:bool;
  var v17:bool;
  var v18:bool;
  var v19:bool;
  var v20:bool;
  var v21:bool;
  var v22:bool;
  var v23:bool;
  var v24:bool;
  var v25:bool;
  var v26:bool;
  var v27:bool;
  var v28:bool;
  var v29:bool;
  var v30:bool;
  var v31:int;
  var v32:int;
  var v33:int;
  var v34:int;
  var v35:int;
  var v36:int;
  var v37:int;
  var v38:int;
  var v39:int;
$0:
  assert {:block_sourceloc} {:sourceloc_num 6} true;
  v0 := BV32_ZEXT64(group_id_x);
  v1 := BV32_ZEXT64(group_id_y);
  v2 := BV32_ZEXT64(local_id_x);
  v3 := BV32_ZEXT64(local_id_y);
  v4 := BV64_SLT(BV64_SUB(BV64_ADD(BV64_SUB(BV32_SEXT64(BV32_SUB(BV32_SUB(0, $n), BV32_MUL(4, $tsteps))), BV64_MUL(64, v0)), $c0), 57), 0);
  goto $truebb, $falsebb;
$1:
  assert {:block_sourceloc} {:sourceloc_num 7} true;
  $0 := BV64_SUB(0, BV64_SDIV(BV64_SUB(BV64_ADD(BV64_SUB(0, BV64_SUB(BV64_ADD(BV64_SUB(BV32_SEXT64(BV32_SUB(BV32_SUB(0, $n), BV32_MUL(4, $tsteps))), BV64_MUL(64, v0)), $c0), 57)), 16384), 1), 16384));
  goto $3;
$2:
  assert {:block_sourceloc} {:sourceloc_num 8} true;
  $0 := BV64_SDIV(BV64_SUB(BV64_ADD(BV64_SUB(BV32_SEXT64(BV32_SUB(BV32_SUB(0, $n), BV32_MUL(4, $tsteps))), BV64_MUL(64, v0)), $c0), 57), 16384);
  goto $3;
$3:
  assert {:block_sourceloc} {:sourceloc_num 9} true;
  v5 := BV64_SGT(BV64_MUL(32, v0), BV64_ADD(BV64_ADD(BV64_MUL(32, v0), BV64_MUL(8192, $0)), 8192));
  goto $truebb0, $falsebb0;
$4:
  assert {:block_sourceloc} {:sourceloc_num 10} true;
  $1 := BV64_MUL(32, v0);
  goto $9;
$5:
  assert {:block_sourceloc} {:sourceloc_num 11} true;
  v6 := BV64_SLT(BV64_SUB(BV64_ADD(BV64_SUB(BV32_SEXT64(BV32_SUB(BV32_SUB(0, $n), BV32_MUL(4, $tsteps))), BV64_MUL(64, v0)), $c0), 57), 0);
  goto $truebb1, $falsebb1;
$6:
  assert {:block_sourceloc} {:sourceloc_num 12} true;
  $2 := BV64_SUB(0, BV64_SDIV(BV64_SUB(BV64_ADD(BV64_SUB(0, BV64_SUB(BV64_ADD(BV64_SUB(BV32_SEXT64(BV32_SUB(BV32_SUB(0, $n), BV32_MUL(4, $tsteps))), BV64_MUL(64, v0)), $c0), 57)), 16384), 1), 16384));
  goto $8;
$7:
  assert {:block_sourceloc} {:sourceloc_num 13} true;
  $2 := BV64_SDIV(BV64_SUB(BV64_ADD(BV64_SUB(BV32_SEXT64(BV32_SUB(BV32_SUB(0, $n), BV32_MUL(4, $tsteps))), BV64_MUL(64, v0)), $c0), 57), 16384);
  goto $8;
$8:
  assert {:block_sourceloc} {:sourceloc_num 14} true;
  $1 := BV64_ADD(BV64_ADD(BV64_MUL(32, v0), BV64_MUL(8192, $2)), 8192);
  goto $9;
$9:
  assert {:block_sourceloc} {:sourceloc_num 15} true;
  $c1.0 := $1;
  goto $10;
$10:
  assert {:block_sourceloc} {:sourceloc_num 16} true;
  v7 := BV64_SLT(BV32_SEXT64(BV32_SUB($n, 1)), BV64_SDIV(BV64_ADD($c0, 1), 2));
  goto $truebb2, $falsebb2;
$11:
  assert {:block_sourceloc} {:sourceloc_num 17} true;
  $3 := BV32_SEXT64(BV32_SUB($n, 1));
  goto $13;
$12:
  assert {:block_sourceloc} {:sourceloc_num 18} true;
  $3 := BV64_SDIV(BV64_ADD($c0, 1), 2);
  goto $13;
$13:
  assert {:block_sourceloc} {:sourceloc_num 19} true;
  v8 := BV64_SLT($c1.0, $3);
  goto $truebb3, $falsebb3;
$14:
  assert {:block_sourceloc} {:sourceloc_num 20} true;
  v9 := BV64_SGE(BV32_SEXT64($n), BV64_ADD(BV64_ADD(v2, $c1.0), 2));
  goto $truebb4, $falsebb4;
$15:
  assert {:block_sourceloc} {:sourceloc_num 21} true;
  v10 := BV64_SGE(BV64_ADD(v2, $c1.0), 1);
  goto $truebb5, $falsebb5;
$16:
  assert {:block_sourceloc} {:sourceloc_num 22} true;
  v11 := BV64_SLT(BV64_SUB(BV64_ADD(BV64_SUB(BV32_SEXT64(BV32_MUL(-3, $n)), BV64_MUL(128, v1)), $c0), 119), 0);
  goto $truebb6, $falsebb6;
$17:
  assert {:block_sourceloc} {:sourceloc_num 23} true;
  $4 := BV64_SUB(0, BV64_SDIV(BV64_SUB(BV64_ADD(BV64_SUB(0, BV64_SUB(BV64_ADD(BV64_SUB(BV32_SEXT64(BV32_MUL(-3, $n)), BV64_MUL(128, v1)), $c0), 119)), 32768), 1), 32768));
  goto $19;
$18:
  assert {:block_sourceloc} {:sourceloc_num 24} true;
  $4 := BV64_SDIV(BV64_SUB(BV64_ADD(BV64_SUB(BV32_SEXT64(BV32_MUL(-3, $n)), BV64_MUL(128, v1)), $c0), 119), 32768);
  goto $19;
$19:
  assert {:block_sourceloc} {:sourceloc_num 25} true;
  v12 := BV64_SGT(BV64_MUL(32, v1), BV64_ADD(BV64_ADD(BV64_MUL(32, v1), BV64_MUL(8192, $4)), 8192));
  goto $truebb7, $falsebb7;
$20:
  assert {:block_sourceloc} {:sourceloc_num 26} true;
  $5 := BV64_MUL(32, v1);
  goto $25;
$21:
  assert {:block_sourceloc} {:sourceloc_num 27} true;
  v13 := BV64_SLT(BV64_SUB(BV64_ADD(BV64_SUB(BV32_SEXT64(BV32_MUL(-3, $n)), BV64_MUL(128, v1)), $c0), 119), 0);
  goto $truebb8, $falsebb8;
$22:
  assert {:block_sourceloc} {:sourceloc_num 28} true;
  $6 := BV64_SUB(0, BV64_SDIV(BV64_SUB(BV64_ADD(BV64_SUB(0, BV64_SUB(BV64_ADD(BV64_SUB(BV32_SEXT64(BV32_MUL(-3, $n)), BV64_MUL(128, v1)), $c0), 119)), 32768), 1), 32768));
  goto $24;
$23:
  assert {:block_sourceloc} {:sourceloc_num 29} true;
  $6 := BV64_SDIV(BV64_SUB(BV64_ADD(BV64_SUB(BV32_SEXT64(BV32_MUL(-3, $n)), BV64_MUL(128, v1)), $c0), 119), 32768);
  goto $24;
$24:
  assert {:block_sourceloc} {:sourceloc_num 30} true;
  $5 := BV64_ADD(BV64_ADD(BV64_MUL(32, v1), BV64_MUL(8192, $6)), 8192);
  goto $25;
$25:
  assert {:block_sourceloc} {:sourceloc_num 31} true;
  v14 := BV64_SLT(BV64_SUB(BV64_SUB(BV64_ADD(BV64_SUB(BV32_SEXT64(BV32_SUB(0, $n)), BV64_MUL(128, v1)), $c0), BV64_MUL(2, $c1.0)), 185), 0);
  goto $truebb9, $falsebb9;
$26:
  assert {:block_sourceloc} {:sourceloc_num 32} true;
  $7 := BV64_SUB(0, BV64_SDIV(BV64_SUB(BV64_ADD(BV64_SUB(0, BV64_SUB(BV64_SUB(BV64_ADD(BV64_SUB(BV32_SEXT64(BV32_SUB(0, $n)), BV64_MUL(128, v1)), $c0), BV64_MUL(2, $c1.0)), 185)), 32768), 1), 32768));
  goto $28;
$27:
  assert {:block_sourceloc} {:sourceloc_num 33} true;
  $7 := BV64_SDIV(BV64_SUB(BV64_SUB(BV64_ADD(BV64_SUB(BV32_SEXT64(BV32_SUB(0, $n)), BV64_MUL(128, v1)), $c0), BV64_MUL(2, $c1.0)), 185), 32768);
  goto $28;
$28:
  assert {:block_sourceloc} {:sourceloc_num 34} true;
  v15 := BV64_SGT($5, BV64_ADD(BV64_ADD(BV64_MUL(32, v1), BV64_MUL(8192, $7)), 8192));
  goto $truebb10, $falsebb10;
$29:
  assert {:block_sourceloc} {:sourceloc_num 35} true;
  v16 := BV64_SLT(BV64_SUB(BV64_ADD(BV64_SUB(BV32_SEXT64(BV32_MUL(-3, $n)), BV64_MUL(128, v1)), $c0), 119), 0);
  goto $truebb11, $falsebb11;
$30:
  assert {:block_sourceloc} {:sourceloc_num 36} true;
  $8 := BV64_SUB(0, BV64_SDIV(BV64_SUB(BV64_ADD(BV64_SUB(0, BV64_SUB(BV64_ADD(BV64_SUB(BV32_SEXT64(BV32_MUL(-3, $n)), BV64_MUL(128, v1)), $c0), 119)), 32768), 1), 32768));
  goto $32;
$31:
  assert {:block_sourceloc} {:sourceloc_num 37} true;
  $8 := BV64_SDIV(BV64_SUB(BV64_ADD(BV64_SUB(BV32_SEXT64(BV32_MUL(-3, $n)), BV64_MUL(128, v1)), $c0), 119), 32768);
  goto $32;
$32:
  assert {:block_sourceloc} {:sourceloc_num 38} true;
  v17 := BV64_SGT(BV64_MUL(32, v1), BV64_ADD(BV64_ADD(BV64_MUL(32, v1), BV64_MUL(8192, $8)), 8192));
  goto $truebb12, $falsebb12;
$33:
  assert {:block_sourceloc} {:sourceloc_num 39} true;
  $9 := BV64_MUL(32, v1);
  goto $38;
$34:
  assert {:block_sourceloc} {:sourceloc_num 40} true;
  v18 := BV64_SLT(BV64_SUB(BV64_ADD(BV64_SUB(BV32_SEXT64(BV32_MUL(-3, $n)), BV64_MUL(128, v1)), $c0), 119), 0);
  goto $truebb13, $falsebb13;
$35:
  assert {:block_sourceloc} {:sourceloc_num 41} true;
  $10 := BV64_SUB(0, BV64_SDIV(BV64_SUB(BV64_ADD(BV64_SUB(0, BV64_SUB(BV64_ADD(BV64_SUB(BV32_SEXT64(BV32_MUL(-3, $n)), BV64_MUL(128, v1)), $c0), 119)), 32768), 1), 32768));
  goto $37;
$36:
  assert {:block_sourceloc} {:sourceloc_num 42} true;
  $10 := BV64_SDIV(BV64_SUB(BV64_ADD(BV64_SUB(BV32_SEXT64(BV32_MUL(-3, $n)), BV64_MUL(128, v1)), $c0), 119), 32768);
  goto $37;
$37:
  assert {:block_sourceloc} {:sourceloc_num 43} true;
  $9 := BV64_ADD(BV64_ADD(BV64_MUL(32, v1), BV64_MUL(8192, $10)), 8192);
  goto $38;
$38:
  assert {:block_sourceloc} {:sourceloc_num 44} true;
  $11 := $9;
  goto $43;
$39:
  assert {:block_sourceloc} {:sourceloc_num 45} true;
  v19 := BV64_SLT(BV64_SUB(BV64_SUB(BV64_ADD(BV64_SUB(BV32_SEXT64(BV32_SUB(0, $n)), BV64_MUL(128, v1)), $c0), BV64_MUL(2, $c1.0)), 185), 0);
  goto $truebb14, $falsebb14;
$40:
  assert {:block_sourceloc} {:sourceloc_num 46} true;
  $12 := BV64_SUB(0, BV64_SDIV(BV64_SUB(BV64_ADD(BV64_SUB(0, BV64_SUB(BV64_SUB(BV64_ADD(BV64_SUB(BV32_SEXT64(BV32_SUB(0, $n)), BV64_MUL(128, v1)), $c0), BV64_MUL(2, $c1.0)), 185)), 32768), 1), 32768));
  goto $42;
$41:
  assert {:block_sourceloc} {:sourceloc_num 47} true;
  $12 := BV64_SDIV(BV64_SUB(BV64_SUB(BV64_ADD(BV64_SUB(BV32_SEXT64(BV32_SUB(0, $n)), BV64_MUL(128, v1)), $c0), BV64_MUL(2, $c1.0)), 185), 32768);
  goto $42;
$42:
  assert {:block_sourceloc} {:sourceloc_num 48} true;
  $11 := BV64_ADD(BV64_ADD(BV64_MUL(32, v1), BV64_MUL(8192, $12)), 8192);
  goto $43;
$43:
  assert {:block_sourceloc} {:sourceloc_num 49} true;
  $c2.0 := $11;
  goto $44;
$44:
  assert {:block_sourceloc} {:sourceloc_num 50} true;
  v20 := BV64_SLT(BV32_SEXT64(BV32_SUB($tsteps, 1)), BV64_SUB(BV64_SDIV(BV64_ADD($c0, 1), 4), 1));
  goto $truebb15, $falsebb15;
$45:
  assert {:block_sourceloc} {:sourceloc_num 51} true;
  $13 := BV32_SEXT64(BV32_SUB($tsteps, 1));
  goto $47;
$46:
  assert {:block_sourceloc} {:sourceloc_num 52} true;
  $13 := BV64_SUB(BV64_SDIV(BV64_ADD($c0, 1), 4), 1);
  goto $47;
$47:
  assert {:block_sourceloc} {:sourceloc_num 53} true;
  v21 := BV64_SLT($13, BV64_ADD(BV64_SUB(0, $c1.0), BV64_SDIV(BV64_SUB(BV64_ADD($c0, BV64_MUL(2, $c1.0)), 1), 4)));
  goto $truebb16, $falsebb16;
$48:
  assert {:block_sourceloc} {:sourceloc_num 54} true;
  v22 := BV64_SLT(BV32_SEXT64(BV32_SUB($tsteps, 1)), BV64_SUB(BV64_SDIV(BV64_ADD($c0, 1), 4), 1));
  goto $truebb17, $falsebb17;
$49:
  assert {:block_sourceloc} {:sourceloc_num 55} true;
  $14 := BV32_SEXT64(BV32_SUB($tsteps, 1));
  goto $51;
$50:
  assert {:block_sourceloc} {:sourceloc_num 56} true;
  $14 := BV64_SUB(BV64_SDIV(BV64_ADD($c0, 1), 4), 1);
  goto $51;
$51:
  assert {:block_sourceloc} {:sourceloc_num 57} true;
  $15 := $14;
  goto $53;
$52:
  assert {:block_sourceloc} {:sourceloc_num 58} true;
  $15 := BV64_ADD(BV64_SUB(0, $c1.0), BV64_SDIV(BV64_SUB(BV64_ADD($c0, BV64_MUL(2, $c1.0)), 1), 4));
  goto $53;
$53:
  assert {:block_sourceloc} {:sourceloc_num 59} true;
  v23 := BV64_SLE($c2.0, $15);
  goto $truebb18, $falsebb18;
$54:
  assert {:block_sourceloc} {:sourceloc_num 60} true;
  v24 := BV64_SLT(BV64_ADD(BV64_SUB(BV64_SUB(BV64_ADD(BV64_SUB(BV64_SUB(BV32_SEXT64(BV32_SUB(0, $n)), BV64_MUL(2, v2)), BV64_MUL(4, v3)), $c0), BV64_MUL(2, $c1.0)), BV64_MUL(4, $c2.0)), 1), 0);
  goto $truebb19, $falsebb19;
$55:
  assert {:block_sourceloc} {:sourceloc_num 61} true;
  $16 := BV64_SUB(0, BV64_SDIV(BV64_SUB(BV64_ADD(BV64_SUB(0, BV64_ADD(BV64_SUB(BV64_SUB(BV64_ADD(BV64_SUB(BV64_SUB(BV32_SEXT64(BV32_SUB(0, $n)), BV64_MUL(2, v2)), BV64_MUL(4, v3)), $c0), BV64_MUL(2, $c1.0)), BV64_MUL(4, $c2.0)), 1)), 64), 1), 64));
  goto $57;
$56:
  assert {:block_sourceloc} {:sourceloc_num 62} true;
  $16 := BV64_SDIV(BV64_ADD(BV64_SUB(BV64_SUB(BV64_ADD(BV64_SUB(BV64_SUB(BV32_SEXT64(BV32_SUB(0, $n)), BV64_MUL(2, v2)), BV64_MUL(4, v3)), $c0), BV64_MUL(2, $c1.0)), BV64_MUL(4, $c2.0)), 1), 64);
  goto $57;
$57:
  assert {:block_sourceloc} {:sourceloc_num 63} true;
  v25 := BV64_SGT(v3, BV64_ADD(BV64_ADD(v3, BV64_MUL(16, $16)), 16));
  goto $truebb20, $falsebb20;
$58:
  assert {:block_sourceloc} {:sourceloc_num 64} true;
  $17 := v3;
  goto $63;
$59:
  assert {:block_sourceloc} {:sourceloc_num 65} true;
  v26 := BV64_SLT(BV64_ADD(BV64_SUB(BV64_SUB(BV64_ADD(BV64_SUB(BV64_SUB(BV32_SEXT64(BV32_SUB(0, $n)), BV64_MUL(2, v2)), BV64_MUL(4, v3)), $c0), BV64_MUL(2, $c1.0)), BV64_MUL(4, $c2.0)), 1), 0);
  goto $truebb21, $falsebb21;
$60:
  assert {:block_sourceloc} {:sourceloc_num 66} true;
  $18 := BV64_SUB(0, BV64_SDIV(BV64_SUB(BV64_ADD(BV64_SUB(0, BV64_ADD(BV64_SUB(BV64_SUB(BV64_ADD(BV64_SUB(BV64_SUB(BV32_SEXT64(BV32_SUB(0, $n)), BV64_MUL(2, v2)), BV64_MUL(4, v3)), $c0), BV64_MUL(2, $c1.0)), BV64_MUL(4, $c2.0)), 1)), 64), 1), 64));
  goto $62;
$61:
  assert {:block_sourceloc} {:sourceloc_num 67} true;
  $18 := BV64_SDIV(BV64_ADD(BV64_SUB(BV64_SUB(BV64_ADD(BV64_SUB(BV64_SUB(BV32_SEXT64(BV32_SUB(0, $n)), BV64_MUL(2, v2)), BV64_MUL(4, v3)), $c0), BV64_MUL(2, $c1.0)), BV64_MUL(4, $c2.0)), 1), 64);
  goto $62;
$62:
  assert {:block_sourceloc} {:sourceloc_num 68} true;
  $17 := BV64_ADD(BV64_ADD(v3, BV64_MUL(16, $18)), 16);
  goto $63;
$63:
  assert {:block_sourceloc} {:sourceloc_num 69} true;
  $c4.0 := $17;
  goto $64;
$64:
  assert {:block_sourceloc} {:sourceloc_num 70} true;
  v27 := BV64_SLT(31, BV64_SUB(BV64_SUB(BV32_SEXT64($tsteps), $c2.0), 1));
  goto $truebb22, $falsebb22;
$65:
  assert {:block_sourceloc} {:sourceloc_num 71} true;
  $19 := 31;
  goto $67;
$66:
  assert {:block_sourceloc} {:sourceloc_num 72} true;
  $19 := BV64_SUB(BV64_SUB(BV32_SEXT64($tsteps), $c2.0), 1);
  goto $67;
$67:
  assert {:block_sourceloc} {:sourceloc_num 73} true;
  v28 := BV64_SLT($19, BV64_ADD(BV64_SUB(BV64_SUB(BV64_SUB(0, v2), $c1.0), $c2.0), BV64_SDIV(BV64_SUB(BV64_ADD(BV64_ADD(BV64_MUL(2, v2), $c0), BV64_MUL(2, $c1.0)), 1), 4)));
  goto $truebb23, $falsebb23;
$68:
  assert {:block_sourceloc} {:sourceloc_num 74} true;
  v29 := BV64_SLT(31, BV64_SUB(BV64_SUB(BV32_SEXT64($tsteps), $c2.0), 1));
  goto $truebb24, $falsebb24;
$69:
  assert {:block_sourceloc} {:sourceloc_num 75} true;
  $20 := 31;
  goto $71;
$70:
  assert {:block_sourceloc} {:sourceloc_num 76} true;
  $20 := BV64_SUB(BV64_SUB(BV32_SEXT64($tsteps), $c2.0), 1);
  goto $71;
$71:
  assert {:block_sourceloc} {:sourceloc_num 77} true;
  $21 := $20;
  goto $73;
$72:
  assert {:block_sourceloc} {:sourceloc_num 78} true;
  $21 := BV64_ADD(BV64_SUB(BV64_SUB(BV64_SUB(0, v2), $c1.0), $c2.0), BV64_SDIV(BV64_SUB(BV64_ADD(BV64_ADD(BV64_MUL(2, v2), $c0), BV64_MUL(2, $c1.0)), 1), 4));
  goto $73;
$73:
  assert {:block_sourceloc} {:sourceloc_num 79} true;
  v30 := BV64_SLE($c4.0, $21);
  goto $truebb25, $falsebb25;
$74:
  assert {:block_sourceloc} {:sourceloc_num 80} true;
  assert {:sourceloc} {:sourceloc_num 81} true;
  v31 := $$A[BV_EXTRACT(BV64_ADD(BV64_MUL(BV64_SUB(BV64_ADD(v2, $c1.0), 1), BV32_SEXT64($n)), BV64_SUB(BV64_SUB(BV64_SUB(BV64_SUB(BV64_ADD(BV64_MUL(-2, v2), $c0), BV64_MUL(2, $c1.0)), BV64_MUL(4, $c2.0)), BV64_MUL(4, $c4.0)), 1)), 32, 0)];
  assert {:sourceloc} {:sourceloc_num 82} true;
  v32 := $$A[BV_EXTRACT(BV64_ADD(BV64_MUL(BV64_SUB(BV64_ADD(v2, $c1.0), 1), BV32_SEXT64($n)), BV64_SUB(BV64_SUB(BV64_SUB(BV64_ADD(BV64_MUL(-2, v2), $c0), BV64_MUL(2, $c1.0)), BV64_MUL(4, $c2.0)), BV64_MUL(4, $c4.0))), 32, 0)];
  assert {:sourceloc} {:sourceloc_num 83} true;
  v33 := $$A[BV_EXTRACT(BV64_ADD(BV64_MUL(BV64_SUB(BV64_ADD(v2, $c1.0), 1), BV32_SEXT64($n)), BV64_ADD(BV64_SUB(BV64_SUB(BV64_SUB(BV64_ADD(BV64_MUL(-2, v2), $c0), BV64_MUL(2, $c1.0)), BV64_MUL(4, $c2.0)), BV64_MUL(4, $c4.0)), 1)), 32, 0)];
  assert {:sourceloc} {:sourceloc_num 84} true;
  v34 := $$A[BV_EXTRACT(BV64_ADD(BV64_MUL(BV64_ADD(v2, $c1.0), BV32_SEXT64($n)), BV64_SUB(BV64_SUB(BV64_SUB(BV64_SUB(BV64_ADD(BV64_MUL(-2, v2), $c0), BV64_MUL(2, $c1.0)), BV64_MUL(4, $c2.0)), BV64_MUL(4, $c4.0)), 1)), 32, 0)];
  assert {:sourceloc} {:sourceloc_num 85} true;
  v35 := $$A[BV_EXTRACT(BV64_ADD(BV64_MUL(BV64_ADD(v2, $c1.0), BV32_SEXT64($n)), BV64_SUB(BV64_SUB(BV64_SUB(BV64_ADD(BV64_MUL(-2, v2), $c0), BV64_MUL(2, $c1.0)), BV64_MUL(4, $c2.0)), BV64_MUL(4, $c4.0))), 32, 0)];
  assert {:sourceloc} {:sourceloc_num 86} true;
  v36 := $$A[BV_EXTRACT(BV64_ADD(BV64_MUL(BV64_ADD(v2, $c1.0), BV32_SEXT64($n)), BV64_ADD(BV64_SUB(BV64_SUB(BV64_SUB(BV64_ADD(BV64_MUL(-2, v2), $c0), BV64_MUL(2, $c1.0)), BV64_MUL(4, $c2.0)), BV64_MUL(4, $c4.0)), 1)), 32, 0)];
  assert {:sourceloc} {:sourceloc_num 87} true;
  v37 := $$A[BV_EXTRACT(BV64_ADD(BV64_MUL(BV64_ADD(BV64_ADD(v2, $c1.0), 1), BV32_SEXT64($n)), BV64_SUB(BV64_SUB(BV64_SUB(BV64_SUB(BV64_ADD(BV64_MUL(-2, v2), $c0), BV64_MUL(2, $c1.0)), BV64_MUL(4, $c2.0)), BV64_MUL(4, $c4.0)), 1)), 32, 0)];
  assert {:sourceloc} {:sourceloc_num 88} true;
  v38 := $$A[BV_EXTRACT(BV64_ADD(BV64_MUL(BV64_ADD(BV64_ADD(v2, $c1.0), 1), BV32_SEXT64($n)), BV64_SUB(BV64_SUB(BV64_SUB(BV64_ADD(BV64_MUL(-2, v2), $c0), BV64_MUL(2, $c1.0)), BV64_MUL(4, $c2.0)), BV64_MUL(4, $c4.0))), 32, 0)];
  assert {:sourceloc} {:sourceloc_num 89} true;
  v39 := $$A[BV_EXTRACT(BV64_ADD(BV64_MUL(BV64_ADD(BV64_ADD(v2, $c1.0), 1), BV32_SEXT64($n)), BV64_ADD(BV64_SUB(BV64_SUB(BV64_SUB(BV64_ADD(BV64_MUL(-2, v2), $c0), BV64_MUL(2, $c1.0)), BV64_MUL(4, $c2.0)), BV64_MUL(4, $c4.0)), 1)), 32, 0)];
  assert {:sourceloc} {:sourceloc_num 90} true;
  $$A[BV_EXTRACT(BV64_ADD(BV64_MUL(BV64_ADD(v2, $c1.0), BV32_SEXT64($n)), BV64_SUB(BV64_SUB(BV64_SUB(BV64_ADD(BV64_MUL(-2, v2), $c0), BV64_MUL(2, $c1.0)), BV64_MUL(4, $c2.0)), BV64_MUL(4, $c4.0))), 32, 0)] := FDIV64(FADD64(FADD64(FADD64(FADD64(FADD64(FADD64(FADD64(FADD64(v31, v32), v33), v34), v35), v36), v37), v38), v39), 4621256167635550208);
  goto $75;
$75:
  assert {:block_sourceloc} {:sourceloc_num 91} true;
  $c4.0 := BV64_ADD($c4.0, 16);
  goto $64;
$76:
  assert {:block_sourceloc} {:sourceloc_num 92} true;
  goto $77;
$77:
  assert {:block_sourceloc} {:sourceloc_num 93} true;
  $c2.0 := BV64_ADD($c2.0, 8192);
  goto $44;
$78:
  assert {:block_sourceloc} {:sourceloc_num 94} true;
  goto $79;
$79:
  assert {:block_sourceloc} {:sourceloc_num 95} true;
  goto $80;
$80:
  assert {:block_sourceloc} {:sourceloc_num 96} true;
  $c1.0 := BV64_ADD($c1.0, 8192);
  goto $10;
$81:
  assert {:block_sourceloc} {:sourceloc_num 97} true;
  return;
$truebb:
  assume {:partition} v4;
  assert {:block_sourceloc} {:sourceloc_num 98} true;
  goto $1;
$falsebb:
  assume {:partition} !v4;
  assert {:block_sourceloc} {:sourceloc_num 99} true;
  goto $2;
$truebb0:
  assume {:partition} v5;
  assert {:block_sourceloc} {:sourceloc_num 100} true;
  goto $4;
$falsebb0:
  assume {:partition} !v5;
  assert {:block_sourceloc} {:sourceloc_num 101} true;
  goto $5;
$truebb1:
  assume {:partition} v6;
  assert {:block_sourceloc} {:sourceloc_num 102} true;
  goto $6;
$falsebb1:
  assume {:partition} !v6;
  assert {:block_sourceloc} {:sourceloc_num 103} true;
  goto $7;
$truebb2:
  assume {:partition} v7;
  assert {:block_sourceloc} {:sourceloc_num 104} true;
  goto $11;
$falsebb2:
  assume {:partition} !v7;
  assert {:block_sourceloc} {:sourceloc_num 105} true;
  goto $12;
$truebb3:
  assume {:partition} v8;
  assert {:block_sourceloc} {:sourceloc_num 106} true;
  goto $14;
$falsebb3:
  assume {:partition} !v8;
  assert {:block_sourceloc} {:sourceloc_num 107} true;
  goto $81;
$truebb4:
  assume {:partition} v9;
  assert {:block_sourceloc} {:sourceloc_num 108} true;
  goto $15;
$falsebb4:
  assume {:partition} !v9;
  assert {:block_sourceloc} {:sourceloc_num 109} true;
  goto $79;
$truebb5:
  assume {:partition} v10;
  assert {:block_sourceloc} {:sourceloc_num 110} true;
  goto $16;
$falsebb5:
  assume {:partition} !v10;
  assert {:block_sourceloc} {:sourceloc_num 111} true;
  goto $79;
$truebb6:
  assume {:partition} v11;
  assert {:block_sourceloc} {:sourceloc_num 112} true;
  goto $17;
$falsebb6:
  assume {:partition} !v11;
  assert {:block_sourceloc} {:sourceloc_num 113} true;
  goto $18;
$truebb7:
  assume {:partition} v12;
  assert {:block_sourceloc} {:sourceloc_num 114} true;
  goto $20;
$falsebb7:
  assume {:partition} !v12;
  assert {:block_sourceloc} {:sourceloc_num 115} true;
  goto $21;
$truebb8:
  assume {:partition} v13;
  assert {:block_sourceloc} {:sourceloc_num 116} true;
  goto $22;
$falsebb8:
  assume {:partition} !v13;
  assert {:block_sourceloc} {:sourceloc_num 117} true;
  goto $23;
$truebb9:
  assume {:partition} v14;
  assert {:block_sourceloc} {:sourceloc_num 118} true;
  goto $26;
$falsebb9:
  assume {:partition} !v14;
  assert {:block_sourceloc} {:sourceloc_num 119} true;
  goto $27;
$truebb10:
  assume {:partition} v15;
  assert {:block_sourceloc} {:sourceloc_num 120} true;
  goto $29;
$falsebb10:
  assume {:partition} !v15;
  assert {:block_sourceloc} {:sourceloc_num 121} true;
  goto $39;
$truebb11:
  assume {:partition} v16;
  assert {:block_sourceloc} {:sourceloc_num 122} true;
  goto $30;
$falsebb11:
  assume {:partition} !v16;
  assert {:block_sourceloc} {:sourceloc_num 123} true;
  goto $31;
$truebb12:
  assume {:partition} v17;
  assert {:block_sourceloc} {:sourceloc_num 124} true;
  goto $33;
$falsebb12:
  assume {:partition} !v17;
  assert {:block_sourceloc} {:sourceloc_num 125} true;
  goto $34;
$truebb13:
  assume {:partition} v18;
  assert {:block_sourceloc} {:sourceloc_num 126} true;
  goto $35;
$falsebb13:
  assume {:partition} !v18;
  assert {:block_sourceloc} {:sourceloc_num 127} true;
  goto $36;
$truebb14:
  assume {:partition} v19;
  assert {:block_sourceloc} {:sourceloc_num 128} true;
  goto $40;
$falsebb14:
  assume {:partition} !v19;
  assert {:block_sourceloc} {:sourceloc_num 129} true;
  goto $41;
$truebb15:
  assume {:partition} v20;
  assert {:block_sourceloc} {:sourceloc_num 130} true;
  goto $45;
$falsebb15:
  assume {:partition} !v20;
  assert {:block_sourceloc} {:sourceloc_num 131} true;
  goto $46;
$truebb16:
  assume {:partition} v21;
  assert {:block_sourceloc} {:sourceloc_num 132} true;
  goto $48;
$falsebb16:
  assume {:partition} !v21;
  assert {:block_sourceloc} {:sourceloc_num 133} true;
  goto $52;
$truebb17:
  assume {:partition} v22;
  assert {:block_sourceloc} {:sourceloc_num 134} true;
  goto $49;
$falsebb17:
  assume {:partition} !v22;
  assert {:block_sourceloc} {:sourceloc_num 135} true;
  goto $50;
$truebb18:
  assume {:partition} v23;
  assert {:block_sourceloc} {:sourceloc_num 136} true;
  goto $54;
$falsebb18:
  assume {:partition} !v23;
  assert {:block_sourceloc} {:sourceloc_num 137} true;
  goto $78;
$truebb19:
  assume {:partition} v24;
  assert {:block_sourceloc} {:sourceloc_num 138} true;
  goto $55;
$falsebb19:
  assume {:partition} !v24;
  assert {:block_sourceloc} {:sourceloc_num 139} true;
  goto $56;
$truebb20:
  assume {:partition} v25;
  assert {:block_sourceloc} {:sourceloc_num 140} true;
  goto $58;
$falsebb20:
  assume {:partition} !v25;
  assert {:block_sourceloc} {:sourceloc_num 141} true;
  goto $59;
$truebb21:
  assume {:partition} v26;
  assert {:block_sourceloc} {:sourceloc_num 142} true;
  goto $60;
$falsebb21:
  assume {:partition} !v26;
  assert {:block_sourceloc} {:sourceloc_num 143} true;
  goto $61;
$truebb22:
  assume {:partition} v27;
  assert {:block_sourceloc} {:sourceloc_num 144} true;
  goto $65;
$falsebb22:
  assume {:partition} !v27;
  assert {:block_sourceloc} {:sourceloc_num 145} true;
  goto $66;
$truebb23:
  assume {:partition} v28;
  assert {:block_sourceloc} {:sourceloc_num 146} true;
  goto $68;
$falsebb23:
  assume {:partition} !v28;
  assert {:block_sourceloc} {:sourceloc_num 147} true;
  goto $72;
$truebb24:
  assume {:partition} v29;
  assert {:block_sourceloc} {:sourceloc_num 148} true;
  goto $69;
$falsebb24:
  assume {:partition} !v29;
  assert {:block_sourceloc} {:sourceloc_num 149} true;
  goto $70;
$truebb25:
  assume {:partition} v30;
  assert {:block_sourceloc} {:sourceloc_num 150} true;
  goto $74;
$falsebb25:
  assume {:partition} !v30;
  assert {:block_sourceloc} {:sourceloc_num 151} true;
  goto $76;
}
axiom (if group_size_z == 1 then 1 else 0) != 0;
axiom (if num_groups_z == 1 then 1 else 0) != 0;
axiom (if group_size_x == 32 then 1 else 0) != 0;
axiom (if group_size_y == 16 then 1 else 0) != 0;
axiom (if num_groups_x == 2 then 1 else 0) != 0;
axiom (if num_groups_y == 2 then 1 else 0) != 0;
axiom (if global_offset_x == 0 then 1 else 0) != 0;
axiom (if global_offset_y == 0 then 1 else 0) != 0;
axiom (if global_offset_z == 0 then 1 else 0) != 0;
