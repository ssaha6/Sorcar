type _SIZE_T_TYPE = bv32;

procedure _ATOMIC_OP64(x : [int]int, y : int) returns (z : int, A : [int]int);
var {:source_name "A"} {:global} $$A : [int]int;
axiom {:array_info "$$A"} {:global} {:elem_width 64} {:source_name "A"} {:source_elem_width 64} {:source_dimensions "*"} true;
var {:race_checking} {:global} {:elem_width 64} {:source_elem_width 64} {:source_dimensions "*"} _READ_HAS_OCCURRED_$$A : bool;
var {:race_checking} {:global} {:elem_width 64} {:source_elem_width 64} {:source_dimensions "*"} _WRITE_HAS_OCCURRED_$$A : bool;
var {:race_checking} {:global} {:elem_width 64} {:source_elem_width 64} {:source_dimensions "*"} _ATOMIC_HAS_OCCURRED_$$A : bool;

var {:source_name "B"} {:global} $$B : [int]int;
axiom {:array_info "$$B"} {:global} {:elem_width 64} {:source_name "B"} {:source_elem_width 64} {:source_dimensions "*"} true;
var {:race_checking} {:global} {:elem_width 64} {:source_elem_width 64} {:source_dimensions "*"} _READ_HAS_OCCURRED_$$B : bool;
var {:race_checking} {:global} {:elem_width 64} {:source_elem_width 64} {:source_dimensions "*"} _WRITE_HAS_OCCURRED_$$B : bool;
var {:race_checking} {:global} {:elem_width 64} {:source_elem_width 64} {:source_dimensions "*"} _ATOMIC_HAS_OCCURRED_$$B : bool;

const _WATCHED_OFFSET : int;
const {:global_offset_x} global_offset_x : int;
const {:global_offset_y} global_offset_y : int;
const {:global_offset_z} global_offset_z : int;
const {:group_id_x} group_id_x : int;
const {:group_id_y} group_id_y : int;
const {:group_size_x} group_size_x : int;
const {:group_size_y} group_size_y : int;
const {:group_size_z} group_size_z : int;
const {:local_id_x} local_id_x : int;
const {:local_id_y} local_id_y : int;
const {:num_groups_x} num_groups_x : int;
const {:num_groups_y} num_groups_y : int;
const {:num_groups_z} num_groups_z : int;
function BV32_SEXT64(int) : int;
function BV_EXTRACT(int, int, int) : int;
function FADD64(int, int) : int;
function FMUL64(int, int) : int;
function {:inline true} BV1_ZEXT32(x : int) : int {
  x
}
function {:inline true} BV32_ADD(x : int, y : int) : int {
  x + y
}
function {:inline true} BV32_AND(x : int, y : int) : int {
  if x == y then x else (if x == 0 || y == 0 then 0 else BV32_AND_UF(x, y))
}
function BV32_AND_UF(int, int) : int;
function {:inline true} BV32_MUL(x : int, y : int) : int {
  x * y
}
function {:inline true} BV32_OR(x : int, y : int) : int {
  if x == y then x else (if x == 0 then y else (if y == 0 then x else BV32_OR_UF(x, y)))
}
function BV32_OR_UF(int, int) : int;
function {:inline true} BV32_SGE(x : int, y : int) : bool {
  x >= y
}
function {:inline true} BV32_SLE(x : int, y : int) : bool {
  x <= y
}
function {:inline true} BV32_SUB(x : int, y : int) : int {
  x - y
}
function {:inline true} BV32_UDIV(x : int, y : int) : int {
  x div y
}
function {:inline true} BV32_UGE(x : int, y : int) : bool {
  x >= y
}
function {:inline true} BV32_ULE(x : int, y : int) : bool {
  x <= y
}
function {:inline true} BV32_UREM(x : int, y : int) : int {
  x mod y
}
function {:inline true} BV32_ZEXT64(x : int) : int {
  x
}
function {:inline true} BV64_ADD(x : int, y : int) : int {
  x + y
}
function {:inline true} BV64_MUL(x : int, y : int) : int {
  x * y
}
function {:inline true} BV64_SDIV(x : int, y : int) : int {
  x div y
}
function {:inline true} BV64_SGE(x : int, y : int) : bool {
  x >= y
}
function {:inline true} BV64_SGT(x : int, y : int) : bool {
  x > y
}
function {:inline true} BV64_SLE(x : int, y : int) : bool {
  x <= y
}
function {:inline true} BV64_SLT(x : int, y : int) : bool {
  x < y
}
function {:inline true} BV64_SREM(x : int, y : int) : int {
  x mod y
}
function {:inline true} BV64_SUB(x : int, y : int) : int {
  x - y
}
procedure {:source_name "kernel0"} {:kernel} $kernel0($n:int, $tsteps:int, $c0:int)
requires {:sourceloc_num 0} (if $n == 1024 then 1 else 0) != 0;
requires {:sourceloc_num 1} (if $tsteps == 2048 then 1 else 0) != 0;
requires {:sourceloc_num 2} (if BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV1_ZEXT32((if BV32_SGE($n, 3) then 1 else 0)), BV1_ZEXT32((if BV32_SLE($n, 2147483647) then 1 else 0))), BV1_ZEXT32((if BV32_SLE($tsteps, 2147483647) then 1 else 0))), BV1_ZEXT32((if BV64_SGE(BV32_SEXT64($tsteps), BV64_ADD($c0, 1)) then 1 else 0))), BV1_ZEXT32((if BV64_SGE($c0, 0) then 1 else 0))) != 0 then 1 else 0) != 0;
requires {:procedure_wide_invariant} {:do_not_predicate} {:sourceloc_num 3} (if (_WRITE_HAS_OCCURRED_$$B ==> BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV1_ZEXT32((if BV64_SGE(BV64_ADD(BV64_MUL(32, BV32_ZEXT64(group_id_x)), BV32_ZEXT64(local_id_x)), 0) then 1 else 0)), BV1_ZEXT32((if BV64_SLE(BV64_ADD(BV64_MUL(32, BV32_ZEXT64(group_id_x)), BV32_ZEXT64(local_id_x)), 8191) then 1 else 0))), BV1_ZEXT32((if BV32_UGE($n, BV32_ADD(BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n), 2)) then 1 else 0))), BV1_ZEXT32((if BV32_UGE(BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n), 1) then 1 else 0))), BV1_ZEXT32((if BV32_UGE($n, BV32_ADD(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), 2)) then 1 else 0))), BV1_ZEXT32((if BV32_UGE(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), 1) then 1 else 0))), BV1_ZEXT32((if BV64_SGE(BV32_ZEXT64(BV32_UREM(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), 8192)), BV64_MUL(32, BV32_ZEXT64(group_id_y))) then 1 else 0))), BV1_ZEXT32((if BV64_SGE(BV64_ADD(BV64_MUL(32, BV32_ZEXT64(group_id_y)), 31), BV32_ZEXT64(BV32_UREM(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), 8192))) then 1 else 0))), BV1_ZEXT32((if BV64_SREM(BV64_SUB(BV64_ADD(BV64_MUL(32, BV32_ZEXT64(group_id_x)), BV32_ZEXT64(local_id_x)), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n))), 8192) == 0 then 1 else 0))), BV1_ZEXT32((if BV64_SREM(BV64_SUB(BV32_ZEXT64(local_id_y), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n))), 16) == 0 then 1 else 0))) != 0) then 1 else 0) != 0;
requires {:procedure_wide_invariant} {:do_not_predicate} {:sourceloc_num 4} (if (_READ_HAS_OCCURRED_$$A ==> BV32_OR(BV32_OR(BV32_OR(BV32_OR(BV32_OR(BV32_OR(BV32_OR(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV1_ZEXT32((if BV32_UGE(BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n), 3) then 1 else 0)), BV1_ZEXT32((if BV32_UGE($n, BV32_ADD(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), 2)) then 1 else 0))), BV1_ZEXT32((if BV64_SGE(BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n)), BV64_ADD(BV64_SREM(BV64_ADD(BV64_ADD(BV64_ADD(BV64_MUL(32, BV32_ZEXT64(group_id_x)), BV32_ZEXT64(local_id_x)), BV32_ZEXT64(BV32_MUL(8191, BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n)))), 1), 8192), 1)) then 1 else 0))), BV1_ZEXT32((if BV64_SGE(BV32_SEXT64($n), BV64_ADD(BV64_ADD(BV64_SREM(BV64_ADD(BV64_ADD(BV64_ADD(BV64_MUL(32, BV32_ZEXT64(group_id_x)), BV32_ZEXT64(local_id_x)), BV32_ZEXT64(BV32_MUL(8191, BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n)))), 1), 8192), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n))), 1)) then 1 else 0))), BV1_ZEXT32((if BV64_SLE(BV64_SREM(BV64_ADD(BV64_ADD(BV64_ADD(BV64_MUL(32, BV32_ZEXT64(group_id_x)), BV32_ZEXT64(local_id_x)), BV32_ZEXT64(BV32_MUL(8191, BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n)))), 1), 8192), 1) then 1 else 0))), BV1_ZEXT32((if BV64_SGE(BV64_SREM(BV64_ADD(BV64_ADD(BV64_ADD(BV64_MUL(32, BV32_ZEXT64(group_id_x)), BV32_ZEXT64(local_id_x)), BV32_ZEXT64(BV32_MUL(8191, BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n)))), 1), 8192), BV64_SREM(BV64_ADD(BV64_ADD(BV64_ADD(BV64_ADD(BV32_ZEXT64(local_id_x), BV32_ZEXT64(local_id_y)), BV32_ZEXT64(BV32_MUL(15, BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n)))), BV32_ZEXT64(BV32_MUL(15, BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n)))), 1), 16)) then 1 else 0))), BV1_ZEXT32((if BV64_SGE(BV64_ADD(BV64_ADD(BV64_SREM(BV64_ADD(BV64_ADD(BV64_ADD(BV64_ADD(BV32_ZEXT64(local_id_x), BV32_ZEXT64(local_id_y)), BV32_ZEXT64(BV32_MUL(15, BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n)))), BV32_ZEXT64(BV32_MUL(15, BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n)))), 1), 16), BV64_MUL(16, BV64_SDIV(BV64_ADD(BV64_ADD(BV64_ADD(BV64_ADD(BV32_ZEXT64(local_id_x), BV32_ZEXT64(local_id_y)), BV32_ZEXT64(BV32_MUL(15, BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n)))), BV32_ZEXT64(BV32_MUL(15, BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n)))), 1), 16))), 30), BV64_ADD(BV64_ADD(BV64_ADD(BV64_ADD(BV64_SREM(BV64_ADD(BV64_ADD(BV64_ADD(BV64_ADD(BV64_ADD(BV64_ADD(BV64_ADD(BV64_SUB(0, BV64_SREM(BV64_ADD(BV64_ADD(BV64_ADD(BV64_ADD(BV32_ZEXT64(local_id_x), BV32_ZEXT64(local_id_y)), BV32_ZEXT64(BV32_MUL(15, BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n)))), BV32_ZEXT64(BV32_MUL(15, BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n)))), 1), 16)), BV64_MUL(32, BV32_ZEXT64(group_id_x))), BV64_MUL(32, BV32_ZEXT64(group_id_y))), BV32_ZEXT64(local_id_x)), BV64_MUL(8192, BV32_ZEXT64(local_id_y))), BV32_ZEXT64(BV32_MUL(8191, BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n)))), BV32_ZEXT64(BV32_MUL(8191, BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n)))), 32), 8192), BV32_ZEXT64(local_id_x)), BV32_ZEXT64(local_id_y)), BV32_ZEXT64(BV32_MUL(15, BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n)))), BV32_ZEXT64(BV32_MUL(15, BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n))))) then 1 else 0))), BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV1_ZEXT32((if BV32_ZEXT64(group_id_x) == 0 then 1 else 0)), BV1_ZEXT32((if BV64_SGE(BV32_ZEXT64(local_id_x), 1) then 1 else 0))), BV1_ZEXT32((if BV64_SGE(BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n)), BV32_ZEXT64(local_id_x)) then 1 else 0))), BV1_ZEXT32((if BV32_ULE(BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n), 2) then 1 else 0))), BV1_ZEXT32((if BV32_UGE(BV32_ADD(BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n), BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n)), 4) then 1 else 0))), BV1_ZEXT32((if BV32_UGE($n, BV32_ADD(BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n), BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n))) then 1 else 0))), BV1_ZEXT32((if BV64_SGE(BV64_ADD(BV32_SEXT64($n), BV32_ZEXT64(local_id_x)), BV64_ADD(BV64_ADD(BV64_ADD(BV64_SREM(BV64_ADD(BV64_ADD(BV64_ADD(BV64_ADD(BV32_ZEXT64(local_id_x), BV32_ZEXT64(local_id_y)), BV32_ZEXT64(BV32_MUL(15, BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n)))), BV32_ZEXT64(BV32_MUL(15, BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n)))), 1), 16), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n))), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n))), 1)) then 1 else 0))), BV1_ZEXT32((if BV64_SGE(BV64_ADD(BV32_ZEXT64(local_id_x), 1), BV64_ADD(BV64_SREM(BV64_ADD(BV64_ADD(BV64_ADD(BV64_ADD(BV32_ZEXT64(local_id_x), BV32_ZEXT64(local_id_y)), BV32_ZEXT64(BV32_MUL(15, BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n)))), BV32_ZEXT64(BV32_MUL(15, BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n)))), 1), 16), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n)))) then 1 else 0))), BV1_ZEXT32((if BV64_SGE(BV64_ADD(BV64_ADD(BV64_SREM(BV64_ADD(BV64_ADD(BV64_ADD(BV64_ADD(BV32_ZEXT64(local_id_x), BV32_ZEXT64(local_id_y)), BV32_ZEXT64(BV32_MUL(15, BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n)))), BV32_ZEXT64(BV32_MUL(15, BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n)))), 1), 16), BV64_MUL(16, BV64_SDIV(BV64_ADD(BV64_ADD(BV64_ADD(BV64_ADD(BV32_ZEXT64(local_id_x), BV32_ZEXT64(local_id_y)), BV32_ZEXT64(BV32_MUL(15, BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n)))), BV32_ZEXT64(BV32_MUL(15, BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n)))), 1), 16))), 30), BV64_ADD(BV64_ADD(BV64_ADD(BV64_ADD(BV64_SREM(BV64_ADD(BV64_ADD(BV64_ADD(BV64_ADD(BV64_ADD(BV64_ADD(BV64_SUB(0, BV64_SREM(BV64_ADD(BV64_ADD(BV64_ADD(BV64_ADD(BV32_ZEXT64(local_id_x), BV32_ZEXT64(local_id_y)), BV32_ZEXT64(BV32_MUL(15, BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n)))), BV32_ZEXT64(BV32_MUL(15, BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n)))), 1), 16)), BV64_MUL(32, BV32_ZEXT64(group_id_y))), BV32_ZEXT64(local_id_x)), BV64_MUL(8192, BV32_ZEXT64(local_id_y))), BV32_ZEXT64(BV32_MUL(8191, BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n)))), BV32_ZEXT64(BV32_MUL(8191, BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n)))), 32), 8192), BV32_ZEXT64(local_id_x)), BV32_ZEXT64(local_id_y)), BV32_ZEXT64(BV32_MUL(15, BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n)))), BV32_ZEXT64(BV32_MUL(15, BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n))))) then 1 else 0)))), BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV1_ZEXT32((if BV32_ZEXT64(group_id_x) == 0 then 1 else 0)), BV1_ZEXT32((if BV32_ZEXT64(group_id_y) == 0 then 1 else 0))), BV1_ZEXT32((if BV64_SGE(BV32_SEXT64($n), BV64_ADD(BV32_ZEXT64(local_id_x), 2)) then 1 else 0))), BV1_ZEXT32((if BV64_SGE(BV32_ZEXT64(local_id_x), 1) then 1 else 0))), BV1_ZEXT32((if BV64_SGE(BV32_SEXT64($n), BV64_ADD(BV32_ZEXT64(local_id_y), 2)) then 1 else 0))), BV1_ZEXT32((if BV64_SGE(BV32_ZEXT64(local_id_y), 1) then 1 else 0))), BV1_ZEXT32((if BV64_SGE(BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n)), BV32_ZEXT64(local_id_x)) then 1 else 0))), BV1_ZEXT32((if BV64_SGE(BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n)), BV32_ZEXT64(local_id_y)) then 1 else 0))), BV1_ZEXT32((if BV32_ULE(BV32_ADD(BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n), BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n)), 3) then 1 else 0)))), BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV1_ZEXT32((if BV64_SGE(BV64_ADD(BV64_MUL(32, BV32_ZEXT64(group_id_x)), BV32_ZEXT64(local_id_x)), 0) then 1 else 0)), BV1_ZEXT32((if BV64_SLE(BV64_ADD(BV64_MUL(32, BV32_ZEXT64(group_id_x)), BV32_ZEXT64(local_id_x)), 8191) then 1 else 0))), BV1_ZEXT32((if BV32_UGE($n, BV32_ADD(BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n), 3)) then 1 else 0))), BV1_ZEXT32((if BV32_UGE(BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n), 0) then 1 else 0))), BV1_ZEXT32((if BV32_UGE($n, BV32_ADD(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), 2)) then 1 else 0))), BV1_ZEXT32((if BV32_UGE(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), 1) then 1 else 0))), BV1_ZEXT32((if BV64_SGE(BV32_ZEXT64(BV32_UREM(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), 8192)), BV64_MUL(32, BV32_ZEXT64(group_id_y))) then 1 else 0))), BV1_ZEXT32((if BV64_SGE(BV64_ADD(BV64_MUL(32, BV32_ZEXT64(group_id_y)), 31), BV32_ZEXT64(BV32_UREM(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), 8192))) then 1 else 0))), BV1_ZEXT32((if BV64_SREM(BV64_ADD(BV64_SUB(BV64_ADD(BV64_MUL(32, BV32_ZEXT64(group_id_x)), BV32_ZEXT64(local_id_x)), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n))), 8191), 8192) == 0 then 1 else 0))), BV1_ZEXT32((if BV64_SREM(BV64_SUB(BV32_ZEXT64(local_id_y), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n))), 16) == 0 then 1 else 0)))), BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV1_ZEXT32((if BV64_SGE(BV64_ADD(BV64_MUL(32, BV32_ZEXT64(group_id_x)), BV32_ZEXT64(local_id_x)), 0) then 1 else 0)), BV1_ZEXT32((if BV64_SLE(BV64_ADD(BV64_MUL(32, BV32_ZEXT64(group_id_x)), BV32_ZEXT64(local_id_x)), 8191) then 1 else 0))), BV1_ZEXT32((if BV32_UGE($n, BV32_ADD(BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n), 2)) then 1 else 0))), BV1_ZEXT32((if BV32_UGE(BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n), 1) then 1 else 0))), BV1_ZEXT32((if BV32_UGE($n, BV32_ADD(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), 3)) then 1 else 0))), BV1_ZEXT32((if BV32_UGE(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), 0) then 1 else 0))), BV1_ZEXT32((if BV64_SGE(BV32_ZEXT64(BV32_UREM(BV32_ADD(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), 1), 8192)), BV64_MUL(32, BV32_ZEXT64(group_id_y))) then 1 else 0))), BV1_ZEXT32((if BV64_SGE(BV64_ADD(BV64_MUL(32, BV32_ZEXT64(group_id_y)), 31), BV32_ZEXT64(BV32_UREM(BV32_ADD(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), 1), 8192))) then 1 else 0))), BV1_ZEXT32((if BV64_SREM(BV64_SUB(BV64_ADD(BV64_MUL(32, BV32_ZEXT64(group_id_x)), BV32_ZEXT64(local_id_x)), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n))), 8192) == 0 then 1 else 0))), BV1_ZEXT32((if BV64_SREM(BV64_ADD(BV64_SUB(BV32_ZEXT64(local_id_y), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n))), 15), 16) == 0 then 1 else 0)))), BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV1_ZEXT32((if BV32_UGE(BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n), 3) then 1 else 0)), BV1_ZEXT32((if BV32_UGE($n, BV32_ADD(BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n), 2)) then 1 else 0))), BV1_ZEXT32((if BV32_ADD(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), 1) == $n then 1 else 0))), BV1_ZEXT32((if BV64_SLE(BV64_SREM(BV64_SUB(BV64_ADD(BV64_ADD(BV64_ADD(BV64_ADD(BV64_ADD(BV32_SEXT64(BV32_MUL(8191, $n)), BV64_MUL(32, BV32_ZEXT64(group_id_x))), BV64_MUL(32, BV32_ZEXT64(group_id_y))), BV32_ZEXT64(local_id_x)), BV64_MUL(8192, BV32_ZEXT64(local_id_y))), BV32_ZEXT64(BV32_MUL(8191, BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n)))), 8159), 8192), 31) then 1 else 0))), BV1_ZEXT32((if BV64_SREM(BV64_SUB(BV64_ADD(BV64_MUL(32, BV32_ZEXT64(group_id_x)), BV32_ZEXT64(local_id_x)), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n))), 8192) == 0 then 1 else 0))), BV1_ZEXT32((if BV64_SREM(BV64_ADD(BV64_ADD(BV32_SEXT64(BV32_SUB(0, $n)), BV32_ZEXT64(local_id_y)), 2), 16) == 0 then 1 else 0)))), BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV1_ZEXT32((if BV32_ZEXT64(group_id_y) == 0 then 1 else 0)), BV1_ZEXT32((if BV32_ZEXT64(local_id_y) == 1 then 1 else 0))), BV1_ZEXT32((if BV32_UGE(BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n), 3) then 1 else 0))), BV1_ZEXT32((if BV32_UGE($n, BV32_ADD(BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n), 2)) then 1 else 0))), BV1_ZEXT32((if BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n) == 1 then 1 else 0))), BV1_ZEXT32((if BV64_SREM(BV64_SUB(BV64_ADD(BV64_MUL(32, BV32_ZEXT64(group_id_x)), BV32_ZEXT64(local_id_x)), BV32_ZEXT64(BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n))), 8192) == 0 then 1 else 0)))), BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV32_AND(BV1_ZEXT32((if BV32_SGE($n, 4) then 1 else 0)), BV1_ZEXT32((if BV32_ZEXT64(group_id_x) == 0 then 1 else 0))), BV1_ZEXT32((if BV32_ZEXT64(local_id_x) == 2 then 1 else 0))), BV1_ZEXT32((if BV32_UREM(BV32_UDIV(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), $n) == 2 then 1 else 0))), BV1_ZEXT32((if BV32_ADD(BV32_UREM(BV32_UDIV(BV32_MUL(8, _WATCHED_OFFSET), 8), $n), 1) == $n then 1 else 0))), BV1_ZEXT32((if BV64_SLE(BV64_SREM(BV64_SUB(BV64_ADD(BV64_ADD(BV32_SEXT64(BV32_MUL(8191, $n)), BV64_MUL(32, BV32_ZEXT64(group_id_y))), BV64_MUL(8192, BV32_ZEXT64(local_id_y))), 8159), 8192), 31) then 1 else 0))), BV1_ZEXT32((if BV64_SREM(BV64_ADD(BV64_ADD(BV32_SEXT64(BV32_SUB(0, $n)), BV32_ZEXT64(local_id_y)), 2), 16) == 0 then 1 else 0)))) != 0) then 1 else 0) != 0;
{
  var $c1.0:int;
  var $c2.0:int;
  var $0:int;
  var $c4.0:int;
  var $1:int;
  var v0:int;
  var v1:int;
  var v2:int;
  var v3:int;
  var v4:bool;
  var v5:bool;
  var v6:bool;
  var v7:bool;
  var v8:bool;
  var v9:bool;
  var v10:bool;
  var v11:int;
  var v12:int;
  var v13:int;
  var v14:int;
  var v15:int;
$0:
  assert {:block_sourceloc} {:sourceloc_num 5} true;
  v0 := BV32_ZEXT64(group_id_x);
  v1 := BV32_ZEXT64(group_id_y);
  v2 := BV32_ZEXT64(local_id_x);
  v3 := BV32_ZEXT64(local_id_y);
  $c1.0 := BV64_MUL(32, v0);
  goto $1;
$1:
  assert {:block_sourceloc} {:sourceloc_num 6} true;
  v4 := BV64_SLT($c1.0, BV32_SEXT64(BV32_SUB($n, 1)));
  goto $truebb, $falsebb;
$2:
  assert {:block_sourceloc} {:sourceloc_num 7} true;
  v5 := BV64_SGE(BV32_SEXT64($n), BV64_ADD(BV64_ADD(v2, $c1.0), 2));
  goto $truebb0, $falsebb0;
$3:
  assert {:block_sourceloc} {:sourceloc_num 8} true;
  v6 := BV64_SGE(BV64_ADD(v2, $c1.0), 1);
  goto $truebb1, $falsebb1;
$4:
  assert {:block_sourceloc} {:sourceloc_num 9} true;
  $c2.0 := BV64_MUL(32, v1);
  goto $5;
$5:
  assert {:block_sourceloc} {:sourceloc_num 10} true;
  v7 := BV64_SLT($c2.0, BV32_SEXT64(BV32_SUB($n, 1)));
  goto $truebb2, $falsebb2;
$6:
  assert {:block_sourceloc} {:sourceloc_num 11} true;
  v8 := BV64_SGT(v3, BV64_ADD(BV64_SUB(BV64_SREM(BV64_ADD(BV64_ADD(v3, $c2.0), 15), 16), $c2.0), 1));
  goto $truebb3, $falsebb3;
$7:
  assert {:block_sourceloc} {:sourceloc_num 12} true;
  $0 := v3;
  goto $9;
$8:
  assert {:block_sourceloc} {:sourceloc_num 13} true;
  $0 := BV64_ADD(BV64_SUB(BV64_SREM(BV64_ADD(BV64_ADD(v3, $c2.0), 15), 16), $c2.0), 1);
  goto $9;
$9:
  assert {:block_sourceloc} {:sourceloc_num 14} true;
  $c4.0 := $0;
  goto $10;
$10:
  assert {:block_sourceloc} {:sourceloc_num 15} true;
  v9 := BV64_SLT(31, BV64_SUB(BV64_SUB(BV32_SEXT64($n), $c2.0), 2));
  goto $truebb4, $falsebb4;
$11:
  assert {:block_sourceloc} {:sourceloc_num 16} true;
  $1 := 31;
  goto $13;
$12:
  assert {:block_sourceloc} {:sourceloc_num 17} true;
  $1 := BV64_SUB(BV64_SUB(BV32_SEXT64($n), $c2.0), 2);
  goto $13;
$13:
  assert {:block_sourceloc} {:sourceloc_num 18} true;
  v10 := BV64_SLE($c4.0, $1);
  goto $truebb5, $falsebb5;
$14:
  assert {:block_sourceloc} {:sourceloc_num 19} true;
  assert {:sourceloc} {:sourceloc_num 20} true;
  v11 := $$A[BV_EXTRACT(BV64_ADD(BV64_MUL(BV64_ADD(v2, $c1.0), BV32_SEXT64($n)), BV64_ADD($c2.0, $c4.0)), 32, 0)];
  assert {:sourceloc} {:sourceloc_num 21} true;
  v12 := $$A[BV_EXTRACT(BV64_ADD(BV64_MUL(BV64_ADD(v2, $c1.0), BV32_SEXT64($n)), BV64_SUB(BV64_ADD($c2.0, $c4.0), 1)), 32, 0)];
  assert {:sourceloc} {:sourceloc_num 22} true;
  v13 := $$A[BV_EXTRACT(BV64_ADD(BV64_MUL(BV64_ADD(v2, $c1.0), BV32_SEXT64($n)), BV64_ADD(BV64_ADD($c2.0, $c4.0), 1)), 32, 0)];
  assert {:sourceloc} {:sourceloc_num 23} true;
  v14 := $$A[BV_EXTRACT(BV64_ADD(BV64_MUL(BV64_ADD(BV64_ADD(v2, $c1.0), 1), BV32_SEXT64($n)), BV64_ADD($c2.0, $c4.0)), 32, 0)];
  assert {:sourceloc} {:sourceloc_num 24} true;
  v15 := $$A[BV_EXTRACT(BV64_ADD(BV64_MUL(BV64_SUB(BV64_ADD(v2, $c1.0), 1), BV32_SEXT64($n)), BV64_ADD($c2.0, $c4.0)), 32, 0)];
  assert {:sourceloc} {:sourceloc_num 25} true;
  $$B[BV_EXTRACT(BV64_ADD(BV64_MUL(BV64_ADD(v2, $c1.0), BV32_SEXT64($n)), BV64_ADD($c2.0, $c4.0)), 32, 0)] := FMUL64(4596373779694328218, FADD64(FADD64(FADD64(FADD64(v11, v12), v13), v14), v15));
  goto $15;
$15:
  assert {:block_sourceloc} {:sourceloc_num 26} true;
  $c4.0 := BV64_ADD($c4.0, 16);
  goto $10;
$16:
  assert {:block_sourceloc} {:sourceloc_num 27} true;
  goto $17;
$17:
  assert {:block_sourceloc} {:sourceloc_num 28} true;
  $c2.0 := BV64_ADD($c2.0, 8192);
  goto $5;
$18:
  assert {:block_sourceloc} {:sourceloc_num 29} true;
  goto $19;
$19:
  assert {:block_sourceloc} {:sourceloc_num 30} true;
  goto $20;
$20:
  assert {:block_sourceloc} {:sourceloc_num 31} true;
  $c1.0 := BV64_ADD($c1.0, 8192);
  goto $1;
$21:
  assert {:block_sourceloc} {:sourceloc_num 32} true;
  return;
$truebb:
  assume {:partition} v4;
  assert {:block_sourceloc} {:sourceloc_num 33} true;
  goto $2;
$falsebb:
  assume {:partition} !v4;
  assert {:block_sourceloc} {:sourceloc_num 34} true;
  goto $21;
$truebb0:
  assume {:partition} v5;
  assert {:block_sourceloc} {:sourceloc_num 35} true;
  goto $3;
$falsebb0:
  assume {:partition} !v5;
  assert {:block_sourceloc} {:sourceloc_num 36} true;
  goto $19;
$truebb1:
  assume {:partition} v6;
  assert {:block_sourceloc} {:sourceloc_num 37} true;
  goto $4;
$falsebb1:
  assume {:partition} !v6;
  assert {:block_sourceloc} {:sourceloc_num 38} true;
  goto $19;
$truebb2:
  assume {:partition} v7;
  assert {:block_sourceloc} {:sourceloc_num 39} true;
  goto $6;
$falsebb2:
  assume {:partition} !v7;
  assert {:block_sourceloc} {:sourceloc_num 40} true;
  goto $18;
$truebb3:
  assume {:partition} v8;
  assert {:block_sourceloc} {:sourceloc_num 41} true;
  goto $7;
$falsebb3:
  assume {:partition} !v8;
  assert {:block_sourceloc} {:sourceloc_num 42} true;
  goto $8;
$truebb4:
  assume {:partition} v9;
  assert {:block_sourceloc} {:sourceloc_num 43} true;
  goto $11;
$falsebb4:
  assume {:partition} !v9;
  assert {:block_sourceloc} {:sourceloc_num 44} true;
  goto $12;
$truebb5:
  assume {:partition} v10;
  assert {:block_sourceloc} {:sourceloc_num 45} true;
  goto $14;
$falsebb5:
  assume {:partition} !v10;
  assert {:block_sourceloc} {:sourceloc_num 46} true;
  goto $16;
}
axiom (if group_size_z == 1 then 1 else 0) != 0;
axiom (if num_groups_z == 1 then 1 else 0) != 0;
axiom (if group_size_x == 32 then 1 else 0) != 0;
axiom (if group_size_y == 16 then 1 else 0) != 0;
axiom (if num_groups_x == 32 then 1 else 0) != 0;
axiom (if num_groups_y == 32 then 1 else 0) != 0;
axiom (if global_offset_x == 0 then 1 else 0) != 0;
axiom (if global_offset_y == 0 then 1 else 0) != 0;
axiom (if global_offset_z == 0 then 1 else 0) != 0;
